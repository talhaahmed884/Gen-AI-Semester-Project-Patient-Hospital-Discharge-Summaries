{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Discharge Summarization using MedGemma 4B with QLoRA\n",
    "\n",
    "**Project Overview:**\n",
    "This notebook demonstrates Parameter-Efficient Fine-Tuning (PEFT) using QLoRA on the MedGemma 4B model for clinical discharge summarization. The objective is to achieve **high recall** - generating detailed, verbose summaries that capture all medical entities (diagnoses, medications, vitals, abnormal lab results) from source clinical notes.\n",
    "\n",
    "**Key Technologies:**\n",
    "- Model: google/medgemma-4b (or base Gemma-4b)\n",
    "- Technique: QLoRA (4-bit quantization)\n",
    "- Evaluation: Clinical BERTScore using Bio_ClinicalBERT\n",
    "- Platform: Google Colab / Consumer GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we install all necessary libraries for model loading, quantization, fine-tuning, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "\"\"\"\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U bert_score\n",
    "!pip install -q -U scipy\n",
    "!pip install -q -U hf-xet\n",
    "# Required for Gemma model architecture\n",
    "!pip install -q -U einops\n",
    "!pip install -q torch==2.9.1+cu130 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130\n",
    "\"\"\"\n",
    "print(\"✓ All libraries installed successfully!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert_score import BERTScorer\n",
    "from datasets import Dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters\n",
    "\n",
    "Define all model paths, LoRA parameters, and training hyperparameters in one place for easy modification."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# NOTE: If google/medgemma-4b is not publicly available, use \"google/gemma-2-4b-it\" \n",
    "MODEL_NAME = \"google/medgemma-4b-it\"  # Update to \"google/medgemma-4b\" when available\n",
    "\n",
    "# ============================================================================\n",
    "# LORA CONFIGURATION\n",
    "# ============================================================================\n",
    "# These parameters control the LoRA adapter architecture:\n",
    "# - r (rank): The dimensionality of the low-rank matrices. Higher = more parameters = better fit but more memory\n",
    "# - lora_alpha: Scaling factor for LoRA updates. Higher alpha = larger learning rate for LoRA weights\n",
    "# - lora_dropout: Dropout probability for LoRA layers to prevent overfitting\n",
    "\n",
    "LORA_R = 32  # Rank of 32 provides good balance between performance and memory\n",
    "LORA_ALPHA = 64  # Alpha = 2*r is a common heuristic\n",
    "LORA_DROPOUT = 0.05  # Small dropout for regularization\n",
    "\n",
    "# Target modules for Gemma architecture\n",
    "# These are the attention and MLP projection layers where LoRA adapters will be inserted\n",
    "# Gemma uses a standard transformer architecture with:\n",
    "# - q_proj, k_proj, v_proj: Query, Key, Value projections in attention\n",
    "# - o_proj: Output projection after attention\n",
    "# - gate_proj, up_proj, down_proj: MLP layers (Gemma uses SwiGLU activation)\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "OUTPUT_DIR = \"./medgemma-discharge-summarization\"\n",
    "# Changed to 1 by Bryan: we want to avoid memorization as much as possible\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 1  # Per device batch size (increase if you have more VRAM)\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Effective batch size = 8\n",
    "LEARNING_RATE = 2e-4  # Standard learning rate for LoRA fine-tuning\n",
    "MAX_SEQ_LENGTH = 2048  # Maximum sequence length (Gemma supports up to 8192, but we use 2048 for memory efficiency)\n",
    "WARMUP_STEPS = 100  # Warmup steps for learning rate scheduler\n",
    "LOGGING_STEPS = 10  # Log training metrics every N steps\n",
    "SAVE_STEPS = 100  # Save checkpoint every N steps\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "MAX_NEW_TOKENS = 512  # Allow longer summaries to capture all details\n",
    "TEMPERATURE = 0.7  # Moderate temperature for balance between creativity and coherence\n",
    "TOP_P = 0.9  # Nucleus sampling for diverse but relevant outputs\n",
    "TOP_K = 50  # Top-K sampling\n",
    "REPETITION_PENALTY = 1.1  # Slight penalty to avoid repetitive text\n",
    "\n",
    "print(\"✓ Configuration loaded successfully!\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA Rank: {LORA_R}, Alpha: {LORA_ALPHA}\")\n",
    "print(\n",
    "    f\"  Training: {NUM_EPOCHS} epochs, Batch Size: {BATCH_SIZE}, Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2A. Google Colab Setup (OPTIONAL - Only for Colab Users)\n",
    "\n",
    "**Use this section ONLY if you're running this notebook on Google Colab**\n",
    "\n",
    "This section will:\n",
    "1. Mount your Google Drive\n",
    "2. Set the path to your dataset in Google Drive\n",
    "3. Verify GPU availability\n",
    "\n",
    "**Instructions:**\n",
    "- If running on **Google Colab**, run the cells below\n",
    "- If running **locally**, skip this entire section and go directly to Section 3"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# MOUNT GOOGLE DRIVE (COLAB ONLY)\n",
    "# ============================================================================\n",
    "\n",
    "# This cell will mount your Google Drive to access your dataset\n",
    "# You'll be prompted to authorize access to your Google Drive\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Mount Google Drive at /content/drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    print(\"✓ Google Drive mounted successfully!\")\n",
    "    print(\"  Your Drive is accessible at: /content/drive/MyDrive/\")\n",
    "    print(\"\\nYou can now access files from your Google Drive.\")\n",
    "\n",
    "    IS_COLAB = True\n",
    "\n",
    "except ImportError:\n",
    "    print(\"⚠ Not running on Google Colab - skipping Drive mount\")\n",
    "    print(\"  If you're running locally, this is expected. Skip to Section 3.\")\n",
    "    IS_COLAB = False"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Load and Prepare Dataset\n",
    "\n",
    "This section provides **THREE** options for loading data:\n",
    "\n",
    "### **Option A (Colab): Load from Google Drive**\n",
    "- **Use if**: Running on Google Colab with dataset in Google Drive\n",
    "- **Section**: 3A (Colab) below\n",
    "\n",
    "### **Option B (Local): Load from Local File**\n",
    "- **Use if**: Running locally with `mimic_cleaned_text_only.csv` in project directory\n",
    "- **Section**: 3A (Local) below\n",
    "\n",
    "**Instructions:**\n",
    "- **Colab users**: Run Section 2A first, then use Section 3A (Colab)\n",
    "- **Local users**: Skip Section 2A, use Section 3A (Local)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3A (Colab). Load Dataset from Google Drive\n",
    "\n",
    "**Use this section if you're running on Google Colab and have your dataset in Google Drive**\n",
    "\n",
    "This will load your MIMIC dataset directly from your Google Drive.\n",
    "\n",
    "**Setup Instructions:**\n",
    "1. Upload `mimic_cleaned_text_only.csv` to your Google Drive\n",
    "2. Update the `DRIVE_DATASET_PATH` below with the correct path\n",
    "3. Common paths:\n",
    "   - `\"/content/drive/MyDrive/mimic_cleaned_text_only.csv\"` (root of My Drive)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# VERIFY GPU AND SETUP (COLAB ONLY)\n",
    "# ============================================================================\n",
    "\n",
    "# Check if running on Colab and verify GPU setup\n",
    "if IS_COLAB:\n",
    "    import subprocess\n",
    "\n",
    "    print(\"Checking GPU availability on Colab...\\n\")\n",
    "\n",
    "    # Run nvidia-smi to check GPU\n",
    "    try:\n",
    "        gpu_info = subprocess.check_output(['nvidia-smi'], encoding='utf-8')\n",
    "        print(gpu_info)\n",
    "        print(\"GPU is available!\")\n",
    "        print(\"\\nIMPORTANT: Make sure you're using a GPU runtime:\")\n",
    "        print(\"Runtime → Change runtime type → Hardware accelerator → GPU (T4 or better recommended)\")\n",
    "    except:\n",
    "        print(\"No GPU detected!\")\n",
    "        print(\"\\nYou MUST enable GPU for this notebook:\")\n",
    "        print(\"  1. Go to Runtime → Change runtime type\")\n",
    "        print(\"  2. Set Hardware accelerator to 'GPU'\")\n",
    "        print(\"  3. Click Save\")\n",
    "        print(\"  4. Restart the runtime\")\n",
    "\n",
    "    # Check RAM\n",
    "    import psutil\n",
    "\n",
    "    ram_gb = psutil.virtual_memory().total / 1e9\n",
    "    print(f\"\\nAvailable RAM: {ram_gb:.2f} GB\")\n",
    "\n",
    "    if ram_gb < 12:\n",
    "        print(\"WARNING: Low RAM detected. Consider using Colab Pro for High-RAM runtime.\")\n",
    "else:\n",
    "    print(\"Skipping Colab-specific checks (running locally)\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# LOAD MIMIC DATASET FROM GOOGLE DRIVE (COLAB ONLY)\n",
    "# ============================================================================\n",
    "\n",
    "# IMPORTANT: Only run this cell if you're on Google Colab\n",
    "# Update the path below to match where you uploaded your dataset in Google Drive\n",
    "\n",
    "if IS_COLAB:\n",
    "    import os\n",
    "\n",
    "    # ========================================================================\n",
    "    # CONFIGURE THIS PATH TO MATCH YOUR GOOGLE DRIVE STRUCTURE\n",
    "    # ========================================================================\n",
    "    # Update this to the actual path where you uploaded your CSV file in Google Drive\n",
    "    DRIVE_DATASET_PATH = \"/content/drive/MyDrive/Colab Notebooks/Gen AI/Semester Project/mimic_cleaned_text_only.csv\"\n",
    "\n",
    "    # Check if file exists\n",
    "    if os.path.exists(DRIVE_DATASET_PATH):\n",
    "        print(f\"Loading MIMIC dataset from Google Drive...\")\n",
    "        print(f\"Path: {DRIVE_DATASET_PATH}\\n\")\n",
    "\n",
    "        # Load the CSV file using pandas\n",
    "        mimic_df = pd.read_csv(DRIVE_DATASET_PATH)\n",
    "        mimic_df = mimic_df[:10_000]\n",
    "\n",
    "        print(f\" Dataset loaded successfully from Google Drive!\")\n",
    "        print(f\"  Total samples: {len(mimic_df)}\")\n",
    "        print(f\"  Columns: {list(mimic_df.columns)}\\n\")\n",
    "\n",
    "        # Display basic statistics\n",
    "        print(\"Dataset Statistics:\")\n",
    "        print(f\"  Average input length: {mimic_df['final_input'].str.len().mean():.0f} characters\")\n",
    "        print(f\"  Average target length: {mimic_df['final_target'].str.len().mean():.0f} characters\")\n",
    "        print(f\"  Minimum input length: {mimic_df['final_input'].str.len().min():.0f} characters\")\n",
    "        print(f\"  Maximum input length: {mimic_df['final_input'].str.len().max():.0f} characters\")\n",
    "\n",
    "        # Add instruction column emphasizing HIGH RECALL\n",
    "        instruction_text = \"Summarize the following clinical discharge notes. Include ALL diagnoses, medications, vitals, lab results, procedures, and follow-up instructions. Ensure complete coverage of all medical entities.\"\n",
    "        mimic_df['instruction'] = instruction_text\n",
    "\n",
    "        # Rename columns to match expected format\n",
    "        mimic_df = mimic_df.rename(columns={\n",
    "            'final_input': 'input',\n",
    "            'final_target': 'output'\n",
    "        })\n",
    "\n",
    "        # Remove rows with missing data\n",
    "        initial_count = len(mimic_df)\n",
    "        mimic_df = mimic_df.dropna(subset=['input', 'output'])\n",
    "        dropped_count = initial_count - len(mimic_df)\n",
    "\n",
    "        if dropped_count > 0:\n",
    "            print(f\"\\n Removed {dropped_count} rows with missing data\")\n",
    "\n",
    "        # Convert to Hugging Face Dataset\n",
    "        dataset = Dataset.from_pandas(mimic_df[['instruction', 'input', 'output']])\n",
    "\n",
    "        # Split into train and test sets (90/10 split)\n",
    "        dataset = dataset.train_test_split(train_size=0.1, test_size=0.1, seed=42)\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        test_dataset = dataset[\"test\"]\n",
    "\n",
    "        print(f\"\\n Dataset prepared and split!\")\n",
    "        print(f\"  Training samples: {len(train_dataset)}\")\n",
    "        print(f\"  Test samples: {len(test_dataset)}\")\n",
    "\n",
    "        # Display a sample\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(\"SAMPLE TRAINING EXAMPLE:\")\n",
    "        print(f\"{'=' * 80}\\n\")\n",
    "        print(f\"Instruction: {train_dataset[0]['instruction'][:150]}...\")\n",
    "        print(f\"\\nInput (first 300 chars):\\n{train_dataset[0]['input'][:300]}...\")\n",
    "        print(f\"\\nOutput (first 300 chars):\\n{train_dataset[0]['output'][:300]}...\")\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "\n",
    "        print(\"\\n MIMIC dataset loaded from Google Drive!\")\n",
    "        print(\"  You can now skip sections 3A (Local) and 3B (Sample Data)\")\n",
    "        print(\"  Proceed to Section 4 (Load Model with 4-bit Quantization)\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found at: {DRIVE_DATASET_PATH}\")\n",
    "        print(f\"\\nPlease check:\")\n",
    "        print(f\"  1. Is the file uploaded to your Google Drive?\")\n",
    "        print(f\"  2. Is the path correct?\")\n",
    "        print(f\"  3. Did you mount Google Drive (run Section 2A)?\")\n",
    "        print(f\"\\nTo find the correct path:\")\n",
    "        print(f\"  1. In the left sidebar, click the folder icon\")\n",
    "        print(f\"  2. Navigate to drive/MyDrive/\")\n",
    "        print(f\"  3. Find your CSV file\")\n",
    "        print(f\"  4. Right-click → Copy path\")\n",
    "        print(f\"  5. Update DRIVE_DATASET_PATH above\")\n",
    "\n",
    "else:\n",
    "    print(\"Not running on Google Colab - skipping Google Drive dataset loading\")\n",
    "    print(\"Use Section 3A (Local) or 3B (Sample Data) instead\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3B. Load The Dataset\n",
    "\n",
    "**Use this section if you have the `mimic_cleaned_text_only.csv` file**\n",
    "\n",
    "This loads your actual MIMIC clinical discharge dataset with the correct column mappings:\n",
    "- `final_input` → clinical notes\n",
    "- `final_target` → reference summaries"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "# Path to your MIMIC dataset CSV file\n",
    "# Adjust this path if your file is located elsewhere\n",
    "MIMIC_CSV_PATH = \"mimic_cleaned_text_only.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(MIMIC_CSV_PATH):\n",
    "    print(f\"Loading MIMIC dataset from: {MIMIC_CSV_PATH}\\n\")\n",
    "\n",
    "    # Load the CSV file using pandas\n",
    "    # The file should have two columns: final_input and final_target\n",
    "    mimic_df = pd.read_csv(MIMIC_CSV_PATH)\n",
    "\n",
    "    mimic_df = mimic_df[:10_000]\n",
    "\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"  Total samples: {len(mimic_df)}\")\n",
    "    print(f\"  Columns: {list(mimic_df.columns)}\\n\")\n",
    "\n",
    "    # Display basic statistics\n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(f\"  Average input length: {mimic_df['final_input'].str.len().mean():.0f} characters\")\n",
    "    print(f\"  Average target length: {mimic_df['final_target'].str.len().mean():.0f} characters\")\n",
    "    print(f\"  Minimum input length: {mimic_df['final_input'].str.len().min():.0f} characters\")\n",
    "    print(f\"  Maximum input length: {mimic_df['final_input'].str.len().max():.0f} characters\")\n",
    "\n",
    "    # Add a consistent instruction column\n",
    "    # This instruction emphasizes HIGH RECALL - capturing all medical details\n",
    "    instruction_text = \"Summarize the following clinical discharge notes. Include ALL diagnoses, medications, vitals, lab results, procedures, and follow-up instructions. Ensure complete coverage of all medical entities.\"\n",
    "    mimic_df['instruction'] = instruction_text\n",
    "\n",
    "    # Rename columns to match the expected format\n",
    "    # final_input → input (clinical notes)\n",
    "    # final_target → output (reference summary)\n",
    "    mimic_df = mimic_df.rename(columns={\n",
    "        'final_input': 'input',\n",
    "        'final_target': 'output'\n",
    "    })\n",
    "\n",
    "    # Remove any rows with missing data\n",
    "    initial_count = len(mimic_df)\n",
    "    mimic_df = mimic_df.dropna(subset=['input', 'output'])\n",
    "    dropped_count = initial_count - len(mimic_df)\n",
    "\n",
    "    if dropped_count > 0:\n",
    "        print(f\"\\nRemoved {dropped_count} rows with missing data\")\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(mimic_df[['instruction', 'input', 'output']])\n",
    "\n",
    "    # Split into train and test sets\n",
    "    dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    test_dataset = dataset[\"test\"]\n",
    "\n",
    "    print(f\"\\n✓ Dataset prepared and split!\")\n",
    "    print(f\"  Training samples: {len(train_dataset)}\")\n",
    "    print(f\"  Test samples: {len(test_dataset)}\")\n",
    "\n",
    "    # Display a sample from the training set\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"SAMPLE TRAINING EXAMPLE:\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    print(f\"Instruction: {train_dataset[0]['instruction'][:150]}...\")\n",
    "    print(f\"\\nInput (first 300 chars):\\n{train_dataset[0]['input'][:300]}...\")\n",
    "    print(f\"\\nOutput (first 300 chars):\\n{train_dataset[0]['output'][:300]}...\")\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "\n",
    "    print(\"\\n✓ MIMIC dataset loaded successfully! You can now skip Section 3B.\")\n",
    "    print(\"  Proceed to Section 4 (Load Model with 4-bit Quantization)\")\n",
    "\n",
    "else:\n",
    "    print(f\"⚠ File not found: {MIMIC_CSV_PATH}\")\n",
    "    print(f\"\\nPlease either:\")\n",
    "    print(f\"  1. Place the mimic_cleaned_text_only.csv file in the current directory\")\n",
    "    print(f\"  2. Update MIMIC_CSV_PATH variable with the correct file path\")\n",
    "    print(f\"  3. Skip to Section 3B to use sample data instead\\n\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with 4-bit Quantization (QLoRA)\n",
    "\n",
    "QLoRA (Quantized LoRA) enables fine-tuning large models on consumer GPUs by:\n",
    "1. Loading the base model in 4-bit precision (NormalFloat 4-bit)\n",
    "2. Using double quantization to further reduce memory\n",
    "3. Computing gradients in float16 for numerical stability\n",
    "4. Training only LoRA adapter weights (a small fraction of total parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Quantization configuration created\n",
      "  Quantization type: NF4 (4-bit NormalFloat)\n",
      "  Double quantization: Enabled\n",
      "  Compute dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "compute_dtype = torch.float16  # Use float16 for faster computation\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "print(\"✓ Quantization configuration created\")\n",
    "print(f\"  Quantization type: NF4 (4-bit NormalFloat)\")\n",
    "print(f\"  Double quantization: Enabled\")\n",
    "print(f\"  Compute dtype: {compute_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae58dfb8977940f291475ddc07f9dca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer loaded successfully\n",
      "  Vocabulary size: 262145\n",
      "  EOS token: <eos> (ID: 1)\n",
      "  PAD token: <eos> (ID: 1)\n"
     ]
    }
   ],
   "source": [
    "# - add_eos_token: Automatically add end-of-sequence token (important for Gemma)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",  # Right padding is standard for causal language models\n",
    "    add_eos_token=True,  # Ensure EOS token is added for proper sequence termination\n",
    ")\n",
    "\n",
    "# Set the padding token to be the same as EOS token\n",
    "# (Gemma models don't have a separate PAD token by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"✓ Tokenizer loaded successfully\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model... This may take a few minutes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea5a106ab8943b6a682a6653d6b9a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model loaded successfully with 4-bit quantization\n",
      "  Model type: gemma3\n",
      "  Number of parameters: 4.30B\n",
      "  Device map: {'': 0}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD MODEL WITH QUANTIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading model... This may take a few minutes.\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,  # Apply 4-bit quantization\n",
    "    device_map=\"auto\",  # Automatically distribute model across available devices\n",
    "    trust_remote_code=True,\n",
    "    dtype=compute_dtype,  # Use float16 for non-quantized layers\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "# This function:\n",
    "# 1. Freezes all base model weights \n",
    "# 2. Enables gradient checkpointing to save memory\n",
    "# 3. Prepares input embeddings for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "# This trades compute for memory by recomputing activations during backward pass\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"  Model loaded successfully with 4-bit quantization\")\n",
    "print(f\"  Model type: {model.config.model_type}\")\n",
    "print(f\"  Number of parameters: {model.num_parameters() / 1e9:.2f}B\")\n",
    "print(f\"  Device map: {model.hf_device_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA Adapters\n",
    "\n",
    "LoRA (Low-Rank Adaptation) works by adding small trainable matrices to specific layers of the frozen base model. This dramatically reduces the number of trainable parameters while maintaining performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LoRA configuration created\n",
      "  Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "  LoRA rank (r): 32\n",
      "  LoRA alpha: 64\n",
      "  LoRA dropout: 0.05\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LORA CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# LoRA configuration parameters:\n",
    "# - r: Rank of the low-rank matrices (higher = more capacity but more parameters)\n",
    "# - lora_alpha: Scaling factor (controls magnitude of LoRA updates)\n",
    "# - target_modules: Which model layers to apply LoRA to\n",
    "# - lora_dropout: Dropout for regularization\n",
    "# - bias: Whether to train bias parameters (\"none\" is standard)\n",
    "# - task_type: Type of task (CAUSAL_LM for text generation)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",  # Don't train bias parameters\n",
    "    task_type=\"CAUSAL_LM\",  # Causal language modeling task\n",
    ")\n",
    "\n",
    "# NOTE: We will NOT apply LoRA here with get_peft_model()\n",
    "# Instead, we'll pass lora_config to SFTTrainer, which will handle it\n",
    "# This is required for newer versions of TRL\n",
    "\n",
    "print(\"  LoRA configuration created\")\n",
    "print(f\"  Target modules: {TARGET_MODULES}\")\n",
    "print(f\"  LoRA rank (r): {LORA_R}\")\n",
    "print(f\"  LoRA alpha: {LORA_ALPHA}\")\n",
    "print(f\"  LoRA dropout: {LORA_DROPOUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Training Data with Gemma Prompt Format\n",
    "\n",
    "**Critical:** Gemma models use a specific prompt format with special tokens:\n",
    "- `<start_of_turn>user`: Indicates user input\n",
    "- `<end_of_turn>`: Marks end of turn\n",
    "- `<start_of_turn>model`: Indicates model output\n",
    "\n",
    "Using the correct format is essential for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60cd72cc7a354f248de828313e0bcc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed12e6ab2eba4901b0932f0c80a876d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset formatted with Gemma prompt template\n",
      "\n",
      "Example formatted prompt (truncated):\n",
      "================================================================================\n",
      "<start_of_turn>user\n",
      "Summarize the following clinical discharge notes. Include ALL diagnoses, medications, vitals, lab results, procedures, and follow-up instructions. Ensure complete coverage of all medical entities.\n",
      "\n",
      "Clinical Notes:\n",
      "create summary based following information chief complaint dyspnea past medical history diastolic congestive heart failure aortic stenosis aortic insufficiency mild mitral regurgitation mild tricuspid regurgitation pulmonary hypertension coronary artery disease hype\n",
      "...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GEMMA PROMPT FORMATTING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def format_prompt_gemma(sample):\n",
    "    \"\"\"\n",
    "    Format a training sample using Gemma's conversation template.\n",
    "    \n",
    "    Gemma uses a turn-based conversation format:\n",
    "    <start_of_turn>user\n",
    "    {instruction}\n",
    "    {input}\n",
    "    <end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    {output}<end_of_turn>\n",
    "    \n",
    "    Args:\n",
    "        sample: Dictionary containing 'instruction', 'input', and 'output' keys\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with formatted 'text' field\n",
    "    \"\"\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample[\"input\"]\n",
    "    output_text = sample[\"output\"]\n",
    "\n",
    "    # Construct the full prompt using Gemma's format\n",
    "    # The user turn contains both the instruction and the clinical notes\n",
    "    # The model turn contains the expected summary output\n",
    "    full_prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{output_text}<end_of_turn>\"\"\"\n",
    "\n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "\n",
    "# Apply formatting to both train and test datasets\n",
    "train_dataset = train_dataset.map(format_prompt_gemma)\n",
    "test_dataset = test_dataset.map(format_prompt_gemma)\n",
    "\n",
    "print(\"✓ Dataset formatted with Gemma prompt template\")\n",
    "print(\"\\nExample formatted prompt (truncated):\")\n",
    "print(\"=\" * 80)\n",
    "print(train_dataset[0][\"text\"][:500])\n",
    "print(\"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Configuration and Trainer Setup\n",
    "\n",
    "Configure the training process using Hugging Face's `TrainingArguments` and the specialized `SFTTrainer` from the TRL library."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# TRAINING ARGUMENTS\n",
    "# ============================================================================\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# TrainingArguments control all aspects of the training loop:\n",
    "# Memory optimization:\n",
    "#   - gradient_accumulation_steps: Accumulate gradients over N steps (simulates larger batch)\n",
    "#   - gradient_checkpointing: Trade compute for memory\n",
    "#   - fp16: Use mixed precision training (faster + less memory)\n",
    "# \n",
    "# Optimization:\n",
    "#   - learning_rate: Step size for parameter updates\n",
    "#   - weight_decay: L2 regularization\n",
    "#   - warmup_steps: Gradually increase LR at start of training\n",
    "#   - lr_scheduler_type: How to adjust LR during training\n",
    "#   - optim: Optimizer choice (adamw_torch is standard)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",  # Standard AdamW optimizer\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,  # Gradient clipping to prevent exploding gradients\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine learning rate schedule\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,  # Keep only the 3 most recent checkpoints\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=SAVE_STEPS,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard (can enable if you want tracking)\n",
    "    push_to_hub=False,  # Don't push to Hugging Face Hub automatically\n",
    ")\n",
    "\n",
    "print(\" Training arguments configured\")\n",
    "print(f\"  Total training steps: ~{len(train_dataset) * NUM_EPOCHS // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Warmup steps: {WARMUP_STEPS}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# CREATE SUPERVISED FINE-TUNING TRAINER\n",
    "# ============================================================================\n",
    "\n",
    "# SFTTrainer from TRL library is specifically\n",
    "# designed for instruction fine-tuning of language models. It handles:\n",
    "# - Proper formatting of training data\n",
    "# - Automatic application of PEFT/LoRA adapters\n",
    "# - Memory-efficient training with large sequence lengths\n",
    "\n",
    "# Update training arguments to include max_seq_length\n",
    "training_args.max_seq_length = MAX_SEQ_LENGTH\n",
    "\n",
    "\n",
    "# Define formatting function to extract the 'text' field\n",
    "def formatting_func(example):\n",
    "    \"\"\"\n",
    "    Extract the formatted text from the dataset.\n",
    "    Our dataset already has a 'text' column with Gemma-formatted prompts.\n",
    "    \"\"\"\n",
    "    return example[\"text\"]  # Return as list for batch processing\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Pass the base model (before PEFT was applied)\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=lora_config,  # SFTTrainer will apply LoRA adapters\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,  # Explicitly pass tokenizer to avoid processor issues\n",
    "    formatting_func=formatting_func,  # Function to extract text from examples\n",
    ")\n",
    "\n",
    "print(\"  SFTTrainer initialized successfully\")\n",
    "print(f\"  Using max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  LoRA adapters applied automatically by SFTTrainer\")\n",
    "\n",
    "# Print trainable parameters (now that PEFT has been applied by SFTTrainer)\n",
    "trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "trainable_percent = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"\\n  Trainable parameters: {trainable_params:,} ({trainable_percent:.2f}% of total)\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Memory savings: Training only {trainable_percent:.2f}% of parameters!\")\n",
    "\n",
    "print(\"\\nTrainer is ready to begin fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fine-Tune the Model\n",
    "\n",
    "Now we train the model. This process will:\n",
    "1. Iterate through the training data for `NUM_EPOCHS` epochs\n",
    "2. Update only the LoRA adapter weights (not the base model)\n",
    "3. Log training metrics periodically\n",
    "4. Save checkpoints for recovery and evaluation\n",
    "\n",
    "**Note:** Training time depends on your GPU and dataset size. For the sample data, this should complete in a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "\n",
      "This will train for 1 epochs with:\n",
      "  - 9500 training samples\n",
      "  - Batch size: 1 (effective: 8)\n",
      "  - Learning rate: 0.0002\n",
      "\n",
      "Monitor the loss below. For good convergence, loss should decrease steadily.\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='1188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 101/1188 07:39 < 1:24:09, 0.22 it/s, Epoch 0.08/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/63 07:22 < 01:05, 0.12 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# START TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Starting fine-tuning...\\n\")\n",
    "print(\"This will train for {} epochs with:\".format(NUM_EPOCHS))\n",
    "print(f\"  - {len(train_dataset)} training samples\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(\"\\nMonitor the loss below. For good convergence, loss should decrease steadily.\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Train the model\n",
    "# The trainer will handle:\n",
    "# - Forward pass (compute predictions)\n",
    "# - Loss computation (compare predictions to ground truth)\n",
    "# - Backward pass (compute gradients)\n",
    "# - Optimizer step (update LoRA weights)\n",
    "# - Logging and checkpointing\n",
    "training_output = trainer.train()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n  Training completed successfully!\")\n",
    "print(f\"\\nFinal training loss: {training_output.training_loss:.4f}\")\n",
    "print(f\"Total training time: {training_output.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Samples per second: {training_output.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved to: ./medgemma-discharge-summarization/final\n",
      "\n",
      "The saved files include:\n",
      "  - adapter_config.json: LoRA configuration\n",
      "  - adapter_model.bin: Trained LoRA weights\n",
      "  - tokenizer files\n",
      "\n",
      "To load this model later, use:\n",
      "  model = AutoModelForCausalLM.from_pretrained('google/medgemma-4b-it', ...)\n",
      "  model = PeftModel.from_pretrained(model, './medgemma-discharge-summarization/final')\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE THE FINE-TUNED MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Save the trained LoRA adapters\n",
    "\n",
    "output_dir_final = f\"{OUTPUT_DIR}/final\"\n",
    "trainer.model.save_pretrained(output_dir_final)\n",
    "tokenizer.save_pretrained(output_dir_final)\n",
    "\n",
    "print(f\"✓ Model saved to: {output_dir_final}\")\n",
    "print(\"\\nThe saved files include:\")\n",
    "print(\"  - adapter_config.json: LoRA configuration\")\n",
    "print(\"  - adapter_model.bin: Trained LoRA weights\")\n",
    "print(\"  - tokenizer files\")\n",
    "print(\"\\nTo load this model later, use:\")\n",
    "print(f\"  model = AutoModelForCausalLM.from_pretrained('{MODEL_NAME}', ...)\")\n",
    "print(f\"  model = PeftModel.from_pretrained(model, '{output_dir_final}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Clinical BERTScore Evaluation\n",
    "\n",
    "**Why Clinical BERTScore?**\n",
    "\n",
    "Traditional metrics like BLEU or ROUGE measure word overlap, which is insufficient for medical text where:\n",
    "- Synonyms are common (\"myocardial infarction\" = \"heart attack\")\n",
    "- Semantic equivalence matters more than exact wording\n",
    "- Clinical accuracy is critical\n",
    "\n",
    "**BERTScore** measures semantic similarity using contextual embeddings. By using **Bio_ClinicalBERT** (trained on clinical notes from MIMIC-III), we get embeddings that understand medical terminology and context.\n",
    "\n",
    "**Interpretation:**\n",
    "- Precision: How much of the generated summary is relevant?\n",
    "- Recall: How much of the reference summary is captured? (Our primary metric for completeness)\n",
    "- F1: Harmonic mean of precision and recall\n",
    "\n",
    "Scores range from 0 to 1, with higher being better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Clinical BERTScore...\n",
      "This will download emilyalsentzer/Bio_ClinicalBERT if not cached.\n",
      "\n",
      "  Clinical BERTScore initialized\n",
      "  Model: emilyalsentzer/Bio_ClinicalBERT\n",
      "  Device: cuda\n",
      "\n",
      "This model was trained on MIMIC-III clinical notes and understands:\n",
      "  - Medical terminology and abbreviations\n",
      "  - Clinical context and relationships\n",
      "  - Semantic equivalence in healthcare text\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE CLINICAL BERTSCORE\n",
    "# ============================================================================\n",
    "\n",
    "# Create a BERTScorer with Bio_ClinicalBERT as the backbone\n",
    "# This model was trained on MIMIC-III clinical notes and understands medical language\n",
    "#\n",
    "# Important parameters:\n",
    "# - model_type: The BERT model to use for embeddings\n",
    "# - num_layers: Which layer's embeddings to use (9 is optimal for Bio_ClinicalBERT)\n",
    "# - rescale_with_baseline: Normalize scores using baseline statistics\n",
    "# - lang: Language (en for English)\n",
    "# - device: GPU if available, else CPU\n",
    "\n",
    "print(\"Initializing Clinical BERTScore...\")\n",
    "print(\"This will download emilyalsentzer/Bio_ClinicalBERT if not cached.\\n\")\n",
    "\n",
    "clinical_scorer = BERTScorer(\n",
    "    model_type=\"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    num_layers=9,  # Layer 9 has been found optimal for clinical text\n",
    "    rescale_with_baseline=True,\n",
    "    lang=\"en\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"  Clinical BERTScore initialized\")\n",
    "print(f\"  Model: emilyalsentzer/Bio_ClinicalBERT\")\n",
    "print(f\"  Device: {clinical_scorer.device}\")\n",
    "print(\"\\nThis model was trained on MIMIC-III clinical notes and understands:\")\n",
    "print(\"  - Medical terminology and abbreviations\")\n",
    "print(\"  - Clinical context and relationships\")\n",
    "print(\"  - Semantic equivalence in healthcare text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test set...\n",
      "\n",
      "Generating summary 1/1...\n",
      "  Generated 1878 characters\n",
      "\n",
      "✓ All predictions generated\n",
      "  Total predictions: 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GENERATE PREDICTIONS ON TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Generating predictions on test set...\\n\")\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Generate predictions for each test example\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    print(f\"Generating summary {i + 1}/{len(test_dataset)}...\")\n",
    "\n",
    "    # Extract the input (clinical notes)\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample[\"input\"]\n",
    "    reference = sample[\"output\"]\n",
    "\n",
    "    # Format the prompt for inference (same format as training, but without the model's response)\n",
    "    inference_prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate the summary\n",
    "    # Generation parameters are tuned for high recall (detailed outputs):\n",
    "    # - max_new_tokens: Allow long summaries\n",
    "    # - temperature: Control randomness (0.7 = moderate creativity)\n",
    "    # - top_p: Nucleus sampling for diverse but coherent text\n",
    "    # - do_sample: Enable sampling (vs greedy decoding)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the generated summary (remove the prompt)\n",
    "    # Find where the model's response starts\n",
    "    model_response_marker = \"<start_of_turn>model\"\n",
    "    if model_response_marker in generated_text:\n",
    "        generated_summary = generated_text.split(model_response_marker)[-1].strip()\n",
    "    else:\n",
    "        generated_summary = generated_text[len(inference_prompt):].strip()\n",
    "\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(reference)\n",
    "\n",
    "    print(f\"  Generated {len(generated_summary)} characters\\n\")\n",
    "\n",
    "print(\"✓ All predictions generated\")\n",
    "print(f\"  Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Clinical BERTScore...\n",
      "\n",
      "This metric measures semantic similarity using Bio_ClinicalBERT embeddings.\n",
      "Unlike BLEU/ROUGE (word overlap), BERTScore captures:\n",
      "  - Semantic equivalence (synonyms, paraphrases)\n",
      "  - Clinical context and medical terminology\n",
      "  - Conceptual similarity beyond surface form\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (531) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [2, 531].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m  - Conceptual similarity beyond surface form\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Compute BERTScore\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Returns three tensors: Precision, Recall, F1\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m P, R, F1 = \u001B[43mclinical_scorer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcands\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Generated summaries\u001B[39;49;00m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreferences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Reference summaries\u001B[39;49;00m\n\u001B[32m     17\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# Convert to numpy for easier manipulation\u001B[39;00m\n\u001B[32m     20\u001B[39m precision_scores = P.cpu().numpy()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cpp/Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries/.venv/lib/python3.12/site-packages/bert_score/scorer.py:220\u001B[39m, in \u001B[36mBERTScorer.score\u001B[39m\u001B[34m(self, cands, refs, verbose, batch_size, return_hash)\u001B[39m\n\u001B[32m    217\u001B[39m     idf_dict[\u001B[38;5;28mself\u001B[39m._tokenizer.sep_token_id] = \u001B[32m0\u001B[39m\n\u001B[32m    218\u001B[39m     idf_dict[\u001B[38;5;28mself\u001B[39m._tokenizer.cls_token_id] = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m220\u001B[39m all_preds = \u001B[43mbert_cos_score_idf\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    222\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    223\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcands\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    224\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_tokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    225\u001B[39m \u001B[43m    \u001B[49m\u001B[43midf_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    227\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    228\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    229\u001B[39m \u001B[43m    \u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    230\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m.cpu()\n\u001B[32m    232\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ref_group_boundaries \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    233\u001B[39m     max_preds = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cpp/Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries/.venv/lib/python3.12/site-packages/bert_score/utils.py:616\u001B[39m, in \u001B[36mbert_cos_score_idf\u001B[39m\u001B[34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001B[39m\n\u001B[32m    614\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch_start \u001B[38;5;129;01min\u001B[39;00m iter_range:\n\u001B[32m    615\u001B[39m     sen_batch = sentences[batch_start : batch_start + batch_size]\n\u001B[32m--> \u001B[39m\u001B[32m616\u001B[39m     embs, masks, padded_idf = \u001B[43mget_bert_embedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    617\u001B[39m \u001B[43m        \u001B[49m\u001B[43msen_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midf_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mall_layers\u001B[49m\n\u001B[32m    618\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    619\u001B[39m     embs = embs.cpu()\n\u001B[32m    620\u001B[39m     masks = masks.cpu()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cpp/Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries/.venv/lib/python3.12/site-packages/bert_score/utils.py:455\u001B[39m, in \u001B[36mget_bert_embedding\u001B[39m\u001B[34m(all_sens, model, tokenizer, idf_dict, batch_size, device, all_layers)\u001B[39m\n\u001B[32m    453\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m    454\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(all_sens), batch_size):\n\u001B[32m--> \u001B[39m\u001B[32m455\u001B[39m         batch_embedding = \u001B[43mbert_encode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    456\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    457\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpadded_sens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    458\u001B[39m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    459\u001B[39m \u001B[43m            \u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    460\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    461\u001B[39m         embeddings.append(batch_embedding)\n\u001B[32m    462\u001B[39m         \u001B[38;5;28;01mdel\u001B[39;00m batch_embedding\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cpp/Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries/.venv/lib/python3.12/site-packages/bert_score/utils.py:351\u001B[39m, in \u001B[36mbert_encode\u001B[39m\u001B[34m(model, x, attention_mask, all_layers)\u001B[39m\n\u001B[32m    349\u001B[39m model.eval()\n\u001B[32m    350\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m351\u001B[39m     out = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m all_layers:\n\u001B[32m    353\u001B[39m     emb = torch.stack(out[-\u001B[32m1\u001B[39m], dim=\u001B[32m2\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cpp/Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cpp/Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/cpp/Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:931\u001B[39m, in \u001B[36mBertModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m    929\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.embeddings, \u001B[33m\"\u001B[39m\u001B[33mtoken_type_ids\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    930\u001B[39m     buffered_token_type_ids = \u001B[38;5;28mself\u001B[39m.embeddings.token_type_ids[:, :seq_length]\n\u001B[32m--> \u001B[39m\u001B[32m931\u001B[39m     buffered_token_type_ids_expanded = \u001B[43mbuffered_token_type_ids\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexpand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m     token_type_ids = buffered_token_type_ids_expanded\n\u001B[32m    933\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mRuntimeError\u001B[39m: The expanded size of the tensor (531) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [2, 531].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPUTE CLINICAL BERTSCORE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Computing Clinical BERTScore...\\n\")\n",
    "print(\"This metric measures semantic similarity using Bio_ClinicalBERT embeddings.\")\n",
    "print(\"Unlike BLEU/ROUGE (word overlap), BERTScore captures:\")\n",
    "print(\"  - Semantic equivalence (synonyms, paraphrases)\")\n",
    "print(\"  - Clinical context and medical terminology\")\n",
    "print(\"  - Conceptual similarity beyond surface form\\n\")\n",
    "\n",
    "# Compute BERTScore\n",
    "# Returns three tensors: Precision, Recall, F1\n",
    "P, R, F1 = clinical_scorer.score(\n",
    "    cands=predictions,  # Generated summaries\n",
    "    refs=references,  # Reference summaries\n",
    ")\n",
    "\n",
    "# Convert to numpy for easier manipulation\n",
    "precision_scores = P.cpu().numpy()\n",
    "recall_scores = R.cpu().numpy()\n",
    "f1_scores = F1.cpu().numpy()\n",
    "\n",
    "# Compute averages\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLINICAL BERTSCORE RESULTS (using Bio_ClinicalBERT)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAverage Precision: {avg_precision:.4f}\")\n",
    "print(\"  → Measures: How much of the generated summary is clinically relevant?\")\n",
    "print(\"  → Interpretation: Higher = fewer irrelevant or hallucinated details\\n\")\n",
    "\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(\"  → Measures: How much of the reference summary is captured?\")\n",
    "print(\"  → Interpretation: Higher = more complete, captures more medical entities\")\n",
    "print(\"  → THIS IS YOUR PRIMARY METRIC FOR HIGH RECALL!\\n\")\n",
    "\n",
    "print(f\"Average F1: {avg_f1:.4f}\")\n",
    "print(\"  → Measures: Harmonic mean of precision and recall\")\n",
    "print(\"  → Interpretation: Balanced measure of overall quality\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nPer-sample scores:\")\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"\\nSample {i + 1}:\")\n",
    "    print(f\"  Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"  F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nWHY CLINICAL BERTSCORE?\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Standard metrics like BLEU and ROUGE only measure word overlap, which fails for medical text:\n",
    "\n",
    "Example:\n",
    "  Reference: \"Patient had myocardial infarction with ST elevations\"\n",
    "  Candidate: \"Patient experienced heart attack with ST segment elevation\"\n",
    "  \n",
    "  BLEU/ROUGE: Low score (different words)\n",
    "  Clinical BERTScore: High score (same medical meaning)\n",
    "\n",
    "Bio_ClinicalBERT was pre-trained on 2 million clinical notes from MIMIC-III,\n",
    "so it understands medical synonyms, abbreviations, and clinical context.\n",
    "\n",
    "This makes BERTScore with Bio_ClinicalBERT the gold standard for evaluating\n",
    "clinical text generation tasks like discharge summarization.\n",
    "\"\"\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Qualitative Analysis\n",
    "\n",
    "Let's examine the actual generated summaries to qualitatively assess how well the model captures medical entities and details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DISPLAY PREDICTIONS VS REFERENCES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUALITATIVE ANALYSIS: Generated vs Reference Summaries\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"EXAMPLE {i + 1}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "    print(\"INPUT (Clinical Notes):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(test_dataset[i][\"input\"][:500] + \"...\\n\")  # Show first 500 chars\n",
    "\n",
    "    print(\"REFERENCE SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(references[i])\n",
    "    print()\n",
    "\n",
    "    print(\"GENERATED SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(predictions[i])\n",
    "    print()\n",
    "\n",
    "    print(\"SCORES:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"END OF QUALITATIVE ANALYSIS\")\n",
    "print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "EVALUATION CHECKLIST FOR HIGH RECALL:\n",
    "□ Are all diagnoses mentioned?\n",
    "□ Are all medications listed with dosages?\n",
    "□ Are vital signs included?\n",
    "□ Are abnormal lab results reported?\n",
    "□ Are procedures and treatments described?\n",
    "□ Are follow-up instructions present?\n",
    "□ Is the timeline/hospital course clear?\n",
    "\n",
    "If any of these are missing, consider:\n",
    "1. Training for more epochs\n",
    "2. Increasing MAX_NEW_TOKENS for generation\n",
    "3. Using more training data\n",
    "4. Adjusting prompt engineering to emphasize completeness\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Inference Function for New Clinical Notes\n",
    "\n",
    "This function allows you to generate summaries for new clinical notes using your fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INFERENCE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_discharge_summary(\n",
    "        clinical_notes: str,\n",
    "        instruction: str = \"Summarize the following clinical discharge notes. Include all diagnoses, medications, vitals, and significant findings.\",\n",
    "        max_new_tokens: int = MAX_NEW_TOKENS,\n",
    "        temperature: float = TEMPERATURE,\n",
    "        top_p: float = TOP_P,\n",
    "        top_k: int = TOP_K,\n",
    "        repetition_penalty: float = REPETITION_PENALTY,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a discharge summary from clinical notes using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        clinical_notes: Raw clinical notes as a string\n",
    "        instruction: Task instruction (default is optimized for completeness)\n",
    "        max_new_tokens: Maximum length of generated summary\n",
    "        temperature: Sampling temperature (higher = more creative)\n",
    "        top_p: Nucleus sampling parameter\n",
    "        top_k: Top-K sampling parameter\n",
    "        repetition_penalty: Penalty for repeating tokens\n",
    "    \n",
    "    Returns:\n",
    "        Generated discharge summary as a string\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the prompt using Gemma's template\n",
    "    inference_prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{clinical_notes}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the model's response\n",
    "    model_response_marker = \"<start_of_turn>model\"\n",
    "    if model_response_marker in generated_text:\n",
    "        summary = generated_text.split(model_response_marker)[-1].strip()\n",
    "    else:\n",
    "        summary = generated_text[len(inference_prompt):].strip()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "print(\"✓ Inference function defined\")\n",
    "print(\"\\nUsage example:\")\n",
    "print(\"  summary = generate_discharge_summary(clinical_notes)\")\n",
    "print(\"  print(summary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEST THE INFERENCE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "# Example: Generate a summary for a new clinical note\n",
    "sample_clinical_note = \"\"\"45-year-old female with history of asthma presented to ED with acute dyspnea and wheezing.\n",
    "Vitals: BP 118/76, HR 110, RR 28, O2 sat 89% on RA improved to 95% on 4L NC, Temp 98.4F.\n",
    "Patient reports missed doses of controller inhaler. Exam notable for diffuse expiratory wheezes.\n",
    "Peak flow 40% of predicted. Treated with continuous albuterol nebulizers, IV methylprednisolone 125mg,\n",
    "and magnesium sulfate 2g IV. Clinical improvement noted within 2 hours. Transitioned to albuterol q4h.\n",
    "Discharge on prednisone 40mg daily x 5 days, continue home fluticasone/salmeterol, albuterol PRN.\n",
    "Follow-up with pulmonology in 1 week. Patient educated on importance of daily controller medication.\"\"\"\n",
    "\n",
    "print(\"Generating summary for new clinical note...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "generated_summary = generate_discharge_summary(sample_clinical_note)\n",
    "\n",
    "print(\"GENERATED DISCHARGE SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(generated_summary)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✓ Summary generated successfully!\")\n",
    "print(\"\\nYou can now use this function to generate summaries for any new clinical notes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Saving and Loading Instructions\n",
    "\n",
    "Important notes on how to save and reload your fine-tuned model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HOW TO RELOAD YOUR FINE-TUNED MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "=============================================================================\n",
    "SAVING AND LOADING YOUR FINE-TUNED MODEL\n",
    "=============================================================================\n",
    "\n",
    "Your fine-tuned model has been saved to: {}\n",
    "\n",
    "This directory contains:\n",
    "  - adapter_config.json: LoRA configuration\n",
    "  - adapter_model.bin: Trained LoRA weights (~few MB)\n",
    "  - Tokenizer files\n",
    "\n",
    "TO RELOAD THE MODEL IN A NEW SESSION:\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "1. Install dependencies:\n",
    "   pip install transformers peft bitsandbytes torch\n",
    "\n",
    "2. Load the base model with quantization:\n",
    "   \n",
    "   from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "   from peft import PeftModel\n",
    "   import torch\n",
    "   \n",
    "   # Quantization config\n",
    "   bnb_config = BitsAndBytesConfig(\n",
    "       load_in_4bit=True,\n",
    "       bnb_4bit_quant_type=\"nf4\",\n",
    "       bnb_4bit_use_double_quant=True,\n",
    "       bnb_4bit_compute_dtype=torch.float16\n",
    "   )\n",
    "   \n",
    "   # Load base model\n",
    "   base_model = AutoModelForCausalLM.from_pretrained(\n",
    "       \"{}\",\n",
    "       quantization_config=bnb_config,\n",
    "       device_map=\"auto\",\n",
    "       trust_remote_code=True,\n",
    "   )\n",
    "   \n",
    "   # Load LoRA adapters\n",
    "   model = PeftModel.from_pretrained(base_model, \"{}\")\n",
    "   \n",
    "   # Load tokenizer\n",
    "   tokenizer = AutoTokenizer.from_pretrained(\"{}\")\n",
    "   \n",
    "3. Use the generate_discharge_summary() function defined above\n",
    "\n",
    "ALTERNATIVE: Merge adapters into base model (for deployment)\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "If you want a standalone model without separate adapter files:\n",
    "\n",
    "   # After loading model with PeftModel.from_pretrained():\n",
    "   model = model.merge_and_unload()\n",
    "   model.save_pretrained(\"./merged_model\")\n",
    "   tokenizer.save_pretrained(\"./merged_model\")\n",
    "   \n",
    "   # This creates a single model with adapters merged in\n",
    "   # Can be loaded like a regular Hugging Face model\n",
    "\n",
    "=============================================================================\n",
    "\"\"\".format(output_dir_final, MODEL_NAME, output_dir_final, output_dir_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps and Improvements\n",
    "\n",
    "Recommendations for improving model performance and achieving higher recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "=============================================================================\n",
    "NEXT STEPS FOR IMPROVING HIGH RECALL PERFORMANCE\n",
    "=============================================================================\n",
    "\n",
    "1. DATA IMPROVEMENTS:\n",
    "   □ Collect more training examples (aim for 1000+ samples)\n",
    "   □ Ensure reference summaries are comprehensive and capture all entities\n",
    "   □ Add data augmentation (paraphrasing, entity variations)\n",
    "   □ Balance dataset across different clinical scenarios\n",
    "\n",
    "2. PROMPT ENGINEERING:\n",
    "   □ Experiment with more explicit instructions:\n",
    "     \"List ALL diagnoses, medications, vitals, lab results, procedures...\"\n",
    "   □ Add structured output format in prompt:\n",
    "     \"Include sections: Diagnoses, Medications, Vitals, Labs, Procedures...\"\n",
    "   □ Provide few-shot examples in the prompt\n",
    "\n",
    "3. HYPERPARAMETER TUNING:\n",
    "   □ Increase MAX_NEW_TOKENS (current: {}) to allow longer summaries\n",
    "   □ Lower temperature (current: {}) for more deterministic outputs\n",
    "   □ Train for more epochs if not overfitting\n",
    "   □ Increase LoRA rank (current: {}) for more capacity\n",
    "\n",
    "4. TRAINING IMPROVEMENTS:\n",
    "   □ Use larger batch size if memory allows\n",
    "   □ Implement custom loss that penalizes missing entities\n",
    "   □ Add entity extraction as auxiliary task during training\n",
    "   □ Use curriculum learning (easy → hard examples)\n",
    "\n",
    "5. POST-PROCESSING:\n",
    "   □ Add entity extraction to verify all entities are present\n",
    "   □ Implement retrieval-augmented generation (RAG) to ensure completeness\n",
    "   □ Use template-based post-processing to enforce structure\n",
    "\n",
    "6. EVALUATION:\n",
    "   □ Create entity-level recall metrics (diagnoses, meds, vitals)\n",
    "   □ Manual clinical review by domain experts\n",
    "   □ Compare against baseline models (GPT-4, Claude, etc.)\n",
    "   □ A/B testing with clinicians\n",
    "\n",
    "7. ALTERNATIVE APPROACHES:\n",
    "   □ Try extractive + abstractive hybrid approach\n",
    "   □ Use larger model (7B or 13B parameters)\n",
    "   □ Fine-tune specialized medical models (BioGPT, ClinicalGPT)\n",
    "   □ Multi-stage generation (extract entities → generate summary)\n",
    "\n",
    "8. DEPLOYMENT CONSIDERATIONS:\n",
    "   □ Implement confidence scores for generated summaries\n",
    "   □ Add human-in-the-loop review system\n",
    "   □ Monitor for hallucinations and factual errors\n",
    "   □ Ensure HIPAA compliance and data privacy\n",
    "\n",
    "=============================================================================\n",
    "\n",
    "CURRENT CONFIGURATION SUMMARY:\n",
    "  Model: {}\n",
    "  LoRA Rank: {}\n",
    "  Training Epochs: {}\n",
    "  Max Generation Length: {} tokens\n",
    "  Temperature: {}\n",
    "  \n",
    "  Clinical BERTScore Results:\n",
    "    - Precision: {:.4f}\n",
    "    - Recall: {:.4f} ← PRIMARY METRIC\n",
    "    - F1: {:.4f}\n",
    "\n",
    "TARGET RECALL: Aim for ≥0.90 for production use\n",
    "\n",
    "=============================================================================\n",
    "\"\"\".format(\n",
    "    MAX_NEW_TOKENS,\n",
    "    TEMPERATURE,\n",
    "    LORA_R,\n",
    "    MODEL_NAME,\n",
    "    LORA_R,\n",
    "    NUM_EPOCHS,\n",
    "    MAX_NEW_TOKENS,\n",
    "    TEMPERATURE,\n",
    "    avg_precision,\n",
    "    avg_recall,\n",
    "    avg_f1\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ **Environment setup** with all required libraries\n",
    "2. ✅ **Model loading** with QLoRA (4-bit quantization)\n",
    "3. ✅ **LoRA configuration** for efficient fine-tuning\n",
    "4. ✅ **Data formatting** with Gemma prompt template\n",
    "5. ✅ **Training** with SFTTrainer\n",
    "6. ✅ **Clinical BERTScore evaluation** using Bio_ClinicalBERT\n",
    "7. ✅ **Inference function** for generating new summaries\n",
    "8. ✅ **Comprehensive documentation** for your project report\n",
    "\n",
    "**Key Takeaways:**\n",
    "- QLoRA enables fine-tuning large models on consumer GPUs\n",
    "- Clinical BERTScore is the appropriate metric for medical text\n",
    "- High recall requires careful prompt engineering and sufficient training data\n",
    "- The fine-tuned model can be easily saved and reloaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai-semester-project-patient-hospital-discharge-summaries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
