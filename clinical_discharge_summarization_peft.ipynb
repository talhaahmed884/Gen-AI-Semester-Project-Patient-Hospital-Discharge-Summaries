{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Discharge Summarization using MedGemma 4B with QLoRA\n",
    "\n",
    "**Project Overview:**\n",
    "This notebook demonstrates Parameter-Efficient Fine-Tuning (PEFT) using QLoRA on the MedGemma 4B model for clinical discharge summarization. The objective is to achieve **high recall** - generating detailed, verbose summaries that capture all medical entities (diagnoses, medications, vitals, abnormal lab results) from source clinical notes.\n",
    "\n",
    "**Key Technologies:**\n",
    "- Model: google/medgemma-4b (or base Gemma-4b)\n",
    "- Technique: QLoRA (4-bit quantization)\n",
    "- Evaluation: Clinical BERTScore using Bio_ClinicalBERT\n",
    "- Platform: Google Colab / Consumer GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we install all necessary libraries for model loading, quantization, fine-tuning, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# transformers: Hugging Face library for loading pre-trained models and tokenizers\n",
    "# peft: Parameter-Efficient Fine-Tuning library for LoRA adapters\n",
    "# bitsandbytes: Enables 4-bit/8-bit quantization for memory efficiency\n",
    "# trl: Transformer Reinforcement Learning library with SFTTrainer for supervised fine-tuning\n",
    "# accelerate: Distributed training and mixed precision support\n",
    "# datasets: For loading and processing datasets\n",
    "# bert_score: For computing BERTScore with clinical models\n",
    "# scipy: Required dependency for bert_score\n",
    "\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U bert_score\n",
    "!pip install -q -U scipy\n",
    "!pip install -q -U einops  # Required for Gemma model architecture\n",
    "\n",
    "print(\"✓ All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert_score import BERTScorer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters\n",
    "\n",
    "Define all model paths, LoRA parameters, and training hyperparameters in one place for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# NOTE: If google/medgemma-4b is not publicly available, use \"google/gemma-2-4b-it\" \n",
    "# or \"google/gemma-7b-it\" as the base model. The medical variant should be similar.\n",
    "MODEL_NAME = \"google/medgemma-4b-it\"  # Update to \"google/medgemma-4b\" when available\n",
    "\n",
    "# ============================================================================\n",
    "# LORA CONFIGURATION\n",
    "# ============================================================================\n",
    "# These parameters control the LoRA adapter architecture:\n",
    "# - r (rank): The dimensionality of the low-rank matrices. Higher = more parameters = better fit but more memory\n",
    "# - lora_alpha: Scaling factor for LoRA updates. Higher alpha = larger learning rate for LoRA weights\n",
    "# - lora_dropout: Dropout probability for LoRA layers to prevent overfitting\n",
    "\n",
    "LORA_R = 32  # Rank of 32 provides good balance between performance and memory\n",
    "LORA_ALPHA = 64  # Alpha = 2*r is a common heuristic\n",
    "LORA_DROPOUT = 0.05  # Small dropout for regularization\n",
    "\n",
    "# Target modules for Gemma architecture\n",
    "# These are the attention and MLP projection layers where LoRA adapters will be inserted\n",
    "# Gemma uses a standard transformer architecture with:\n",
    "# - q_proj, k_proj, v_proj: Query, Key, Value projections in attention\n",
    "# - o_proj: Output projection after attention\n",
    "# - gate_proj, up_proj, down_proj: MLP layers (Gemma uses SwiGLU activation)\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\", \n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\"\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "OUTPUT_DIR = \"./medgemma-discharge-summarization\"\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 2  # Per device batch size (increase if you have more VRAM)\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS = 8\n",
    "LEARNING_RATE = 2e-4  # Standard learning rate for LoRA fine-tuning\n",
    "MAX_SEQ_LENGTH = 2048  # Maximum sequence length (Gemma supports up to 8192, but we use 2048 for memory efficiency)\n",
    "WARMUP_STEPS = 100  # Warmup steps for learning rate scheduler\n",
    "LOGGING_STEPS = 10  # Log training metrics every N steps\n",
    "SAVE_STEPS = 100  # Save checkpoint every N steps\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION PARAMETERS (for high recall)\n",
    "# ============================================================================\n",
    "# These parameters are tuned to maximize detail and completeness in summaries:\n",
    "MAX_NEW_TOKENS = 512  # Allow longer summaries to capture all details\n",
    "TEMPERATURE = 0.7  # Moderate temperature for balance between creativity and coherence\n",
    "TOP_P = 0.9  # Nucleus sampling for diverse but relevant outputs\n",
    "TOP_K = 50  # Top-K sampling\n",
    "REPETITION_PENALTY = 1.1  # Slight penalty to avoid repetitive text\n",
    "\n",
    "print(\"✓ Configuration loaded successfully!\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA Rank: {LORA_R}, Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Training: {NUM_EPOCHS} epochs, Batch Size: {BATCH_SIZE}, Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Load and Prepare Dataset\n\nThis section provides TWO options for loading data:\n\n### **Option A (Recommended): Load Your Actual MIMIC Dataset**\n- File: `mimic_cleaned_text_only.csv`\n- Columns: `final_input` (clinical notes), `final_target` (reference summary)\n\n### **Option B: Use Sample Data**\n- For demonstration and testing purposes\n- Contains 3 realistic clinical examples\n\n**Instructions:**\n- If you have the MIMIC dataset file, use **Section 3A** below\n- Otherwise, skip to **Section 3B** to get started with sample data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# SAMPLE DATA CREATION (USE ONLY IF YOU DON'T HAVE MIMIC DATASET)\n# ============================================================================\n\n# IMPORTANT: Skip this cell if you already ran Section 3A successfully\n# This is fallback data for demonstration and testing purposes\n\nprint(\"Using sample data for demonstration...\\n\")\nprint(\"⚠ WARNING: This is limited sample data with only 3 examples.\")\nprint(\"  For actual training, use Section 3A with your MIMIC dataset.\\n\")\n\nsample_data = {\n    \"instruction\": [\n        \"Summarize the following clinical discharge notes. Include all diagnoses, medications, vitals, and significant findings.\",\n        \"Generate a comprehensive discharge summary from the following clinical notes. Ensure all medical entities are captured.\",\n        \"Create a detailed discharge summary including all diagnoses, treatments, medications, and follow-up instructions.\"\n    ],\n    \"input\": [\n        \"\"\"Patient is a 65-year-old male with history of hypertension and type 2 diabetes mellitus who presented to the ED with chest pain. \n        Vital signs on admission: BP 156/92, HR 88, RR 18, O2 sat 96% on RA, Temp 98.6F. \n        ECG showed ST elevations in leads II, III, aVF. Troponin elevated at 2.4 ng/mL. \n        Patient underwent emergent cardiac catheterization revealing 95% occlusion of RCA. \n        Drug-eluting stent placed successfully. Post-procedure course uncomplicated. \n        Started on aspirin 81mg daily, clopidogrel 75mg daily, atorvastatin 80mg daily, metoprolol 25mg BID. \n        Blood glucose controlled with insulin sliding scale. HbA1c 8.2%. \n        Discharge vitals: BP 128/78, HR 72, stable. Patient educated on medication compliance and lifestyle modifications.\"\"\",\n        \n        \"\"\"72-year-old female admitted with acute exacerbation of COPD. \n        History significant for 50 pack-year smoking history, chronic bronchitis, hypertension. \n        Presented with increased dyspnea, productive cough with yellow sputum, wheezing. \n        Vitals: BP 142/88, HR 102, RR 24, O2 sat 88% on RA improved to 94% on 2L NC, Temp 100.8F. \n        Chest X-ray showed hyperinflation, no infiltrates. ABG: pH 7.32, pCO2 58, pO2 62, HCO3 28. \n        Treated with nebulized albuterol/ipratropium q4h, IV methylprednisolone 125mg q6h tapered to prednisone 40mg PO daily, \n        azithromycin 500mg x1 then 250mg daily x 4 days. Clinical improvement noted by day 3. \n        Discharge on prednisone taper, continue home inhalers: tiotropium, fluticasone/salmeterol. \n        Smoking cessation counseling provided. Follow-up with pulmonology in 2 weeks.\"\"\",\n        \n        \"\"\"58-year-old male with no significant past medical history presented with sudden onset severe headache (\"worst headache of my life\"), \n        nausea, vomiting, photophobia. Neurological exam revealed nuchal rigidity, positive Kernig's sign. \n        Vitals: BP 168/95, HR 78, RR 16, Temp 99.2F, O2 sat 99% RA. \n        CT head non-contrast showed subarachnoid hemorrhage in basal cisterns. \n        Neurosurgery consulted. CT angiogram revealed 7mm anterior communicating artery aneurysm. \n        Patient underwent successful endovascular coiling on hospital day 2. \n        Post-procedure monitoring in ICU for vasospasm prevention with nimodipine 60mg q4h, \n        maintaining systolic BP 140-160. No neurological deficits noted. \n        Started on levetiracetam 500mg BID for seizure prophylaxis. Discharge on day 7 with outpatient neurosurgery follow-up.\"\"\"\n    ],\n    \"output\": [\n        \"\"\"DISCHARGE SUMMARY:\n        Primary Diagnosis: ST-Elevation Myocardial Infarction (STEMI) - Inferior wall\n        Secondary Diagnoses: Hypertension, Type 2 Diabetes Mellitus\n        \n        Hospital Course: 65-year-old male admitted with chest pain and ECG changes consistent with inferior STEMI. \n        Elevated troponin (2.4 ng/mL). Emergency cardiac catheterization revealed 95% RCA occlusion, successfully treated with drug-eluting stent placement. \n        Post-procedure recovery uncomplicated.\n        \n        Vitals: Admission BP 156/92, discharge BP 128/78. HR improved from 88 to 72. Remained afebrile.\n        \n        Medications at Discharge:\n        - Aspirin 81mg daily (antiplatelet)\n        - Clopidogrel 75mg daily (antiplatelet, continue for 12 months minimum)\n        - Atorvastatin 80mg daily (high-intensity statin)\n        - Metoprolol 25mg twice daily (beta-blocker)\n        - Continue home diabetes medications, insulin adjustments made\n        \n        Labs: HbA1c 8.2% - diabetes management needs optimization.\n        \n        Follow-up: Cardiology in 1 week, Primary care in 2 weeks for diabetes management.\n        Patient counseled on medication adherence, cardiac rehabilitation, smoking cessation if applicable, diet modification.\"\"\",\n        \n        \"\"\"DISCHARGE SUMMARY:\n        Primary Diagnosis: Acute Exacerbation of Chronic Obstructive Pulmonary Disease (COPD)\n        Secondary Diagnoses: Chronic bronchitis, Hypertension, Tobacco use disorder (50 pack-years)\n        \n        Hospital Course: 72-year-old female with COPD admitted for acute exacerbation with increased dyspnea, productive cough, hypoxemia. \n        Initial O2 saturation 88% on room air, improved to 94% on 2L nasal cannula. Chest X-ray showed hyperinflation without infiltrates. \n        ABG revealed respiratory acidosis (pH 7.32, pCO2 58).\n        \n        Treatment: Aggressive bronchodilator therapy (albuterol/ipratropium nebulizers q4h), systemic corticosteroids \n        (IV methylprednisolone 125mg q6h transitioned to oral prednisone 40mg daily), antibiotics (azithromycin 5-day course). \n        Clinical improvement by hospital day 3.\n        \n        Vital Signs: Admission - BP 142/88, HR 102, RR 24, Temp 100.8F. Improved to normal by discharge.\n        \n        Medications at Discharge:\n        - Prednisone 40mg daily (taper: 40mg x 3 days, 20mg x 3 days, 10mg x 3 days, then stop)\n        - Tiotropium (continue home LAMA inhaler)\n        - Fluticasone/salmeterol (continue home ICS/LABA inhaler)\n        - Continue home antihypertensive medications\n        \n        Follow-up: Pulmonology in 2 weeks. Smoking cessation counseling provided - patient receptive to quitting. \n        Consider pulmonary rehabilitation referral.\"\"\",\n        \n        \"\"\"DISCHARGE SUMMARY:\n        Primary Diagnosis: Subarachnoid Hemorrhage (SAH) secondary to ruptured anterior communicating artery aneurysm\n        \n        Hospital Course: 58-year-old male presented with sudden severe headache (\"thunderclap\"), nuchal rigidity, positive meningeal signs. \n        Non-contrast head CT confirmed subarachnoid hemorrhage. CT angiogram identified 7mm AComm artery aneurysm. \n        Successful endovascular coiling performed on hospital day 2 by neurosurgery.\n        \n        ICU Monitoring: Post-procedure vasospasm prevention protocol initiated with nimodipine 60mg q4h. \n        Blood pressure maintained in target range (systolic 140-160). No delayed cerebral ischemia or neurological deficits observed. \n        Serial neurological exams remained normal.\n        \n        Vital Signs: Admission BP 168/95 (controlled post-procedure), HR 78, afebrile throughout stay.\n        \n        Medications at Discharge:\n        - Nimodipine 60mg every 4 hours (continue for 21 days post-hemorrhage for vasospasm prevention)\n        - Levetiracetam 500mg twice daily (seizure prophylaxis)\n        \n        Follow-up: Neurosurgery clinic in 1 week. Repeat CT angiogram in 6 months to assess aneurysm coiling stability. \n        Patient and family educated on warning signs of rebleeding, vasospasm (new headache, confusion, focal deficits). \n        No driving for 6 months per neurosurgery recommendations. Excellent prognosis given successful intervention and no complications.\"\"\"\n    ]\n}\n\n# Convert to pandas DataFrame then to Hugging Face Dataset\ndf = pd.DataFrame(sample_data)\ndataset = Dataset.from_pandas(df)\n\n# Split into train and test sets (80/20 split)\n# In production, you should have a separate validation set as well\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = dataset[\"train\"]\ntest_dataset = dataset[\"test\"]\n\nprint(f\"✓ Sample dataset created!\")\nprint(f\"  Training samples: {len(train_dataset)}\")\nprint(f\"  Test samples: {len(test_dataset)}\")\nprint(f\"\\nSample training example:\")\nprint(f\"  Instruction: {train_dataset[0]['instruction'][:100]}...\")\nprint(f\"  Input length: {len(train_dataset[0]['input'])} characters\")\nprint(f\"  Output length: {len(train_dataset[0]['output'])} characters\")\nprint(\"\\n⚠ Remember: Use your actual MIMIC dataset (Section 3A) for real training!\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 3B. Sample Data (Alternative Option)\n\n**Use this section ONLY if you don't have the MIMIC dataset file**\n\nThis creates sample data for testing and demonstration purposes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# LOAD ACTUAL MIMIC DATASET FROM CSV\n# ============================================================================\n\n# IMPORTANT: Run this cell if you have the mimic_cleaned_text_only.csv file\n# If you don't have the file, skip to Section 3B for sample data\n\nimport os\n\n# Path to your MIMIC dataset CSV file\n# Adjust this path if your file is located elsewhere\nMIMIC_CSV_PATH = \"mimic_cleaned_text_only.csv\"\n\n# Check if the file exists\nif os.path.exists(MIMIC_CSV_PATH):\n    print(f\"Loading MIMIC dataset from: {MIMIC_CSV_PATH}\\n\")\n    \n    # Load the CSV file using pandas\n    # The file should have two columns: final_input and final_target\n    mimic_df = pd.read_csv(MIMIC_CSV_PATH)\n    \n    print(f\"✓ Dataset loaded successfully!\")\n    print(f\"  Total samples: {len(mimic_df)}\")\n    print(f\"  Columns: {list(mimic_df.columns)}\\n\")\n    \n    # Display basic statistics\n    print(\"Dataset Statistics:\")\n    print(f\"  Average input length: {mimic_df['final_input'].str.len().mean():.0f} characters\")\n    print(f\"  Average target length: {mimic_df['final_target'].str.len().mean():.0f} characters\")\n    print(f\"  Minimum input length: {mimic_df['final_input'].str.len().min():.0f} characters\")\n    print(f\"  Maximum input length: {mimic_df['final_input'].str.len().max():.0f} characters\")\n    \n    # Add a consistent instruction column\n    # This instruction emphasizes HIGH RECALL - capturing all medical details\n    instruction_text = \"Summarize the following clinical discharge notes. Include ALL diagnoses, medications, vitals, lab results, procedures, and follow-up instructions. Ensure complete coverage of all medical entities.\"\n    mimic_df['instruction'] = instruction_text\n    \n    # Rename columns to match the expected format\n    # final_input → input (clinical notes)\n    # final_target → output (reference summary)\n    mimic_df = mimic_df.rename(columns={\n        'final_input': 'input',\n        'final_target': 'output'\n    })\n    \n    # Remove any rows with missing data\n    initial_count = len(mimic_df)\n    mimic_df = mimic_df.dropna(subset=['input', 'output'])\n    dropped_count = initial_count - len(mimic_df)\n    \n    if dropped_count > 0:\n        print(f\"\\n⚠ Removed {dropped_count} rows with missing data\")\n    \n    # Convert to Hugging Face Dataset\n    dataset = Dataset.from_pandas(mimic_df[['instruction', 'input', 'output']])\n    \n    # Split into train and test sets\n    # Using 90/10 split since we have a larger dataset\n    # Adjust test_size as needed (e.g., 0.15 for 85/15 split)\n    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = dataset[\"train\"]\n    test_dataset = dataset[\"test\"]\n    \n    print(f\"\\n✓ Dataset prepared and split!\")\n    print(f\"  Training samples: {len(train_dataset)}\")\n    print(f\"  Test samples: {len(test_dataset)}\")\n    \n    # Display a sample from the training set\n    print(f\"\\n{'='*80}\")\n    print(\"SAMPLE TRAINING EXAMPLE:\")\n    print(f\"{'='*80}\\n\")\n    print(f\"Instruction: {train_dataset[0]['instruction'][:150]}...\")\n    print(f\"\\nInput (first 300 chars):\\n{train_dataset[0]['input'][:300]}...\")\n    print(f\"\\nOutput (first 300 chars):\\n{train_dataset[0]['output'][:300]}...\")\n    print(f\"\\n{'='*80}\")\n    \n    print(\"\\n✓ MIMIC dataset loaded successfully! You can now skip Section 3B.\")\n    print(\"  Proceed to Section 4 (Load Model with 4-bit Quantization)\")\n    \nelse:\n    print(f\"⚠ File not found: {MIMIC_CSV_PATH}\")\n    print(f\"\\nPlease either:\")\n    print(f\"  1. Place the mimic_cleaned_text_only.csv file in the current directory\")\n    print(f\"  2. Update MIMIC_CSV_PATH variable with the correct file path\")\n    print(f\"  3. Skip to Section 3B to use sample data instead\\n\")\n    print(f\"Current directory: {os.getcwd()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3A. Load Your Actual MIMIC Dataset (RECOMMENDED)\n\n**Use this section if you have the `mimic_cleaned_text_only.csv` file**\n\nThis loads your actual MIMIC clinical discharge dataset with the correct column mappings:\n- `final_input` → clinical notes\n- `final_target` → reference summaries",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with 4-bit Quantization (QLoRA)\n",
    "\n",
    "QLoRA (Quantized LoRA) enables fine-tuning large models on consumer GPUs by:\n",
    "1. Loading the base model in 4-bit precision (NormalFloat 4-bit)\n",
    "2. Using double quantization to further reduce memory\n",
    "3. Computing gradients in float16 for numerical stability\n",
    "4. Training only LoRA adapter weights (a small fraction of total parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QUANTIZATION CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# BitsAndBytesConfig controls how the model is quantized:\n",
    "# - load_in_4bit: Enable 4-bit quantization (reduces memory by ~75% vs FP32)\n",
    "# - bnb_4bit_quant_type=\"nf4\": Use NormalFloat 4-bit (optimal for normally distributed weights)\n",
    "# - bnb_4bit_use_double_quant: Apply quantization to quantization constants (extra memory savings)\n",
    "# - bnb_4bit_compute_dtype: Data type for computations (float16 for speed, bfloat16 for stability)\n",
    "\n",
    "compute_dtype = torch.float16  # Use float16 for faster computation\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "print(\"✓ Quantization configuration created\")\n",
    "print(f\"  Quantization type: NF4 (4-bit NormalFloat)\")\n",
    "print(f\"  Double quantization: Enabled\")\n",
    "print(f\"  Compute dtype: {compute_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD TOKENIZER\n",
    "# ============================================================================\n",
    "\n",
    "# The tokenizer converts text to tokens (numbers) that the model can process\n",
    "# Important configurations:\n",
    "# - padding_side=\"right\": Pad sequences on the right (standard for causal LM)\n",
    "# - add_eos_token: Automatically add end-of-sequence token (important for Gemma)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",  # Right padding is standard for causal language models\n",
    "    add_eos_token=True,  # Ensure EOS token is added for proper sequence termination\n",
    ")\n",
    "\n",
    "# Set the padding token to be the same as EOS token\n",
    "# (Gemma models don't have a separate PAD token by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"✓ Tokenizer loaded successfully\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"  PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD MODEL WITH QUANTIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Load the base Gemma model with 4-bit quantization\n",
    "# This significantly reduces memory usage (4-bit vs 32-bit = 8x reduction)\n",
    "# making it possible to fine-tune on consumer GPUs\n",
    "\n",
    "print(\"Loading model... This may take a few minutes.\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,  # Apply 4-bit quantization\n",
    "    device_map=\"auto\",  # Automatically distribute model across available devices\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=compute_dtype,  # Use float16 for non-quantized layers\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "# This function:\n",
    "# 1. Freezes all base model weights (only LoRA adapters will be trained)\n",
    "# 2. Enables gradient checkpointing to save memory\n",
    "# 3. Prepares input embeddings for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "# This trades compute for memory by recomputing activations during backward pass\n",
    "model.config.use_cache = False  # Required for gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"✓ Model loaded successfully with 4-bit quantization\")\n",
    "print(f\"  Model type: {model.config.model_type}\")\n",
    "print(f\"  Number of parameters: {model.num_parameters() / 1e9:.2f}B\")\n",
    "print(f\"  Device map: {model.hf_device_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA Adapters\n",
    "\n",
    "LoRA (Low-Rank Adaptation) works by adding small trainable matrices to specific layers of the frozen base model. This dramatically reduces the number of trainable parameters while maintaining performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LORA CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# LoRA configuration parameters:\n",
    "# - r: Rank of the low-rank matrices (higher = more capacity but more parameters)\n",
    "# - lora_alpha: Scaling factor (controls magnitude of LoRA updates)\n",
    "# - target_modules: Which model layers to apply LoRA to\n",
    "# - lora_dropout: Dropout for regularization\n",
    "# - bias: Whether to train bias parameters (\"none\" is standard)\n",
    "# - task_type: Type of task (CAUSAL_LM for text generation)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",  # Don't train bias parameters\n",
    "    task_type=\"CAUSAL_LM\",  # Causal language modeling task\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "# This shows the massive parameter reduction achieved by LoRA\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percent = 100 * trainable_params / total_params\n",
    "\n",
    "print(\"✓ LoRA adapters configured and applied\")\n",
    "print(f\"  Target modules: {TARGET_MODULES}\")\n",
    "print(f\"  LoRA rank (r): {LORA_R}\")\n",
    "print(f\"  LoRA alpha: {LORA_ALPHA}\")\n",
    "print(f\"  LoRA dropout: {LORA_DROPOUT}\")\n",
    "print(f\"\\n  Trainable parameters: {trainable_params:,} ({trainable_percent:.2f}% of total)\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Memory savings: Training only {trainable_percent:.2f}% of parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Training Data with Gemma Prompt Format\n",
    "\n",
    "**Critical:** Gemma models use a specific prompt format with special tokens:\n",
    "- `<start_of_turn>user`: Indicates user input\n",
    "- `<end_of_turn>`: Marks end of turn\n",
    "- `<start_of_turn>model`: Indicates model output\n",
    "\n",
    "Using the correct format is essential for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GEMMA PROMPT FORMATTING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def format_prompt_gemma(sample):\n",
    "    \"\"\"\n",
    "    Format a training sample using Gemma's conversation template.\n",
    "    \n",
    "    Gemma uses a turn-based conversation format:\n",
    "    <start_of_turn>user\n",
    "    {instruction}\n",
    "    {input}\n",
    "    <end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    {output}<end_of_turn>\n",
    "    \n",
    "    Args:\n",
    "        sample: Dictionary containing 'instruction', 'input', and 'output' keys\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with formatted 'text' field\n",
    "    \"\"\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample[\"input\"]\n",
    "    output_text = sample[\"output\"]\n",
    "    \n",
    "    # Construct the full prompt using Gemma's format\n",
    "    # The user turn contains both the instruction and the clinical notes\n",
    "    # The model turn contains the expected summary output\n",
    "    full_prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{output_text}<end_of_turn>\"\"\"\n",
    "    \n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# Apply formatting to both train and test datasets\n",
    "train_dataset = train_dataset.map(format_prompt_gemma)\n",
    "test_dataset = test_dataset.map(format_prompt_gemma)\n",
    "\n",
    "print(\"✓ Dataset formatted with Gemma prompt template\")\n",
    "print(\"\\nExample formatted prompt (truncated):\")\n",
    "print(\"=\" * 80)\n",
    "print(train_dataset[0][\"text\"][:500])\n",
    "print(\"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Configuration and Trainer Setup\n",
    "\n",
    "Configure the training process using Hugging Face's `TrainingArguments` and the specialized `SFTTrainer` from the TRL library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING ARGUMENTS\n",
    "# ============================================================================\n",
    "\n",
    "# TrainingArguments control all aspects of the training loop:\n",
    "# Memory optimization:\n",
    "#   - per_device_train_batch_size: Batch size per GPU (keep low for memory)\n",
    "#   - gradient_accumulation_steps: Accumulate gradients over N steps (simulates larger batch)\n",
    "#   - gradient_checkpointing: Trade compute for memory\n",
    "#   - fp16: Use mixed precision training (faster + less memory)\n",
    "# \n",
    "# Optimization:\n",
    "#   - learning_rate: Step size for parameter updates\n",
    "#   - weight_decay: L2 regularization\n",
    "#   - warmup_steps: Gradually increase LR at start of training\n",
    "#   - lr_scheduler_type: How to adjust LR during training\n",
    "#   - optim: Optimizer choice (adamw_torch is standard)\n",
    "#\n",
    "# Logging and checkpointing:\n",
    "#   - logging_steps: How often to log metrics\n",
    "#   - save_steps: How often to save model checkpoints\n",
    "#   - evaluation_strategy: When to run evaluation\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch\",  # Standard AdamW optimizer\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,  # L2 regularization\n",
    "    fp16=True,  # Mixed precision training (use bf16 if your GPU supports it)\n",
    "    max_grad_norm=1.0,  # Gradient clipping to prevent exploding gradients\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine learning rate schedule\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,  # Keep only the 3 most recent checkpoints\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=SAVE_STEPS,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard (can enable if you want tracking)\n",
    "    push_to_hub=False,  # Don't push to Hugging Face Hub automatically\n",
    ")\n",
    "\n",
    "print(\"✓ Training arguments configured\")\n",
    "print(f\"  Total training steps: ~{len(train_dataset) * NUM_EPOCHS // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Warmup steps: {WARMUP_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE SUPERVISED FINE-TUNING TRAINER\n",
    "# ============================================================================\n",
    "\n",
    "# SFTTrainer (Supervised Fine-Tuning Trainer) from TRL library is specifically\n",
    "# designed for instruction fine-tuning of language models. It handles:\n",
    "# - Proper packing of sequences\n",
    "# - Masking of prompt tokens (only compute loss on completion)\n",
    "# - Memory-efficient training with large sequence lengths\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",  # Column containing formatted prompts\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,  # Set to True to pack multiple samples per sequence (more efficient but complex)\n",
    ")\n",
    "\n",
    "print(\"✓ SFTTrainer initialized successfully\")\n",
    "print(\"\\nTrainer is ready to begin fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fine-Tune the Model\n",
    "\n",
    "Now we train the model. This process will:\n",
    "1. Iterate through the training data for `NUM_EPOCHS` epochs\n",
    "2. Update only the LoRA adapter weights (not the base model)\n",
    "3. Log training metrics periodically\n",
    "4. Save checkpoints for recovery and evaluation\n",
    "\n",
    "**Note:** Training time depends on your GPU and dataset size. For the sample data, this should complete in a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# START TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Starting fine-tuning...\\n\")\n",
    "print(\"This will train for {} epochs with:\".format(NUM_EPOCHS))\n",
    "print(f\"  - {len(train_dataset)} training samples\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(\"\\nMonitor the loss below. For good convergence, loss should decrease steadily.\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Train the model\n",
    "# The trainer will handle:\n",
    "# - Forward pass (compute predictions)\n",
    "# - Loss computation (compare predictions to ground truth)\n",
    "# - Backward pass (compute gradients)\n",
    "# - Optimizer step (update LoRA weights)\n",
    "# - Logging and checkpointing\n",
    "training_output = trainer.train()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n✓ Training completed successfully!\")\n",
    "print(f\"\\nFinal training loss: {training_output.training_loss:.4f}\")\n",
    "print(f\"Total training time: {training_output.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Samples per second: {training_output.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE THE FINE-TUNED MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Save the trained LoRA adapters\n",
    "# Note: This saves only the adapter weights (~few MB), not the full model\n",
    "# To use the model later, you'll load the base model + these adapters\n",
    "\n",
    "output_dir_final = f\"{OUTPUT_DIR}/final\"\n",
    "trainer.model.save_pretrained(output_dir_final)\n",
    "tokenizer.save_pretrained(output_dir_final)\n",
    "\n",
    "print(f\"✓ Model saved to: {output_dir_final}\")\n",
    "print(\"\\nThe saved files include:\")\n",
    "print(\"  - adapter_config.json: LoRA configuration\")\n",
    "print(\"  - adapter_model.bin: Trained LoRA weights\")\n",
    "print(\"  - tokenizer files\")\n",
    "print(\"\\nTo load this model later, use:\")\n",
    "print(f\"  model = AutoModelForCausalLM.from_pretrained('{MODEL_NAME}', ...)\")\n",
    "print(f\"  model = PeftModel.from_pretrained(model, '{output_dir_final}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Clinical BERTScore Evaluation\n",
    "\n",
    "**Why Clinical BERTScore?**\n",
    "\n",
    "Traditional metrics like BLEU or ROUGE measure word overlap, which is insufficient for medical text where:\n",
    "- Synonyms are common (\"myocardial infarction\" = \"heart attack\")\n",
    "- Semantic equivalence matters more than exact wording\n",
    "- Clinical accuracy is critical\n",
    "\n",
    "**BERTScore** measures semantic similarity using contextual embeddings. By using **Bio_ClinicalBERT** (trained on clinical notes from MIMIC-III), we get embeddings that understand medical terminology and context.\n",
    "\n",
    "**Interpretation:**\n",
    "- Precision: How much of the generated summary is relevant?\n",
    "- Recall: How much of the reference summary is captured? (Our primary metric for completeness)\n",
    "- F1: Harmonic mean of precision and recall\n",
    "\n",
    "Scores range from 0 to 1, with higher being better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE CLINICAL BERTSCORE\n",
    "# ============================================================================\n",
    "\n",
    "# Create a BERTScorer with Bio_ClinicalBERT as the backbone\n",
    "# This model was trained on MIMIC-III clinical notes and understands medical language\n",
    "#\n",
    "# Important parameters:\n",
    "# - model_type: The BERT model to use for embeddings\n",
    "# - num_layers: Which layer's embeddings to use (9 is optimal for Bio_ClinicalBERT)\n",
    "# - rescale_with_baseline: Normalize scores using baseline statistics\n",
    "# - lang: Language (en for English)\n",
    "# - device: GPU if available, else CPU\n",
    "\n",
    "print(\"Initializing Clinical BERTScore...\")\n",
    "print(\"This will download emilyalsentzer/Bio_ClinicalBERT if not cached.\\n\")\n",
    "\n",
    "clinical_scorer = BERTScorer(\n",
    "    model_type=\"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    num_layers=9,  # Layer 9 has been found optimal for clinical text\n",
    "    rescale_with_baseline=True,\n",
    "    lang=\"en\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"✓ Clinical BERTScore initialized\")\n",
    "print(f\"  Model: emilyalsentzer/Bio_ClinicalBERT\")\n",
    "print(f\"  Device: {clinical_scorer.device}\")\n",
    "print(\"\\nThis model was trained on MIMIC-III clinical notes and understands:\")\n",
    "print(\"  - Medical terminology and abbreviations\")\n",
    "print(\"  - Clinical context and relationships\")\n",
    "print(\"  - Semantic equivalence in healthcare text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERATE PREDICTIONS ON TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Generating predictions on test set...\\n\")\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Generate predictions for each test example\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    print(f\"Generating summary {i+1}/{len(test_dataset)}...\")\n",
    "    \n",
    "    # Extract the input (clinical notes)\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample[\"input\"]\n",
    "    reference = sample[\"output\"]\n",
    "    \n",
    "    # Format the prompt for inference (same format as training, but without the model's response)\n",
    "    inference_prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate the summary\n",
    "    # Generation parameters are tuned for high recall (detailed outputs):\n",
    "    # - max_new_tokens: Allow long summaries\n",
    "    # - temperature: Control randomness (0.7 = moderate creativity)\n",
    "    # - top_p: Nucleus sampling for diverse but coherent text\n",
    "    # - do_sample: Enable sampling (vs greedy decoding)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens to text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the generated summary (remove the prompt)\n",
    "    # Find where the model's response starts\n",
    "    model_response_marker = \"<start_of_turn>model\"\n",
    "    if model_response_marker in generated_text:\n",
    "        generated_summary = generated_text.split(model_response_marker)[-1].strip()\n",
    "    else:\n",
    "        generated_summary = generated_text[len(inference_prompt):].strip()\n",
    "    \n",
    "    predictions.append(generated_summary)\n",
    "    references.append(reference)\n",
    "    \n",
    "    print(f\"  Generated {len(generated_summary)} characters\\n\")\n",
    "\n",
    "print(\"✓ All predictions generated\")\n",
    "print(f\"  Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPUTE CLINICAL BERTSCORE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Computing Clinical BERTScore...\\n\")\n",
    "print(\"This metric measures semantic similarity using Bio_ClinicalBERT embeddings.\")\n",
    "print(\"Unlike BLEU/ROUGE (word overlap), BERTScore captures:\")\n",
    "print(\"  - Semantic equivalence (synonyms, paraphrases)\")\n",
    "print(\"  - Clinical context and medical terminology\")\n",
    "print(\"  - Conceptual similarity beyond surface form\\n\")\n",
    "\n",
    "# Compute BERTScore\n",
    "# Returns three tensors: Precision, Recall, F1\n",
    "P, R, F1 = clinical_scorer.score(\n",
    "    cands=predictions,  # Generated summaries\n",
    "    refs=references,    # Reference summaries\n",
    ")\n",
    "\n",
    "# Convert to numpy for easier manipulation\n",
    "precision_scores = P.cpu().numpy()\n",
    "recall_scores = R.cpu().numpy()\n",
    "f1_scores = F1.cpu().numpy()\n",
    "\n",
    "# Compute averages\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLINICAL BERTSCORE RESULTS (using Bio_ClinicalBERT)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAverage Precision: {avg_precision:.4f}\")\n",
    "print(\"  → Measures: How much of the generated summary is clinically relevant?\")\n",
    "print(\"  → Interpretation: Higher = fewer irrelevant or hallucinated details\\n\")\n",
    "\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(\"  → Measures: How much of the reference summary is captured?\")\n",
    "print(\"  → Interpretation: Higher = more complete, captures more medical entities\")\n",
    "print(\"  → THIS IS YOUR PRIMARY METRIC FOR HIGH RECALL!\\n\")\n",
    "\n",
    "print(f\"Average F1: {avg_f1:.4f}\")\n",
    "print(\"  → Measures: Harmonic mean of precision and recall\")\n",
    "print(\"  → Interpretation: Balanced measure of overall quality\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nPer-sample scores:\")\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"  F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nWHY CLINICAL BERTSCORE?\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Standard metrics like BLEU and ROUGE only measure word overlap, which fails for medical text:\n",
    "\n",
    "Example:\n",
    "  Reference: \"Patient had myocardial infarction with ST elevations\"\n",
    "  Candidate: \"Patient experienced heart attack with ST segment elevation\"\n",
    "  \n",
    "  BLEU/ROUGE: Low score (different words)\n",
    "  Clinical BERTScore: High score (same medical meaning)\n",
    "\n",
    "Bio_ClinicalBERT was pre-trained on 2 million clinical notes from MIMIC-III,\n",
    "so it understands medical synonyms, abbreviations, and clinical context.\n",
    "\n",
    "This makes BERTScore with Bio_ClinicalBERT the gold standard for evaluating\n",
    "clinical text generation tasks like discharge summarization.\n",
    "\"\"\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Qualitative Analysis\n",
    "\n",
    "Let's examine the actual generated summaries to qualitatively assess how well the model captures medical entities and details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DISPLAY PREDICTIONS VS REFERENCES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUALITATIVE ANALYSIS: Generated vs Reference Summaries\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"EXAMPLE {i+1}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    print(\"INPUT (Clinical Notes):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(test_dataset[i][\"input\"][:500] + \"...\\n\")  # Show first 500 chars\n",
    "    \n",
    "    print(\"REFERENCE SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(references[i])\n",
    "    print()\n",
    "    \n",
    "    print(\"GENERATED SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(predictions[i])\n",
    "    print()\n",
    "    \n",
    "    print(\"SCORES:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"F1: {f1_scores[i]:.4f}\")\n",
    "    \n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"END OF QUALITATIVE ANALYSIS\")\n",
    "print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "EVALUATION CHECKLIST FOR HIGH RECALL:\n",
    "□ Are all diagnoses mentioned?\n",
    "□ Are all medications listed with dosages?\n",
    "□ Are vital signs included?\n",
    "□ Are abnormal lab results reported?\n",
    "□ Are procedures and treatments described?\n",
    "□ Are follow-up instructions present?\n",
    "□ Is the timeline/hospital course clear?\n",
    "\n",
    "If any of these are missing, consider:\n",
    "1. Training for more epochs\n",
    "2. Increasing MAX_NEW_TOKENS for generation\n",
    "3. Using more training data\n",
    "4. Adjusting prompt engineering to emphasize completeness\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Inference Function for New Clinical Notes\n",
    "\n",
    "This function allows you to generate summaries for new clinical notes using your fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INFERENCE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_discharge_summary(\n",
    "    clinical_notes: str,\n",
    "    instruction: str = \"Summarize the following clinical discharge notes. Include all diagnoses, medications, vitals, and significant findings.\",\n",
    "    max_new_tokens: int = MAX_NEW_TOKENS,\n",
    "    temperature: float = TEMPERATURE,\n",
    "    top_p: float = TOP_P,\n",
    "    top_k: int = TOP_K,\n",
    "    repetition_penalty: float = REPETITION_PENALTY,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a discharge summary from clinical notes using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        clinical_notes: Raw clinical notes as a string\n",
    "        instruction: Task instruction (default is optimized for completeness)\n",
    "        max_new_tokens: Maximum length of generated summary\n",
    "        temperature: Sampling temperature (higher = more creative)\n",
    "        top_p: Nucleus sampling parameter\n",
    "        top_k: Top-K sampling parameter\n",
    "        repetition_penalty: Penalty for repeating tokens\n",
    "    \n",
    "    Returns:\n",
    "        Generated discharge summary as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format the prompt using Gemma's template\n",
    "    inference_prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{clinical_notes}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the model's response\n",
    "    model_response_marker = \"<start_of_turn>model\"\n",
    "    if model_response_marker in generated_text:\n",
    "        summary = generated_text.split(model_response_marker)[-1].strip()\n",
    "    else:\n",
    "        summary = generated_text[len(inference_prompt):].strip()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(\"✓ Inference function defined\")\n",
    "print(\"\\nUsage example:\")\n",
    "print(\"  summary = generate_discharge_summary(clinical_notes)\")\n",
    "print(\"  print(summary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEST THE INFERENCE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "# Example: Generate a summary for a new clinical note\n",
    "sample_clinical_note = \"\"\"45-year-old female with history of asthma presented to ED with acute dyspnea and wheezing.\n",
    "Vitals: BP 118/76, HR 110, RR 28, O2 sat 89% on RA improved to 95% on 4L NC, Temp 98.4F.\n",
    "Patient reports missed doses of controller inhaler. Exam notable for diffuse expiratory wheezes.\n",
    "Peak flow 40% of predicted. Treated with continuous albuterol nebulizers, IV methylprednisolone 125mg,\n",
    "and magnesium sulfate 2g IV. Clinical improvement noted within 2 hours. Transitioned to albuterol q4h.\n",
    "Discharge on prednisone 40mg daily x 5 days, continue home fluticasone/salmeterol, albuterol PRN.\n",
    "Follow-up with pulmonology in 1 week. Patient educated on importance of daily controller medication.\"\"\"\n",
    "\n",
    "print(\"Generating summary for new clinical note...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "generated_summary = generate_discharge_summary(sample_clinical_note)\n",
    "\n",
    "print(\"GENERATED DISCHARGE SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(generated_summary)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✓ Summary generated successfully!\")\n",
    "print(\"\\nYou can now use this function to generate summaries for any new clinical notes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Saving and Loading Instructions\n",
    "\n",
    "Important notes on how to save and reload your fine-tuned model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HOW TO RELOAD YOUR FINE-TUNED MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "=============================================================================\n",
    "SAVING AND LOADING YOUR FINE-TUNED MODEL\n",
    "=============================================================================\n",
    "\n",
    "Your fine-tuned model has been saved to: {}\n",
    "\n",
    "This directory contains:\n",
    "  - adapter_config.json: LoRA configuration\n",
    "  - adapter_model.bin: Trained LoRA weights (~few MB)\n",
    "  - Tokenizer files\n",
    "\n",
    "TO RELOAD THE MODEL IN A NEW SESSION:\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "1. Install dependencies:\n",
    "   pip install transformers peft bitsandbytes torch\n",
    "\n",
    "2. Load the base model with quantization:\n",
    "   \n",
    "   from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "   from peft import PeftModel\n",
    "   import torch\n",
    "   \n",
    "   # Quantization config\n",
    "   bnb_config = BitsAndBytesConfig(\n",
    "       load_in_4bit=True,\n",
    "       bnb_4bit_quant_type=\"nf4\",\n",
    "       bnb_4bit_use_double_quant=True,\n",
    "       bnb_4bit_compute_dtype=torch.float16\n",
    "   )\n",
    "   \n",
    "   # Load base model\n",
    "   base_model = AutoModelForCausalLM.from_pretrained(\n",
    "       \"{}\",\n",
    "       quantization_config=bnb_config,\n",
    "       device_map=\"auto\",\n",
    "       trust_remote_code=True,\n",
    "   )\n",
    "   \n",
    "   # Load LoRA adapters\n",
    "   model = PeftModel.from_pretrained(base_model, \"{}\")\n",
    "   \n",
    "   # Load tokenizer\n",
    "   tokenizer = AutoTokenizer.from_pretrained(\"{}\")\n",
    "   \n",
    "3. Use the generate_discharge_summary() function defined above\n",
    "\n",
    "ALTERNATIVE: Merge adapters into base model (for deployment)\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "If you want a standalone model without separate adapter files:\n",
    "\n",
    "   # After loading model with PeftModel.from_pretrained():\n",
    "   model = model.merge_and_unload()\n",
    "   model.save_pretrained(\"./merged_model\")\n",
    "   tokenizer.save_pretrained(\"./merged_model\")\n",
    "   \n",
    "   # This creates a single model with adapters merged in\n",
    "   # Can be loaded like a regular Hugging Face model\n",
    "\n",
    "=============================================================================\n",
    "\"\"\".format(output_dir_final, MODEL_NAME, output_dir_final, output_dir_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps and Improvements\n",
    "\n",
    "Recommendations for improving model performance and achieving higher recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "=============================================================================\n",
    "NEXT STEPS FOR IMPROVING HIGH RECALL PERFORMANCE\n",
    "=============================================================================\n",
    "\n",
    "1. DATA IMPROVEMENTS:\n",
    "   □ Collect more training examples (aim for 1000+ samples)\n",
    "   □ Ensure reference summaries are comprehensive and capture all entities\n",
    "   □ Add data augmentation (paraphrasing, entity variations)\n",
    "   □ Balance dataset across different clinical scenarios\n",
    "\n",
    "2. PROMPT ENGINEERING:\n",
    "   □ Experiment with more explicit instructions:\n",
    "     \"List ALL diagnoses, medications, vitals, lab results, procedures...\"\n",
    "   □ Add structured output format in prompt:\n",
    "     \"Include sections: Diagnoses, Medications, Vitals, Labs, Procedures...\"\n",
    "   □ Provide few-shot examples in the prompt\n",
    "\n",
    "3. HYPERPARAMETER TUNING:\n",
    "   □ Increase MAX_NEW_TOKENS (current: {}) to allow longer summaries\n",
    "   □ Lower temperature (current: {}) for more deterministic outputs\n",
    "   □ Train for more epochs if not overfitting\n",
    "   □ Increase LoRA rank (current: {}) for more capacity\n",
    "\n",
    "4. TRAINING IMPROVEMENTS:\n",
    "   □ Use larger batch size if memory allows\n",
    "   □ Implement custom loss that penalizes missing entities\n",
    "   □ Add entity extraction as auxiliary task during training\n",
    "   □ Use curriculum learning (easy → hard examples)\n",
    "\n",
    "5. POST-PROCESSING:\n",
    "   □ Add entity extraction to verify all entities are present\n",
    "   □ Implement retrieval-augmented generation (RAG) to ensure completeness\n",
    "   □ Use template-based post-processing to enforce structure\n",
    "\n",
    "6. EVALUATION:\n",
    "   □ Create entity-level recall metrics (diagnoses, meds, vitals)\n",
    "   □ Manual clinical review by domain experts\n",
    "   □ Compare against baseline models (GPT-4, Claude, etc.)\n",
    "   □ A/B testing with clinicians\n",
    "\n",
    "7. ALTERNATIVE APPROACHES:\n",
    "   □ Try extractive + abstractive hybrid approach\n",
    "   □ Use larger model (7B or 13B parameters)\n",
    "   □ Fine-tune specialized medical models (BioGPT, ClinicalGPT)\n",
    "   □ Multi-stage generation (extract entities → generate summary)\n",
    "\n",
    "8. DEPLOYMENT CONSIDERATIONS:\n",
    "   □ Implement confidence scores for generated summaries\n",
    "   □ Add human-in-the-loop review system\n",
    "   □ Monitor for hallucinations and factual errors\n",
    "   □ Ensure HIPAA compliance and data privacy\n",
    "\n",
    "=============================================================================\n",
    "\n",
    "CURRENT CONFIGURATION SUMMARY:\n",
    "  Model: {}\n",
    "  LoRA Rank: {}\n",
    "  Training Epochs: {}\n",
    "  Max Generation Length: {} tokens\n",
    "  Temperature: {}\n",
    "  \n",
    "  Clinical BERTScore Results:\n",
    "    - Precision: {:.4f}\n",
    "    - Recall: {:.4f} ← PRIMARY METRIC\n",
    "    - F1: {:.4f}\n",
    "\n",
    "TARGET RECALL: Aim for ≥0.90 for production use\n",
    "\n",
    "=============================================================================\n",
    "\"\"\".format(\n",
    "    MAX_NEW_TOKENS,\n",
    "    TEMPERATURE,\n",
    "    LORA_R,\n",
    "    MODEL_NAME,\n",
    "    LORA_R,\n",
    "    NUM_EPOCHS,\n",
    "    MAX_NEW_TOKENS,\n",
    "    TEMPERATURE,\n",
    "    avg_precision,\n",
    "    avg_recall,\n",
    "    avg_f1\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ **Environment setup** with all required libraries\n",
    "2. ✅ **Model loading** with QLoRA (4-bit quantization)\n",
    "3. ✅ **LoRA configuration** for efficient fine-tuning\n",
    "4. ✅ **Data formatting** with Gemma prompt template\n",
    "5. ✅ **Training** with SFTTrainer\n",
    "6. ✅ **Clinical BERTScore evaluation** using Bio_ClinicalBERT\n",
    "7. ✅ **Inference function** for generating new summaries\n",
    "8. ✅ **Comprehensive documentation** for your project report\n",
    "\n",
    "**Key Takeaways:**\n",
    "- QLoRA enables fine-tuning large models on consumer GPUs\n",
    "- Clinical BERTScore is the appropriate metric for medical text\n",
    "- High recall requires careful prompt engineering and sufficient training data\n",
    "- The fine-tuned model can be easily saved and reloaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
