{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Stage Clinical Summarization Pipeline: MedGemma â†’ Llama Compression\n",
    "\n",
    "**Course Project - Stage 2: Summary Compression with Entity Retention**\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "```\n",
    "Clinical Notes â†’ [Stage 1: MedGemma-4B] â†’ Verbose Summary â†’ [Stage 2: Llama-3-8B] â†’ Compressed Summary\n",
    "```\n",
    "\n",
    "**Critical Design Decisions**:\n",
    "- **Memory Safety**: Each model is fully unloaded before loading the next (single GPU constraint)\n",
    "- **4-bit Quantization**: Both models use bitsandbytes NF4 for VRAM efficiency\n",
    "- **Entity Preservation**: Chain-of-Density inspired compression with medical entity retention\n",
    "- **Clinical Evaluation**: NER-based entity recall + Clinical BERTScore\n",
    "\n",
    "**Execution Environment**: \n",
    "- Local: Runs with local file paths\n",
    "- Google Colab: Integrates with Google Drive for data I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Environment Detection and Setup\n",
    "\n",
    "Automatically detect execution environment (Colab vs Local) and configure accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Detect execution environment\n",
    "import sys\n",
    "\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Execution Environment: {'Google Colab' if IS_COLAB else 'Local'}\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"\\nâš™ï¸  Colab-specific setup will be activated\")\n",
    "    print(\"   - Google Drive mounting\")\n",
    "    print(\"   - Drive-based data I/O\")\n",
    "    print(\"   - GPU verification\")\n",
    "else:\n",
    "    print(\"\\nâš™ï¸  Local execution mode\")\n",
    "    print(\"   - Using local file paths\")\n",
    "    print(\"   - Saving outputs to project directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Installation and Imports\n",
    "\n",
    "**Memory-Critical Libraries**:\n",
    "- `bitsandbytes`: 4-bit quantization\n",
    "- `accelerate`: Device mapping and offloading\n",
    "- `scispacy`: Medical NER for entity extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (Colab only - local assumes dependencies are installed)\n",
    "if IS_COLAB:\n",
    "    print(\"Installing dependencies for Colab...\\n\")\n",
    "    !pip install -q transformers accelerate bitsandbytes torch\n",
    "    !pip install -q scispacy\n",
    "    !pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
    "    !pip install -q bert-score\n",
    "    !pip install -q pandas numpy\n",
    "    print(\"âœ“ All dependencies installed\")\n",
    "else:\n",
    "    print(\"Local mode: Assuming all dependencies are already installed via uv/pip\")\n",
    "    print(\"Required: transformers, accelerate, bitsandbytes, torch, scispacy, bert-score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from bert_score import BERTScorer\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  WARNING: No GPU detected. This pipeline requires CUDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Google Drive Setup (Colab Only)\n",
    "\n",
    "Mount Google Drive and configure I/O paths for Colab execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Configure paths - UPDATE THESE TO MATCH YOUR DRIVE STRUCTURE\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/Clinical_Summarization_Project\"\n",
    "\n",
    "    # Input: Path to your MIMIC cleaned dataset\n",
    "    INPUT_DATA_PATH = f\"{DRIVE_BASE}/mimic_cleaned_text_only.csv\"\n",
    "\n",
    "    # Input: Path to your fine-tuned MedGemma adapters (if using PEFT)\n",
    "    MEDGEMMA_ADAPTER_PATH = f\"{DRIVE_BASE}/medgemma-discharge-summarization/final\"\n",
    "\n",
    "    # Output: Where to save results\n",
    "    OUTPUT_DIR = f\"{DRIVE_BASE}/compression_results\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"âœ“ Google Drive mounted\")\n",
    "    print(f\"\\nConfigured paths:\")\n",
    "    print(f\"  Input data: {INPUT_DATA_PATH}\")\n",
    "    print(f\"  MedGemma adapters: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    # Verify input file exists\n",
    "    if os.path.exists(INPUT_DATA_PATH):\n",
    "        print(f\"\\nâœ“ Input data file found\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  WARNING: Input file not found at {INPUT_DATA_PATH}\")\n",
    "        print(f\"   Please update INPUT_DATA_PATH variable above\")\n",
    "\n",
    "else:\n",
    "    # Local paths\n",
    "    INPUT_DATA_PATH = \"mimic_cleaned_text_only.csv\"\n",
    "    MEDGEMMA_ADAPTER_PATH = \"./medgemma-discharge-summarization/final\"\n",
    "    OUTPUT_DIR = \"./compression_results\"\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"Local paths configured:\")\n",
    "    print(f\"  Input data: {INPUT_DATA_PATH}\")\n",
    "    print(f\"  MedGemma adapters: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Configuration and Hyperparameters\n",
    "\n",
    "**Critical Memory Settings**:\n",
    "- Both models use NF4 4-bit quantization\n",
    "- `device_map=\"auto\"` for optimal GPU distribution\n",
    "- Explicit memory flushing between stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL IDENTIFIERS\n",
    "# ============================================================================\n",
    "\n",
    "# Stage 1: High-recall summary generation\n",
    "MEDGEMMA_BASE_MODEL = \"google/medgemma-4b-it\"\n",
    "USE_MEDGEMMA_ADAPTER = True  # Set to False if using base model without LoRA\n",
    "\n",
    "# Stage 2: Compression model\n",
    "LLAMA_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# ============================================================================\n",
    "# QUANTIZATION CONFIG (Shared across both models)\n",
    "# ============================================================================\n",
    "\n",
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Stage 1: Verbose summary generation\n",
    "STAGE1_MAX_TOKENS = 512\n",
    "STAGE1_TEMPERATURE = 0.7\n",
    "STAGE1_TOP_P = 0.9\n",
    "\n",
    "# Stage 2: Compression generation\n",
    "STAGE2_MAX_TOKENS = 256  # Deliberately shorter for compression\n",
    "STAGE2_TEMPERATURE = 0.3  # Lower temperature for more deterministic compression\n",
    "STAGE2_TOP_P = 0.9\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "# Number of samples to process (set lower for quick testing)\n",
    "NUM_SAMPLES = 10  # Change to -1 to process entire dataset\n",
    "\n",
    "# NER model for entity extraction\n",
    "SPACY_MODEL = \"en_core_sci_sm\"  # Medical NER model\n",
    "\n",
    "# Clinical BERTScore model\n",
    "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"\\nStage 1 Model: {MEDGEMMA_BASE_MODEL}\")\n",
    "print(f\"  - Using LoRA adapters: {USE_MEDGEMMA_ADAPTER}\")\n",
    "print(f\"  - Max tokens: {STAGE1_MAX_TOKENS}\")\n",
    "print(f\"\\nStage 2 Model: {LLAMA_MODEL}\")\n",
    "print(f\"  - Max tokens: {STAGE2_MAX_TOKENS}\")\n",
    "print(f\"  - Temperature: {STAGE2_TEMPERATURE} (deterministic compression)\")\n",
    "print(f\"\\nProcessing {NUM_SAMPLES if NUM_SAMPLES > 0 else 'ALL'} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Memory Management Utilities\n",
    "\n",
    "**Critical for Single-GPU Execution**: These functions ensure complete model unloading between stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush_memory():\n",
    "    \"\"\"\n",
    "    Aggressively flush GPU and CPU memory.\n",
    "    \n",
    "    This is CRITICAL when switching between models on a single GPU.\n",
    "    Without this, you will encounter OOM errors.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "    # Report memory status\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        print(f\"GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "\n",
    "\n",
    "def unload_model(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Completely unload a model and tokenizer from memory.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to unload (or None)\n",
    "        tokenizer: The tokenizer to unload (or None)\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ§¹ Unloading model from memory...\")\n",
    "\n",
    "    if model is not None:\n",
    "        # Move model to CPU first (if on GPU)\n",
    "        if hasattr(model, 'cpu'):\n",
    "            model.cpu()\n",
    "        del model\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "\n",
    "    flush_memory()\n",
    "    print(\"âœ“ Model unloaded\\n\")\n",
    "\n",
    "\n",
    "# Test memory utilities\n",
    "print(\"Memory management utilities loaded\\n\")\n",
    "flush_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Load Input Data\n",
    "\n",
    "Load clinical notes dataset (preprocessed from Stage 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading data from: {INPUT_DATA_PATH}\\n\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(INPUT_DATA_PATH)\n",
    "\n",
    "print(f\"âœ“ Dataset loaded: {len(df)} total samples\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Select subset if configured\n",
    "if NUM_SAMPLES > 0 and NUM_SAMPLES < len(df):\n",
    "    df = df.head(NUM_SAMPLES)\n",
    "    print(f\"\\nðŸ“Š Processing {len(df)} samples (subset for testing)\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Processing all {len(df)} samples\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"SAMPLE INPUT (First 300 chars):\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(df.iloc[0]['final_input'][:300] + \"...\")\n",
    "print(f\"\\n{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: STAGE 1 - MedGemma Verbose Summary Generation\n",
    "\n",
    "**Objective**: Generate high-recall, detailed clinical summaries.\n",
    "\n",
    "**Memory Strategy**: \n",
    "1. Load MedGemma-4B in 4-bit\n",
    "2. Generate summaries for all samples\n",
    "3. **CRITICAL**: Completely unload before Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1: LOADING MEDGEMMA-4B FOR VERBOSE SUMMARY GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {MEDGEMMA_BASE_MODEL}\")\n",
    "medgemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MEDGEMMA_BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True\n",
    ")\n",
    "medgemma_tokenizer.pad_token = medgemma_tokenizer.eos_token\n",
    "print(\"âœ“ Tokenizer loaded\")\n",
    "\n",
    "# Load base model with quantization\n",
    "print(f\"\\nLoading model: {MEDGEMMA_BASE_MODEL}\")\n",
    "print(\"  Quantization: 4-bit NF4\")\n",
    "print(\"  This may take 2-3 minutes...\")\n",
    "\n",
    "medgemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MEDGEMMA_BASE_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"âœ“ Base model loaded\")\n",
    "\n",
    "# Load LoRA adapters if configured\n",
    "if USE_MEDGEMMA_ADAPTER and os.path.exists(MEDGEMMA_ADAPTER_PATH):\n",
    "    print(f\"\\nLoading LoRA adapters from: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    from peft import PeftModel\n",
    "\n",
    "    medgemma_model = PeftModel.from_pretrained(medgemma_model, MEDGEMMA_ADAPTER_PATH)\n",
    "    print(\"âœ“ LoRA adapters loaded\")\n",
    "elif USE_MEDGEMMA_ADAPTER:\n",
    "    print(f\"\\nâš ï¸  WARNING: Adapter path not found: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(\"   Continuing with base model only\")\n",
    "\n",
    "medgemma_model.eval()\n",
    "print(f\"\\nâœ“ MedGemma ready for inference\")\n",
    "flush_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stage1_summary(clinical_note: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate verbose clinical summary using MedGemma.\n",
    "    \n",
    "    Args:\n",
    "        clinical_note: Input clinical text\n",
    "    \n",
    "    Returns:\n",
    "        Generated summary (high-recall, verbose)\n",
    "    \"\"\"\n",
    "    instruction = (\n",
    "        \"Summarize the following clinical discharge notes. \"\n",
    "        \"Include ALL diagnoses, medications, vitals, lab results, \"\n",
    "        \"procedures, and follow-up instructions. \"\n",
    "        \"Ensure complete coverage of all medical entities.\"\n",
    "    )\n",
    "\n",
    "    # Format using Gemma template\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{clinical_note}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    inputs = medgemma_tokenizer(prompt, return_tensors=\"pt\").to(medgemma_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = medgemma_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=STAGE1_MAX_TOKENS,\n",
    "            temperature=STAGE1_TEMPERATURE,\n",
    "            top_p=STAGE1_TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=medgemma_tokenizer.pad_token_id,\n",
    "            eos_token_id=medgemma_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = medgemma_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the model's response\n",
    "    marker = \"<start_of_turn>model\"\n",
    "    if marker in generated_text:\n",
    "        summary = generated_text.split(marker)[-1].strip()\n",
    "    else:\n",
    "        summary = generated_text[len(prompt):].strip()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "print(\"âœ“ Stage 1 generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Stage 1 summaries for all samples\n",
    "print(f\"\\nGenerating Stage 1 summaries for {len(df)} samples...\\n\")\n",
    "\n",
    "stage1_summaries = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    clinical_note = row['final_input']\n",
    "\n",
    "    print(f\"[{idx + 1}/{len(df)}] Generating verbose summary...\", end=\" \")\n",
    "    summary = generate_stage1_summary(clinical_note)\n",
    "    stage1_summaries.append(summary)\n",
    "    print(f\"âœ“ ({len(summary)} chars)\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['stage1_summary'] = stage1_summaries\n",
    "\n",
    "print(f\"\\nâœ“ Stage 1 complete: {len(stage1_summaries)} summaries generated\")\n",
    "print(f\"  Average length: {np.mean([len(s) for s in stage1_summaries]):.0f} characters\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STAGE 1 OUTPUT SAMPLE:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(stage1_summaries[0][:400] + \"...\")\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRITICAL: Unload MedGemma Before Stage 2\n",
    "\n",
    "**This step is NON-NEGOTIABLE for single-GPU execution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNLOADING MEDGEMMA-4B FROM MEMORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "unload_model(medgemma_model, medgemma_tokenizer)\n",
    "\n",
    "# Verify memory is cleared\n",
    "print(\"Memory status after unloading:\")\n",
    "flush_memory()\n",
    "\n",
    "print(\"\\nâœ“ Safe to proceed to Stage 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: STAGE 2 - Llama-3 Compression\n",
    "\n",
    "**Objective**: Compress verbose summaries while preserving medical entities.\n",
    "\n",
    "**Chain-of-Density Inspired Prompt**: Forces 50% compression with entity retention constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 2: LOADING LLAMA-3-8B FOR COMPRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if we need HuggingFace authentication for Llama\n",
    "if IS_COLAB:\n",
    "    print(\"\\nâš ï¸  Llama-3 requires HuggingFace authentication\")\n",
    "    print(\"   Running authentication flow...\\n\")\n",
    "    from huggingface_hub import notebook_login\n",
    "\n",
    "    notebook_login()\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {LLAMA_MODEL}\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LLAMA_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"  # Llama uses left padding\n",
    ")\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "print(\"âœ“ Tokenizer loaded\")\n",
    "\n",
    "# Load model with quantization\n",
    "print(f\"\\nLoading model: {LLAMA_MODEL}\")\n",
    "print(\"  Quantization: 4-bit NF4\")\n",
    "print(\"  This may take 2-3 minutes...\")\n",
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "llama_model.eval()\n",
    "print(f\"\\nâœ“ Llama-3 ready for compression\")\n",
    "flush_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_compressed_summary(verbose_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Compress summary using Llama-3 with entity retention constraint.\n",
    "    \n",
    "    Chain-of-Density inspired prompt: Forces 50% compression while\n",
    "    preserving all medical entities (medications, numbers, dates).\n",
    "    \n",
    "    Args:\n",
    "        verbose_summary: Stage 1 summary (high-recall)\n",
    "    \n",
    "    Returns:\n",
    "        Compressed summary\n",
    "    \"\"\"\n",
    "    # Chain-of-Density inspired compression prompt\n",
    "    system_prompt = (\n",
    "        \"You are a medical summarization expert. Your task is to compress \"\n",
    "        \"clinical summaries while preserving critical information.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Rewrite the following summary to be 50% shorter. \"\n",
    "        \"You MUST retain ALL entities: medications (with dosages), \"\n",
    "        \"vital signs (with numbers), lab results (with values), \"\n",
    "        \"diagnoses, procedures, and dates. \"\n",
    "        \"If you cannot shorten it without losing a critical fact, do not shorten it. \"\n",
    "        \"Remove only redundant phrasing and verbose descriptions.\\n\\n\"\n",
    "        f\"Summary to compress:\\n{verbose_summary}\"\n",
    "    )\n",
    "\n",
    "    # Format using Llama-3 chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    prompt = llama_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(llama_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=STAGE2_MAX_TOKENS,\n",
    "            temperature=STAGE2_TEMPERATURE,\n",
    "            top_p=STAGE2_TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=llama_tokenizer.pad_token_id,\n",
    "            eos_token_id=llama_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the assistant's response\n",
    "    # Llama-3 format: <|start_header_id|>assistant<|end_header_id|>\\n\\n{response}\n",
    "    if \"assistant\" in generated_text:\n",
    "        compressed = generated_text.split(\"assistant\")[-1].strip()\n",
    "        # Remove any remaining header artifacts\n",
    "        compressed = compressed.split(\"\\n\\n\", 1)[-1] if \"\\n\\n\" in compressed else compressed\n",
    "    else:\n",
    "        compressed = generated_text[len(prompt):].strip()\n",
    "\n",
    "    return compressed\n",
    "\n",
    "\n",
    "print(\"âœ“ Stage 2 compression function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Stage 2 compressed summaries\n",
    "print(f\"\\nCompressing {len(df)} Stage 1 summaries...\\n\")\n",
    "\n",
    "stage2_summaries = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    verbose_summary = row['stage1_summary']\n",
    "\n",
    "    print(f\"[{idx + 1}/{len(df)}] Compressing summary...\", end=\" \")\n",
    "    compressed = generate_compressed_summary(verbose_summary)\n",
    "    stage2_summaries.append(compressed)\n",
    "\n",
    "    # Calculate compression ratio\n",
    "    ratio = len(compressed) / len(verbose_summary)\n",
    "    print(f\"âœ“ ({len(compressed)} chars, {ratio:.2%} of original)\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['stage2_summary'] = stage2_summaries\n",
    "\n",
    "print(f\"\\nâœ“ Stage 2 complete: {len(stage2_summaries)} summaries compressed\")\n",
    "print(f\"  Average length: {np.mean([len(s) for s in stage2_summaries]):.0f} characters\")\n",
    "print(\n",
    "    f\"  Overall compression: {np.mean([len(s2) / len(s1) for s1, s2 in zip(stage1_summaries, stage2_summaries)]):.2%}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STAGE 2 OUTPUT SAMPLE:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(stage2_summaries[0])\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unload Llama-3 (Optional - only if running evaluation separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to free memory before evaluation, uncomment:\n",
    "# unload_model(llama_model, llama_tokenizer)\n",
    "\n",
    "print(\"âœ“ Stage 2 generation complete. Proceeding to evaluation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: EVALUATION - Metric 1: Compression Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 1: COMPRESSION RATIO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def calculate_compression_ratio(stage1: str, stage2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate compression ratio (Stage 2 length / Stage 1 length).\n",
    "    \n",
    "    Lower values indicate stronger compression.\n",
    "    \"\"\"\n",
    "    return len(stage2) / len(stage1)\n",
    "\n",
    "\n",
    "# Calculate for all samples\n",
    "df['compression_ratio'] = df.apply(\n",
    "    lambda row: calculate_compression_ratio(row['stage1_summary'], row['stage2_summary']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nCompression Statistics:\")\n",
    "print(f\"  Mean ratio: {df['compression_ratio'].mean():.2%}\")\n",
    "print(f\"  Median ratio: {df['compression_ratio'].median():.2%}\")\n",
    "print(f\"  Min ratio: {df['compression_ratio'].min():.2%}\")\n",
    "print(f\"  Max ratio: {df['compression_ratio'].max():.2%}\")\n",
    "print(f\"  Std dev: {df['compression_ratio'].std():.2%}\")\n",
    "\n",
    "print(f\"\\nâœ“ Compression ratios calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: EVALUATION - Metric 2: Entity Retention (NER)\n",
    "\n",
    "**Method**: Extract medical entities from both Stage 1 and Stage 2 summaries using SciSpacy, then calculate recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 2: ENTITY RETENTION (NER)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load SciSpacy medical NER model\n",
    "print(f\"\\nLoading SciSpacy model: {SPACY_MODEL}\")\n",
    "nlp = spacy.load(SPACY_MODEL)\n",
    "print(\"âœ“ NER model loaded\")\n",
    "\n",
    "\n",
    "def extract_entities(text: str) -> set:\n",
    "    \"\"\"\n",
    "    Extract medical entities from text using SciSpacy.\n",
    "    \n",
    "    Returns set of entity texts (lowercased for comparison).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = {ent.text.lower() for ent in doc.ents}\n",
    "    return entities\n",
    "\n",
    "\n",
    "def calculate_entity_recall(stage1_text: str, stage2_text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate entity retention recall: \n",
    "    (Entities in Stage 2) / (Entities in Stage 1)\n",
    "    \n",
    "    Returns value between 0 and 1 (higher is better).\n",
    "    \"\"\"\n",
    "    entities_stage1 = extract_entities(stage1_text)\n",
    "    entities_stage2 = extract_entities(stage2_text)\n",
    "\n",
    "    if len(entities_stage1) == 0:\n",
    "        return 1.0  # No entities to preserve\n",
    "\n",
    "    # Calculate recall: how many Stage 1 entities appear in Stage 2?\n",
    "    retained = entities_stage1.intersection(entities_stage2)\n",
    "    recall = len(retained) / len(entities_stage1)\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "print(\"\\nCalculating entity recall for all samples...\\n\")\n",
    "\n",
    "entity_recalls = []\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"[{idx + 1}/{len(df)}] Extracting entities...\", end=\" \")\n",
    "    recall = calculate_entity_recall(row['stage1_summary'], row['stage2_summary'])\n",
    "    entity_recalls.append(recall)\n",
    "    print(f\"âœ“ Recall: {recall:.2%}\")\n",
    "\n",
    "df['entity_recall'] = entity_recalls\n",
    "\n",
    "print(f\"\\nEntity Retention Statistics:\")\n",
    "print(f\"  Mean recall: {df['entity_recall'].mean():.2%}\")\n",
    "print(f\"  Median recall: {df['entity_recall'].median():.2%}\")\n",
    "print(f\"  Min recall: {df['entity_recall'].min():.2%}\")\n",
    "print(f\"  Max recall: {df['entity_recall'].max():.2%}\")\n",
    "print(f\"  Std dev: {df['entity_recall'].std():.2%}\")\n",
    "\n",
    "print(f\"\\nâœ“ Entity recall calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: EVALUATION - Metric 3: Clinical BERTScore\n",
    "\n",
    "**Method**: Measure semantic similarity between original clinical notes and final compressed summaries using Bio_ClinicalBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 3: CLINICAL BERTSCORE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize Clinical BERTScore\n",
    "print(f\"\\nInitializing BERTScorer with {CLINICAL_BERT}\")\n",
    "print(\"This will download the model if not cached...\\n\")\n",
    "\n",
    "bert_scorer = BERTScorer(\n",
    "    model_type=CLINICAL_BERT,\n",
    "    num_layers=9,\n",
    "    rescale_with_baseline=True,\n",
    "    lang=\"en\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ BERTScorer initialized\")\n",
    "\n",
    "print(\"\\nCalculating BERTScores (this may take a few minutes)...\\n\")\n",
    "\n",
    "# Prepare data for batch scoring\n",
    "source_texts = df['final_input'].tolist()\n",
    "compressed_summaries = df['stage2_summary'].tolist()\n",
    "\n",
    "# Calculate BERTScore\n",
    "# We compare compressed summaries (candidates) against original notes (references)\n",
    "P, R, F1 = bert_scorer.score(\n",
    "    cands=compressed_summaries,\n",
    "    refs=source_texts\n",
    ")\n",
    "\n",
    "# Convert to numpy\n",
    "df['bertscore_precision'] = P.cpu().numpy()\n",
    "df['bertscore_recall'] = R.cpu().numpy()\n",
    "df['bertscore_f1'] = F1.cpu().numpy()\n",
    "\n",
    "print(f\"\\nClinical BERTScore Statistics:\")\n",
    "print(f\"\\nPrecision:\")\n",
    "print(f\"  Mean: {df['bertscore_precision'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_precision'].median():.4f}\")\n",
    "print(f\"\\nRecall:\")\n",
    "print(f\"  Mean: {df['bertscore_recall'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_recall'].median():.4f}\")\n",
    "print(f\"\\nF1:\")\n",
    "print(f\"  Mean: {df['bertscore_f1'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_f1'].median():.4f}\")\n",
    "\n",
    "print(f\"\\nâœ“ Clinical BERTScore calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Results Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select columns for final output\n",
    "results_df = df[[\n",
    "    'final_input',\n",
    "    'stage1_summary',\n",
    "    'stage2_summary',\n",
    "    'compression_ratio',\n",
    "    'entity_recall',\n",
    "    'bertscore_precision',\n",
    "    'bertscore_recall',\n",
    "    'bertscore_f1'\n",
    "]].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "results_df.columns = [\n",
    "    'Source_Text',\n",
    "    'Stage1_Summary_Verbose',\n",
    "    'Stage2_Summary_Compressed',\n",
    "    'Compression_Ratio',\n",
    "    'Entity_Recall',\n",
    "    'BERTScore_Precision',\n",
    "    'BERTScore_Recall',\n",
    "    'BERTScore_F1'\n",
    "]\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nProcessed {len(results_df)} samples\\n\")\n",
    "print(\"Aggregate Metrics:\")\n",
    "print(\n",
    "    f\"  Compression Ratio: {results_df['Compression_Ratio'].mean():.2%} Â± {results_df['Compression_Ratio'].std():.2%}\")\n",
    "print(f\"  Entity Recall: {results_df['Entity_Recall'].mean():.2%} Â± {results_df['Entity_Recall'].std():.2%}\")\n",
    "print(f\"  BERTScore F1: {results_df['BERTScore_F1'].mean():.4f} Â± {results_df['BERTScore_F1'].std():.4f}\")\n",
    "\n",
    "# Display sample comparison\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"SAMPLE COMPARISON (First Example)\")\n",
    "print(f\"{'=' * 80}\")\n",
    "sample = results_df.iloc[0]\n",
    "print(f\"\\nSource Text (truncated): {sample['Source_Text'][:200]}...\")\n",
    "print(f\"\\nStage 1 (Verbose): {sample['Stage1_Summary_Verbose'][:300]}...\")\n",
    "print(f\"\\nStage 2 (Compressed): {sample['Stage2_Summary_Compressed']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Compression: {sample['Compression_Ratio']:.2%}\")\n",
    "print(f\"  Entity Recall: {sample['Entity_Recall']:.2%}\")\n",
    "print(f\"  BERTScore F1: {sample['BERTScore_F1']:.4f}\")\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_csv_path = os.path.join(OUTPUT_DIR, \"compression_pipeline_results.csv\")\n",
    "output_json_path = os.path.join(OUTPUT_DIR, \"compression_pipeline_results.json\")\n",
    "\n",
    "# Save as CSV\n",
    "results_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nâœ“ Results saved to CSV: {output_csv_path}\")\n",
    "\n",
    "# Save as JSON (for easier inspection)\n",
    "results_df.to_json(output_json_path, orient='records', indent=2)\n",
    "print(f\"âœ“ Results saved to JSON: {output_json_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"num_samples\": len(results_df),\n",
    "    \"compression_ratio\": {\n",
    "        \"mean\": float(results_df['Compression_Ratio'].mean()),\n",
    "        \"std\": float(results_df['Compression_Ratio'].std()),\n",
    "        \"min\": float(results_df['Compression_Ratio'].min()),\n",
    "        \"max\": float(results_df['Compression_Ratio'].max())\n",
    "    },\n",
    "    \"entity_recall\": {\n",
    "        \"mean\": float(results_df['Entity_Recall'].mean()),\n",
    "        \"std\": float(results_df['Entity_Recall'].std()),\n",
    "        \"min\": float(results_df['Entity_Recall'].min()),\n",
    "        \"max\": float(results_df['Entity_Recall'].max())\n",
    "    },\n",
    "    \"bertscore_f1\": {\n",
    "        \"mean\": float(results_df['BERTScore_F1'].mean()),\n",
    "        \"std\": float(results_df['BERTScore_F1'].std()),\n",
    "        \"min\": float(results_df['BERTScore_F1'].min()),\n",
    "        \"max\": float(results_df['BERTScore_F1'].max())\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"stage1_model\": MEDGEMMA_BASE_MODEL,\n",
    "        \"stage2_model\": LLAMA_MODEL,\n",
    "        \"num_samples_processed\": NUM_SAMPLES if NUM_SAMPLES > 0 else \"all\"\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"summary_statistics.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Summary statistics saved: {summary_path}\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(f\"\\nðŸ“ All outputs saved to Google Drive: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“ All outputs saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Analysis and Insights\n",
    "\n",
    "Key questions to investigate:\n",
    "1. **Compression vs Quality Trade-off**: Does higher compression correlate with lower entity recall?\n",
    "2. **BERTScore Reliability**: Does Clinical BERTScore align with entity retention?\n",
    "3. **Failure Modes**: Which samples fail to compress or lose critical entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis 1: Correlation between compression and entity recall\n",
    "import seaborn as sns\n",
    "\n",
    "if len(results_df) > 1:\n",
    "    correlation = results_df['Compression_Ratio'].corr(results_df['Entity_Recall'])\n",
    "    print(f\"Correlation (Compression Ratio vs Entity Recall): {correlation:.3f}\")\n",
    "\n",
    "    if correlation < -0.3:\n",
    "        print(\"  â†’ Strong negative correlation: Higher compression â†’ Lower entity retention\")\n",
    "    elif correlation > 0.3:\n",
    "        print(\"  â†’ Strong positive correlation: Counterintuitive result, investigate further\")\n",
    "    else:\n",
    "        print(\"  â†’ Weak correlation: Compression and entity retention are relatively independent\")\n",
    "\n",
    "# Analysis 2: Identify best and worst performing samples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Best Performing Sample (Highest Entity Recall):\")\n",
    "print(\"=\" * 80)\n",
    "best_idx = results_df['Entity_Recall'].idxmax()\n",
    "best = results_df.iloc[best_idx]\n",
    "print(f\"Entity Recall: {best['Entity_Recall']:.2%}\")\n",
    "print(f\"Compression: {best['Compression_Ratio']:.2%}\")\n",
    "print(f\"BERTScore F1: {best['BERTScore_F1']:.4f}\")\n",
    "print(f\"\\nStage 2 Summary: {best['Stage2_Summary_Compressed'][:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Worst Performing Sample (Lowest Entity Recall):\")\n",
    "print(\"=\" * 80)\n",
    "worst_idx = results_df['Entity_Recall'].idxmin()\n",
    "worst = results_df.iloc[worst_idx]\n",
    "print(f\"Entity Recall: {worst['Entity_Recall']:.2%}\")\n",
    "print(f\"Compression: {worst['Compression_Ratio']:.2%}\")\n",
    "print(f\"BERTScore F1: {worst['BERTScore_F1']:.4f}\")\n",
    "print(f\"\\nStage 2 Summary: {worst['Stage2_Summary_Compressed'][:300]}...\")\n",
    "\n",
    "print(\"\\nâœ“ Analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Notes and Next Steps\n",
    "\n",
    "### Memory Management Recap\n",
    "This notebook demonstrated:\n",
    "- âœ… Loading 4B and 8B models sequentially on a single GPU\n",
    "- âœ… Complete model unloading with `flush_memory()`\n",
    "- âœ… Successful execution without OOM errors\n",
    "\n",
    "### Results Interpretation\n",
    "- **Compression Ratio**: Target was ~50%, actual performance depends on prompt adherence\n",
    "- **Entity Recall**: Should be >85% for production use\n",
    "- **BERTScore**: Higher F1 indicates better semantic preservation\n",
    "\n",
    "### Potential Improvements\n",
    "1. **Prompt Engineering**: Refine compression prompt for better entity retention\n",
    "2. **Multi-stage Compression**: Iterative compression with entity verification\n",
    "3. **Entity-Aware Loss**: Fine-tune Llama on compression with entity-weighted loss\n",
    "4. **Hybrid Approach**: Extractive + abstractive compression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
