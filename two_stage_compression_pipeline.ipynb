{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Two-Stage Clinical Summarization Pipeline: MedGemma â†’ Llama Compression\n",
    "\n",
    "**Course Project - Stage 2: Summary Compression with Entity Retention**\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "```\n",
    "Clinical Notes â†’ [Stage 1: MedGemma-4B] â†’ Verbose Summary â†’ [Stage 2: Llama-3-8B] â†’ Compressed Summary\n",
    "```\n",
    "\n",
    "**Critical Design Decisions**:\n",
    "- **Memory Safety**: Each model is fully unloaded before loading the next (single GPU constraint)\n",
    "- **4-bit Quantization**: Both models use bitsandbytes NF4 for VRAM efficiency\n",
    "- **Entity Preservation**: Chain-of-Density inspired compression with medical entity retention\n",
    "- **Clinical Evaluation**: NER-based entity recall + Clinical BERTScore\n",
    "\n",
    "**Execution Environment**: \n",
    "- Local: Runs with local file paths\n",
    "- Google Colab: Integrates with Google Drive for data I/O"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 0: Environment Detection and Setup\n",
    "\n",
    "Automatically detect execution environment (Colab vs Local) and configure accordingly."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:37:01.549936Z",
     "start_time": "2025-12-04T10:37:01.542553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# Detect execution environment\n",
    "import sys\n",
    "\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Execution Environment: {'Google Colab' if IS_COLAB else 'Local'}\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"\\nâš™ï¸  Colab-specific setup will be activated\")\n",
    "    print(\"   - Google Drive mounting\")\n",
    "    print(\"   - Drive-based data I/O\")\n",
    "    print(\"   - GPU verification\")\n",
    "else:\n",
    "    print(\"\\nâš™ï¸  Local execution mode\")\n",
    "    print(\"   - Using local file paths\")\n",
    "    print(\"   - Saving outputs to project directory\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Environment: Local\n",
      "\n",
      "âš™ï¸  Local execution mode\n",
      "   - Using local file paths\n",
      "   - Saving outputs to project directory\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 1: Installation and Imports\n",
    "\n",
    "**Memory-Critical Libraries**:\n",
    "- `bitsandbytes`: 4-bit quantization\n",
    "- `accelerate`: Device mapping and offloading\n",
    "- `scispacy`: Medical NER for entity extraction"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:37:07.856055Z",
     "start_time": "2025-12-04T10:37:03.641498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install required packages (Colab only - local assumes dependencies are installed)\n",
    "if IS_COLAB:\n",
    "    print(\"Installing dependencies for Colab...\\n\")\n",
    "    !pip install -q transformers accelerate bitsandbytes torch\n",
    "    !pip install -q scispacy\n",
    "    !pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
    "    !pip install -q bert-score\n",
    "    !pip install -q pandas numpy\n",
    "    print(\"âœ“ All dependencies installed\")\n",
    "else:\n",
    "    !pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
    "    print(\"Local mode: Assuming all dependencies are already installed via uv/pip\")\n",
    "    print(\"Required: transformers, accelerate, bitsandbytes, torch, scispacy, bert-score\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local mode: Assuming all dependencies are already installed via uv/pip\n",
      "Required: transformers, accelerate, bitsandbytes, torch, scispacy, bert-score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:37:24.391041Z",
     "start_time": "2025-12-04T10:37:18.429541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Core imports\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from bert_score import BERTScorer\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  WARNING: No GPU detected. This pipeline requires CUDA.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "VRAM: 8.55 GB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 2: Google Drive Setup (Colab Only)\n",
    "\n",
    "Mount Google Drive and configure I/O paths for Colab execution."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:40:15.670740Z",
     "start_time": "2025-12-04T10:40:15.666217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Configure paths - UPDATE THESE TO MATCH YOUR DRIVE STRUCTURE\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/Clinical_Summarization_Project\"\n",
    "\n",
    "    # Input: Path to your MIMIC cleaned dataset\n",
    "    INPUT_DATA_PATH = f\"{DRIVE_BASE}/mimic_cleaned_text_only.csv\"\n",
    "\n",
    "    # Input: Path to your fine-tuned MedGemma adapters (if using PEFT)\n",
    "    MEDGEMMA_ADAPTER_PATH = f\"{DRIVE_BASE}/medgemma-discharge-summarization/final\"\n",
    "\n",
    "    # Output: Where to save results\n",
    "    OUTPUT_DIR = f\"{DRIVE_BASE}/compression_results\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"âœ“ Google Drive mounted\")\n",
    "    print(f\"\\nConfigured paths:\")\n",
    "    print(f\"  Input data: {INPUT_DATA_PATH}\")\n",
    "    print(f\"  MedGemma adapters: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    # Verify input file exists\n",
    "    if os.path.exists(INPUT_DATA_PATH):\n",
    "        print(f\"\\nâœ“ Input data file found\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  WARNING: Input file not found at {INPUT_DATA_PATH}\")\n",
    "        print(f\"   Please update INPUT_DATA_PATH variable above\")\n",
    "\n",
    "else:\n",
    "    # Local paths\n",
    "    INPUT_DATA_PATH = \"mimic_cleaned_full.csv\"\n",
    "    MEDGEMMA_ADAPTER_PATH = \"./medgemma-discharge-summarization/final\"\n",
    "    OUTPUT_DIR = \"./compression_results\"\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"Local paths configured:\")\n",
    "    print(f\"  Input data: {INPUT_DATA_PATH}\")\n",
    "    print(f\"  MedGemma adapters: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local paths configured:\n",
      "  Input data: mimic_cleaned_full.csv\n",
      "  MedGemma adapters: ./medgemma-discharge-summarization/final\n",
      "  Output directory: ./compression_results\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 3: Configuration and Hyperparameters\n",
    "\n",
    "**Critical Memory Settings**:\n",
    "- Both models use NF4 4-bit quantization\n",
    "- `device_map=\"auto\"` for optimal GPU distribution\n",
    "- Explicit memory flushing between stages"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:40:18.705838Z",
     "start_time": "2025-12-04T10:40:18.698962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# MODEL IDENTIFIERS\n",
    "# ============================================================================\n",
    "\n",
    "# Stage 1: High-recall summary generation\n",
    "MEDGEMMA_BASE_MODEL = \"google/medgemma-4b-it\"\n",
    "USE_MEDGEMMA_ADAPTER = True  # Set to False if using base model without LoRA\n",
    "\n",
    "# Stage 2: Compression model\n",
    "LLAMA_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# ============================================================================\n",
    "# QUANTIZATION CONFIG (Shared across both models)\n",
    "# ============================================================================\n",
    "\n",
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Stage 1: Verbose summary generation\n",
    "STAGE1_MAX_TOKENS = 512\n",
    "STAGE1_TEMPERATURE = 0.7\n",
    "STAGE1_TOP_P = 0.9\n",
    "\n",
    "# Stage 2: Compression generation\n",
    "STAGE2_MAX_TOKENS = 256  # Deliberately shorter for compression\n",
    "STAGE2_TEMPERATURE = 0.3  # Lower temperature for more deterministic compression\n",
    "STAGE2_TOP_P = 0.9\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "# Number of samples to process (set lower for quick testing)\n",
    "NUM_SAMPLES = 10  # Change to -1 to process entire dataset\n",
    "\n",
    "# NER model for entity extraction\n",
    "SPACY_MODEL = \"en_core_sci_sm\"  # Medical NER model\n",
    "\n",
    "# Clinical BERTScore model\n",
    "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"\\nStage 1 Model: {MEDGEMMA_BASE_MODEL}\")\n",
    "print(f\"  - Using LoRA adapters: {USE_MEDGEMMA_ADAPTER}\")\n",
    "print(f\"  - Max tokens: {STAGE1_MAX_TOKENS}\")\n",
    "print(f\"\\nStage 2 Model: {LLAMA_MODEL}\")\n",
    "print(f\"  - Max tokens: {STAGE2_MAX_TOKENS}\")\n",
    "print(f\"  - Temperature: {STAGE2_TEMPERATURE} (deterministic compression)\")\n",
    "print(f\"\\nProcessing {NUM_SAMPLES if NUM_SAMPLES > 0 else 'ALL'} samples\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "\n",
      "Stage 1 Model: google/medgemma-4b-it\n",
      "  - Using LoRA adapters: True\n",
      "  - Max tokens: 512\n",
      "\n",
      "Stage 2 Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  - Max tokens: 256\n",
      "  - Temperature: 0.3 (deterministic compression)\n",
      "\n",
      "Processing 10 samples\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 4: Memory Management Utilities\n",
    "\n",
    "**Critical for Single-GPU Execution**: These functions ensure complete model unloading between stages."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:40:25.203298Z",
     "start_time": "2025-12-04T10:40:25.108250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flush_memory():\n",
    "    \"\"\"\n",
    "    Aggressively flush GPU and CPU memory.\n",
    "    \n",
    "    This is CRITICAL when switching between models on a single GPU.\n",
    "    Without this, you will encounter OOM errors.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "    # Report memory status\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        print(f\"GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "\n",
    "\n",
    "def unload_model(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Completely unload a model and tokenizer from memory.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to unload (or None)\n",
    "        tokenizer: The tokenizer to unload (or None)\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ§¹ Unloading model from memory...\")\n",
    "\n",
    "    if model is not None:\n",
    "        # Move model to CPU first (if on GPU)\n",
    "        if hasattr(model, 'cpu'):\n",
    "            model.cpu()\n",
    "        del model\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "\n",
    "    flush_memory()\n",
    "    print(\"âœ“ Model unloaded\\n\")\n",
    "\n",
    "\n",
    "# Test memory utilities\n",
    "print(\"Memory management utilities loaded\\n\")\n",
    "flush_memory()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory management utilities loaded\n",
      "\n",
      "GPU Memory: 0.00 GB allocated, 0.00 GB reserved\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 5: Load Input Data\n",
    "\n",
    "Load clinical notes dataset (preprocessed from Stage 0)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:41:02.691935Z",
     "start_time": "2025-12-04T10:40:28.071313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Loading data from: {INPUT_DATA_PATH}\\n\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(INPUT_DATA_PATH)\n",
    "\n",
    "print(f\"âœ“ Dataset loaded: {len(df)} total samples\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Select subset if configured\n",
    "if NUM_SAMPLES > 0 and NUM_SAMPLES < len(df):\n",
    "    df = df.head(NUM_SAMPLES)\n",
    "    print(f\"\\nðŸ“Š Processing {len(df)} samples (subset for testing)\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Processing all {len(df)} samples\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"SAMPLE INPUT (First 300 chars):\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(df.iloc[0]['final_input'][:300] + \"...\")\n",
    "print(f\"\\n{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: mimic_cleaned_full.csv\n",
      "\n",
      "âœ“ Dataset loaded: 269516 total samples\n",
      "  Columns: ['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq', 'charttime', 'storetime', 'input', 'target', 'input_length', 'target_length', 'tokenized_input', 'tokenized_target', 'lemmatized_input', 'lemmatized_target', 'filtered_input', 'final_input', 'final_target']\n",
      "\n",
      "ðŸ“Š Processing 10 samples (subset for testing)\n",
      "\n",
      "================================================================================\n",
      "SAMPLE INPUT (First 300 chars):\n",
      "================================================================================\n",
      "summarize chief complaint worsening abd distension pain history present illness hcv cirrhosis cb ascites hiv art ho ivdu copd bioplar ptsd presented osh ed worsening abd distension past week pt report selfdiscontinuing lasix spirnolactone week ago feel like dont anything doesnt want put chemical fol...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 6: STAGE 1 - MedGemma Verbose Summary Generation\n",
    "\n",
    "**Objective**: Generate high-recall, detailed clinical summaries.\n",
    "\n",
    "**Memory Strategy**: \n",
    "1. Load MedGemma-4B in 4-bit\n",
    "2. Generate summaries for all samples\n",
    "3. **CRITICAL**: Completely unload before Stage 2"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:41:32.026699Z",
     "start_time": "2025-12-04T10:41:32.022950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Enable synchronous CUDA for better error messages\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"âœ“ CUDA synchronous mode enabled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CUDA synchronous mode enabled\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:41:50.018513Z",
     "start_time": "2025-12-04T10:41:33.756641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1: LOADING MEDGEMMA-4B FOR VERBOSE SUMMARY GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Load base model FIRST (before tokenizer)\n",
    "print(f\"\\nLoading base model: {MEDGEMMA_BASE_MODEL}\")\n",
    "print(\"  Quantization: 4-bit NF4\")\n",
    "print(\"  This may take 2-3 minutes...\")\n",
    "\n",
    "medgemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MEDGEMMA_BASE_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "print(\"âœ“ Base model loaded\")\n",
    "\n",
    "# Step 2: Load LoRA adapters NEXT (before tokenizer)\n",
    "if USE_MEDGEMMA_ADAPTER and os.path.exists(MEDGEMMA_ADAPTER_PATH):\n",
    "    print(f\"\\nLoading LoRA adapters from: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    from peft import PeftModel\n",
    "\n",
    "    medgemma_model = PeftModel.from_pretrained(medgemma_model, MEDGEMMA_ADAPTER_PATH)\n",
    "    print(\"âœ“ LoRA adapters loaded\")\n",
    "\n",
    "    # CRITICAL: After loading LoRA, the model config may have updated vocab size\n",
    "    # We need to use THIS vocab size for the tokenizer\n",
    "\n",
    "elif USE_MEDGEMMA_ADAPTER:\n",
    "    print(f\"\\nâš ï¸  WARNING: Adapter path not found: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(\"   Continuing with base model only\")\n",
    "\n",
    "# Step 3: Get the ACTUAL vocab size from the loaded model\n",
    "embedding_layer = medgemma_model.get_input_embeddings()\n",
    "actual_vocab_size = embedding_layer.weight.shape[0]\n",
    "print(f\"\\n  Model embedding vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Step 4: Load tokenizer AFTER model is fully loaded\n",
    "# Try adapter path first, then base model\n",
    "if USE_MEDGEMMA_ADAPTER and os.path.exists(MEDGEMMA_ADAPTER_PATH):\n",
    "    print(f\"\\nAttempting to load tokenizer from adapter path: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    try:\n",
    "        medgemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MEDGEMMA_ADAPTER_PATH,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "        print(\"âœ“ Tokenizer loaded from adapter path\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Adapter tokenizer failed: {e}\")\n",
    "        print(\"   Loading base model tokenizer instead\")\n",
    "        medgemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MEDGEMMA_BASE_MODEL,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "else:\n",
    "    print(f\"\\nLoading tokenizer from base model: {MEDGEMMA_BASE_MODEL}\")\n",
    "    medgemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MEDGEMMA_BASE_MODEL,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\",\n",
    "        add_eos_token=True\n",
    "    )\n",
    "\n",
    "medgemma_tokenizer.pad_token = medgemma_tokenizer.eos_token\n",
    "\n",
    "print(f\"\\n  Tokenizer vocab size: {len(medgemma_tokenizer)}\")\n",
    "print(f\"  PAD token ID: {medgemma_tokenizer.pad_token_id}\")\n",
    "print(f\"  EOS token ID: {medgemma_tokenizer.eos_token_id}\")\n",
    "\n",
    "# Step 5: CRITICAL VALIDATION - Check if sizes match\n",
    "if len(medgemma_tokenizer) != actual_vocab_size:\n",
    "    print(f\"\\nâš ï¸  MISMATCH DETECTED!\")\n",
    "    print(f\"   Tokenizer vocab: {len(medgemma_tokenizer)}\")\n",
    "    print(f\"   Model vocab: {actual_vocab_size}\")\n",
    "\n",
    "    if len(medgemma_tokenizer) > actual_vocab_size:\n",
    "        print(f\"\\n   ERROR: Tokenizer is LARGER than model!\")\n",
    "        print(f\"   This WILL cause CUDA errors!\")\n",
    "        print(f\"\\n   SOLUTION: Resizing model embeddings to {len(medgemma_tokenizer)}...\")\n",
    "        medgemma_model.resize_token_embeddings(len(medgemma_tokenizer))\n",
    "        print(f\"   âœ“ Model embeddings resized\")\n",
    "\n",
    "        # Update actual vocab size\n",
    "        actual_vocab_size = medgemma_model.get_input_embeddings().weight.shape[0]\n",
    "        print(f\"   âœ“ New model vocab size: {actual_vocab_size}\")\n",
    "    else:\n",
    "        print(f\"\\n   WARNING: Model is larger than tokenizer\")\n",
    "        print(f\"   This is unusual but may work if all token IDs < {actual_vocab_size}\")\n",
    "\n",
    "# Step 6: VALIDATION TEST\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"CRITICAL VALIDATION TEST\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "test_text = \"Patient presented with chest pain.\"\n",
    "test_tokens = medgemma_tokenizer(test_text, return_tensors=\"pt\")\n",
    "max_id = test_tokens['input_ids'].max().item()\n",
    "min_id = test_tokens['input_ids'].min().item()\n",
    "\n",
    "print(f\"Test text: '{test_text}'\")\n",
    "print(f\"Token IDs: {test_tokens['input_ids'][0].tolist()}\")\n",
    "print(f\"Max token ID: {max_id}\")\n",
    "print(f\"Min token ID: {min_id}\")\n",
    "print(f\"Valid range: [0, {actual_vocab_size - 1}]\")\n",
    "\n",
    "if max_id >= actual_vocab_size:\n",
    "    print(f\"\\nâŒ CRITICAL ERROR: Token ID {max_id} >= vocab size {actual_vocab_size}\")\n",
    "    print(f\"   This WILL cause the CUDA error you're seeing!\")\n",
    "    print(f\"\\n   IMMEDIATE FIX REQUIRED:\")\n",
    "    print(f\"   medgemma_model.resize_token_embeddings({len(medgemma_tokenizer)})\")\n",
    "    raise ValueError(f\"Token ID out of range: {max_id} >= {actual_vocab_size}\")\n",
    "elif min_id < 0:\n",
    "    print(f\"\\nâŒ CRITICAL ERROR: Negative token ID {min_id}\")\n",
    "    raise ValueError(f\"Invalid token ID: {min_id}\")\n",
    "else:\n",
    "    print(f\"\\nâœ… VALIDATION PASSED!\")\n",
    "    print(f\"   All token IDs are within valid range\")\n",
    "    print(f\"   Safe to proceed with generation\")\n",
    "\n",
    "medgemma_model.eval()\n",
    "print(f\"\\nâœ“ MedGemma ready for inference\")\n",
    "flush_memory()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 1: LOADING MEDGEMMA-4B FOR VERBOSE SUMMARY GENERATION\n",
      "================================================================================\n",
      "\n",
      "Loading base model: google/medgemma-4b-it\n",
      "  Quantization: 4-bit NF4\n",
      "  This may take 2-3 minutes...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51d96e9da01a48ce89af688bc7706278"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Base model loaded\n",
      "\n",
      "Loading LoRA adapters from: ./medgemma-discharge-summarization/final\n",
      "âœ“ LoRA adapters loaded\n",
      "\n",
      "  Model embedding vocab size: 262208\n",
      "\n",
      "Attempting to load tokenizer from adapter path: ./medgemma-discharge-summarization/final\n",
      "âœ“ Tokenizer loaded from adapter path\n",
      "\n",
      "  Tokenizer vocab size: 262145\n",
      "  PAD token ID: 1\n",
      "  EOS token ID: 1\n",
      "\n",
      "âš ï¸  MISMATCH DETECTED!\n",
      "   Tokenizer vocab: 262145\n",
      "   Model vocab: 262208\n",
      "\n",
      "   WARNING: Model is larger than tokenizer\n",
      "   This is unusual but may work if all token IDs < 262208\n",
      "\n",
      "================================================================================\n",
      "CRITICAL VALIDATION TEST\n",
      "================================================================================\n",
      "Test text: 'Patient presented with chest pain.'\n",
      "Token IDs: [2, 52420, 6212, 607, 15350, 4331, 236761, 1]\n",
      "Max token ID: 236761\n",
      "Min token ID: 1\n",
      "Valid range: [0, 262207]\n",
      "\n",
      "âœ… VALIDATION PASSED!\n",
      "   All token IDs are within valid range\n",
      "   Safe to proceed with generation\n",
      "\n",
      "âœ“ MedGemma ready for inference\n",
      "GPU Memory: 3.50 GB allocated, 3.51 GB reserved\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:41:59.825487Z",
     "start_time": "2025-12-04T10:41:59.733635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# FINAL VALIDATION BEFORE GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL PRE-GENERATION CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get actual embedding size\n",
    "embedding_size = medgemma_model.get_input_embeddings().weight.shape[0]\n",
    "\n",
    "print(f\"\\nTokenizer vocab: {len(medgemma_tokenizer)}\")\n",
    "print(f\"Model embedding size: {embedding_size}\")\n",
    "print(f\"Match: {'âœ… YES' if len(medgemma_tokenizer) == embedding_size else 'âŒ NO'}\")\n",
    "\n",
    "# Test with actual prompt format from your generation function\n",
    "test_prompt = \"\"\"<start_of_turn>user\n",
    "Summarize the following clinical discharge notes.\n",
    "\n",
    "Clinical Notes:\n",
    "Patient with hypertension.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nTesting with actual prompt format...\")\n",
    "test_inputs = medgemma_tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "max_token = test_inputs['input_ids'].max().item()\n",
    "\n",
    "print(f\"  Prompt token count: {test_inputs['input_ids'].shape[1]}\")\n",
    "print(f\"  Max token ID in prompt: {max_token}\")\n",
    "print(f\"  Valid range: [0, {embedding_size - 1}]\")\n",
    "\n",
    "if max_token >= embedding_size:\n",
    "    print(f\"\\nâŒ STOP! Token ID {max_token} is out of range!\")\n",
    "    print(f\"   DO NOT PROCEED - will cause CUDA error\")\n",
    "    print(f\"\\n   Run this fix:\")\n",
    "    print(f\"   medgemma_model.resize_token_embeddings({len(medgemma_tokenizer)})\")\n",
    "else:\n",
    "    print(f\"\\nâœ… All checks passed - safe to generate summaries\")\n",
    "\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL PRE-GENERATION CHECK\n",
      "================================================================================\n",
      "\n",
      "Tokenizer vocab: 262145\n",
      "Model embedding size: 262208\n",
      "Match: âŒ NO\n",
      "\n",
      "Testing with actual prompt format...\n",
      "  Prompt token count: 27\n",
      "  Max token ID in prompt: 236787\n",
      "  Valid range: [0, 262207]\n",
      "\n",
      "âœ… All checks passed - safe to generate summaries\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:42:04.021115Z",
     "start_time": "2025-12-04T10:42:04.012743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_stage1_summary(clinical_note: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate verbose clinical summary using MedGemma.\n",
    "    \n",
    "    Args:\n",
    "        clinical_note: Input clinical text\n",
    "    \n",
    "    Returns:\n",
    "        Generated summary (high-recall, verbose)\n",
    "    \"\"\"\n",
    "    instruction = (\n",
    "        \"Summarize the following clinical discharge notes. \"\n",
    "        \"Include ALL diagnoses, medications, vitals, lab results, \"\n",
    "        \"procedures, and follow-up instructions. \"\n",
    "        \"Ensure complete coverage of all medical entities.\"\n",
    "    )\n",
    "\n",
    "    # Format using Gemma template\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{clinical_note}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    inputs = medgemma_tokenizer(prompt, return_tensors=\"pt\").to(medgemma_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = medgemma_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=STAGE1_MAX_TOKENS,\n",
    "            temperature=STAGE1_TEMPERATURE,\n",
    "            top_p=STAGE1_TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=medgemma_tokenizer.pad_token_id,\n",
    "            eos_token_id=medgemma_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = medgemma_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the model's response\n",
    "    marker = \"<start_of_turn>model\"\n",
    "    if marker in generated_text:\n",
    "        summary = generated_text.split(marker)[-1].strip()\n",
    "    else:\n",
    "        summary = generated_text[len(prompt):].strip()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "print(\"âœ“ Stage 1 generation function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Stage 1 generation function defined\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:46:26.563961Z",
     "start_time": "2025-12-04T10:42:11.599477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Stage 1 summaries for all samples\n",
    "print(f\"\\nGenerating Stage 1 summaries for {len(df)} samples...\\n\")\n",
    "\n",
    "stage1_summaries = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    clinical_note = row['final_input']\n",
    "\n",
    "    print(f\"[{idx + 1}/{len(df)}] Generating verbose summary...\", end=\" \")\n",
    "    summary = generate_stage1_summary(clinical_note)\n",
    "    stage1_summaries.append(summary)\n",
    "    print(f\"âœ“ ({len(summary)} chars)\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['stage1_summary'] = stage1_summaries\n",
    "\n",
    "print(f\"\\nâœ“ Stage 1 complete: {len(stage1_summaries)} summaries generated\")\n",
    "print(f\"  Average length: {np.mean([len(s) for s in stage1_summaries]):.0f} characters\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STAGE 1 OUTPUT SAMPLE:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(stage1_summaries[0][:400] + \"...\")\n",
    "print(f\"{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Stage 1 summaries for 10 samples...\n",
      "\n",
      "[1/10] Generating verbose summary... âœ“ (896 chars)\n",
      "[2/10] Generating verbose summary... âœ“ (2766 chars)\n",
      "[3/10] Generating verbose summary... âœ“ (383 chars)\n",
      "[4/10] Generating verbose summary... âœ“ (1163 chars)\n",
      "[5/10] Generating verbose summary... âœ“ (641 chars)\n",
      "[6/10] Generating verbose summary... âœ“ (788 chars)\n",
      "[7/10] Generating verbose summary... âœ“ (104 chars)\n",
      "[8/10] Generating verbose summary... âœ“ (67 chars)\n",
      "[9/10] Generating verbose summary... âœ“ (66 chars)\n",
      "[10/10] Generating verbose summary... âœ“ (1746 chars)\n",
      "\n",
      "âœ“ Stage 1 complete: 10 summaries generated\n",
      "  Average length: 862 characters\n",
      "\n",
      "================================================================================\n",
      "STAGE 1 OUTPUT SAMPLE:\n",
      "================================================================================\n",
      "ascites hiv art ho ivdu copd bioplar ptsd present worsening abd distension ascites portal hypertension ascites portal htn ascites portal hypertension patient presented worsening abdominal distension ascites setting spironolactone furosemide held one week prior presentation paracentesis ed removed l fluid spironolactone mg daily lasix mg daily home patient given mg iv lasix mg iv spironolactone ed ...\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CRITICAL: Unload MedGemma Before Stage 2\n",
    "\n",
    "**This step is NON-NEGOTIABLE for single-GPU execution.**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:46:28.063867Z",
     "start_time": "2025-12-04T10:46:26.820840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNLOADING MEDGEMMA-4B FROM MEMORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "unload_model(medgemma_model, medgemma_tokenizer)\n",
    "\n",
    "# Verify memory is cleared\n",
    "print(\"Memory status after unloading:\")\n",
    "flush_memory()\n",
    "\n",
    "print(\"\\nâœ“ Safe to proceed to Stage 2\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UNLOADING MEDGEMMA-4B FROM MEMORY\n",
      "================================================================================\n",
      "\n",
      "ðŸ§¹ Unloading model from memory...\n",
      "GPU Memory: 0.01 GB allocated, 0.02 GB reserved\n",
      "âœ“ Model unloaded\n",
      "\n",
      "Memory status after unloading:\n",
      "GPU Memory: 0.01 GB allocated, 0.02 GB reserved\n",
      "\n",
      "âœ“ Safe to proceed to Stage 2\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 7: STAGE 2 - Llama-3 Compression\n",
    "\n",
    "**Objective**: Compress verbose summaries while preserving medical entities.\n",
    "\n",
    "**Chain-of-Density Inspired Prompt**: Forces 50% compression with entity retention constraint."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:49:09.242593Z",
     "start_time": "2025-12-04T10:49:09.230611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5c8f6cf19de4b88b86515b540dc2db9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:20:59.594174Z",
     "start_time": "2025-12-04T11:20:41.910891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 2: LOADING LLAMA-3-8B FOR COMPRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if we need HuggingFace authentication for Llama\n",
    "if IS_COLAB:\n",
    "    print(\"\\nâš ï¸  Llama-3 requires HuggingFace authentication\")\n",
    "    print(\"   Running authentication flow...\\n\")\n",
    "    from huggingface_hub import notebook_login\n",
    "\n",
    "    notebook_login()\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {LLAMA_MODEL}\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LLAMA_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"  # Llama uses left padding\n",
    ")\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "print(\"âœ“ Tokenizer loaded\")\n",
    "\n",
    "# Load model with quantization\n",
    "print(f\"\\nLoading model: {LLAMA_MODEL}\")\n",
    "print(\"  Quantization: 4-bit NF4\")\n",
    "print(\"  This may take 2-3 minutes...\")\n",
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "llama_model.eval()\n",
    "print(f\"\\nâœ“ Llama-3 ready for compression\")\n",
    "flush_memory()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 2: LOADING LLAMA-3-8B FOR COMPRESSION\n",
      "================================================================================\n",
      "\n",
      "Loading tokenizer: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "âœ“ Tokenizer loaded\n",
      "\n",
      "Loading model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  Quantization: 4-bit NF4\n",
      "  This may take 2-3 minutes...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd901763eda04e8dae3d71b08ddc465e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Llama-3 ready for compression\n",
      "GPU Memory: 6.07 GB allocated, 6.07 GB reserved\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:20:59.613756Z",
     "start_time": "2025-12-04T11:20:59.600218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_compressed_summary(verbose_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Compress summary using Llama-3 with entity retention constraint.\n",
    "    \n",
    "    Chain-of-Density inspired prompt: Forces 50% compression while\n",
    "    preserving all medical entities (medications, numbers, dates).\n",
    "    \n",
    "    Args:\n",
    "        verbose_summary: Stage 1 summary (high-recall)\n",
    "    \n",
    "    Returns:\n",
    "        Compressed summary\n",
    "    \"\"\"\n",
    "    # Chain-of-Density inspired compression prompt\n",
    "    system_prompt = (\n",
    "        \"You are a medical summarization expert. Your task is to compress \"\n",
    "        \"clinical summaries while preserving critical information.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Rewrite the following summary to be 50% shorter. \"\n",
    "        \"You MUST retain ALL entities: medications (with dosages), \"\n",
    "        \"vital signs (with numbers), lab results (with values), \"\n",
    "        \"diagnoses, procedures, and dates. \"\n",
    "        \"If you cannot shorten it without losing a critical fact, do not shorten it. \"\n",
    "        \"Remove only redundant phrasing and verbose descriptions. \"\n",
    "        \"CRITICAL OUTPUT INSTRUCTIONS:\"\n",
    "        \"\\n- Output ONLY the rewritten summary.\"\n",
    "        \"\\n- Do NOT provide an explanation of what you did.\"\n",
    "        \"\\n- Do NOT list the entities you preserved.\"\n",
    "        \"\\n- Do NOT include any text before or after the summary.\\n\\n\"\n",
    "        f\"Summary to compress:\\n{\"\"}\"\n",
    "    )\n",
    "\n",
    "    # Format using Llama-3 chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    prompt = llama_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(llama_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=STAGE2_MAX_TOKENS,\n",
    "            temperature=STAGE2_TEMPERATURE,\n",
    "            top_p=STAGE2_TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=llama_tokenizer.pad_token_id,\n",
    "            eos_token_id=llama_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the assistant's response\n",
    "    # Llama-3 format: <|start_header_id|>assistant<|end_header_id|>\\n\\n{response}\n",
    "    if \"assistant\" in generated_text:\n",
    "        compressed = generated_text.split(\"assistant\")[-1].strip()\n",
    "        # Remove any remaining header artifacts\n",
    "        compressed = compressed.split(\"\\n\\n\", 1)[-1] if \"\\n\\n\" in compressed else compressed\n",
    "    else:\n",
    "        compressed = generated_text[len(prompt):].strip()\n",
    "\n",
    "    return compressed\n",
    "\n",
    "\n",
    "print(\"âœ“ Stage 2 compression function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Stage 2 compression function defined\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:24:21.747290Z",
     "start_time": "2025-12-04T11:21:15.884397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Stage 2 compressed summaries\n",
    "print(f\"\\nCompressing {len(df)} Stage 1 summaries...\\n\")\n",
    "\n",
    "stage2_summaries = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    verbose_summary = row['stage1_summary']\n",
    "\n",
    "    print(f\"[{idx + 1}/{len(df)}] Compressing summary...\", end=\" \")\n",
    "    compressed = generate_compressed_summary(verbose_summary)\n",
    "    stage2_summaries.append(compressed)\n",
    "\n",
    "    # Calculate compression ratio\n",
    "    ratio = len(compressed) / len(verbose_summary)\n",
    "    print(f\"âœ“ ({len(compressed)} chars, {ratio:.2%} of original)\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['stage2_summary'] = stage2_summaries\n",
    "\n",
    "print(f\"\\nâœ“ Stage 2 complete: {len(stage2_summaries)} summaries compressed\")\n",
    "print(f\"  Average length: {np.mean([len(s) for s in stage2_summaries]):.0f} characters\")\n",
    "print(\n",
    "    f\"  Overall compression: {np.mean([len(s2) / len(s1) for s1, s2 in zip(stage1_summaries, stage2_summaries)]):.2%}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STAGE 2 OUTPUT SAMPLE:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(stage2_summaries[0])\n",
    "print(f\"{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compressing 10 Stage 1 summaries...\n",
      "\n",
      "[1/10] Compressing summary... âœ“ (825 chars, 92.08% of original)\n",
      "[2/10] Compressing summary... âœ“ (359 chars, 12.98% of original)\n",
      "[3/10] Compressing summary... âœ“ (668 chars, 174.41% of original)\n",
      "[4/10] Compressing summary... âœ“ (816 chars, 70.16% of original)\n",
      "[5/10] Compressing summary... âœ“ (516 chars, 80.50% of original)\n",
      "[6/10] Compressing summary... âœ“ (392 chars, 49.75% of original)\n",
      "[7/10] Compressing summary... âœ“ (840 chars, 807.69% of original)\n",
      "[8/10] Compressing summary... âœ“ (712 chars, 1062.69% of original)\n",
      "[9/10] Compressing summary... âœ“ (519 chars, 786.36% of original)\n",
      "[10/10] Compressing summary... âœ“ (785 chars, 44.96% of original)\n",
      "\n",
      "âœ“ Stage 2 complete: 10 summaries compressed\n",
      "  Average length: 643 characters\n",
      "  Overall compression: 318.16%\n",
      "\n",
      "================================================================================\n",
      "STAGE 2 OUTPUT SAMPLE:\n",
      "================================================================================\n",
      "A 65-year-old male patient was admitted to the hospital on 2022-02-15 with a 2-day history of worsening chest pain and shortness of breath. He was treated with aspirin 325mg and metoprolol 25mg daily. On admission, his vital signs were: blood pressure 160/90mmHg, heart rate 110bpm, respiratory rate 20/min, and oxygen saturation 88% on room air. Laboratory results showed: troponin 0.5ng/mL, creatine kinase 150IU/L, and white blood cell count 12,000cells/Î¼L. The patient underwent an echocardiogram, which revealed a 30% ejection fraction. He was diagnosed with non-ST elevation myocardial infarction and underwent an urgent coronary angioplasty with stenting of the left anterior descending artery. The patient was discharged on 2022-02-22 with a plan for continued metoprolol therapy and scheduled follow-up appointments.\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unload Llama-3 (Optional - only if running evaluation separately)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:24:30.580320Z",
     "start_time": "2025-12-04T11:24:28.613892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you need to free memory before evaluation, uncomment:\n",
    "unload_model(llama_model, llama_tokenizer)\n",
    "\n",
    "print(\"âœ“ Stage 2 generation complete. Proceeding to evaluation...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¹ Unloading model from memory...\n",
      "GPU Memory: 0.36 GB allocated, 0.52 GB reserved\n",
      "âœ“ Model unloaded\n",
      "\n",
      "âœ“ Stage 2 generation complete. Proceeding to evaluation...\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 8: EVALUATION - Metric 1: Compression Ratio"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:24:37.431062Z",
     "start_time": "2025-12-04T11:24:37.385967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 1: COMPRESSION RATIO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def calculate_compression_ratio(stage1: str, stage2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate compression ratio (Stage 2 length / Stage 1 length).\n",
    "    \n",
    "    Lower values indicate stronger compression.\n",
    "    \"\"\"\n",
    "    return len(stage2) / len(stage1)\n",
    "\n",
    "\n",
    "# Calculate for all samples\n",
    "df['compression_ratio'] = df.apply(\n",
    "    lambda row: calculate_compression_ratio(row['stage1_summary'], row['stage2_summary']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nCompression Statistics:\")\n",
    "print(f\"  Mean ratio: {df['compression_ratio'].mean():.2%}\")\n",
    "print(f\"  Median ratio: {df['compression_ratio'].median():.2%}\")\n",
    "print(f\"  Min ratio: {df['compression_ratio'].min():.2%}\")\n",
    "print(f\"  Max ratio: {df['compression_ratio'].max():.2%}\")\n",
    "print(f\"  Std dev: {df['compression_ratio'].std():.2%}\")\n",
    "\n",
    "print(f\"\\nâœ“ Compression ratios calculated\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRIC 1: COMPRESSION RATIO\n",
      "================================================================================\n",
      "\n",
      "Compression Statistics:\n",
      "  Mean ratio: 318.16%\n",
      "  Median ratio: 86.29%\n",
      "  Min ratio: 12.98%\n",
      "  Max ratio: 1062.69%\n",
      "  Std dev: 400.38%\n",
      "\n",
      "âœ“ Compression ratios calculated\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 9: EVALUATION - Metric 2: Entity Retention (NER)\n",
    "\n",
    "**Method**: Extract medical entities from both Stage 1 and Stage 2 summaries using SciSpacy, then calculate recall."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:24:47.677843Z",
     "start_time": "2025-12-04T11:24:47.059349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 2: ENTITY RETENTION (NER)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load SciSpacy medical NER model\n",
    "print(f\"\\nLoading SciSpacy model: {SPACY_MODEL}\")\n",
    "nlp = spacy.load(SPACY_MODEL)\n",
    "print(\"âœ“ NER model loaded\")\n",
    "\n",
    "\n",
    "def extract_entities(text: str) -> set:\n",
    "    \"\"\"\n",
    "    Extract medical entities from text using SciSpacy.\n",
    "    \n",
    "    Returns set of entity texts (lowercased for comparison).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = {ent.text.lower() for ent in doc.ents}\n",
    "    return entities\n",
    "\n",
    "\n",
    "def calculate_entity_recall(stage1_text: str, stage2_text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate entity retention recall: \n",
    "    (Entities in Stage 2) / (Entities in Stage 1)\n",
    "    \n",
    "    Returns value between 0 and 1 (higher is better).\n",
    "    \"\"\"\n",
    "    entities_stage1 = extract_entities(stage1_text)\n",
    "    entities_stage2 = extract_entities(stage2_text)\n",
    "\n",
    "    if len(entities_stage1) == 0:\n",
    "        return 1.0  # No entities to preserve\n",
    "\n",
    "    # Calculate recall: how many Stage 1 entities appear in Stage 2?\n",
    "    retained = entities_stage1.intersection(entities_stage2)\n",
    "    recall = len(retained) / len(entities_stage1)\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "print(\"\\nCalculating entity recall for all samples...\\n\")\n",
    "\n",
    "entity_recalls = []\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"[{idx + 1}/{len(df)}] Extracting entities...\", end=\" \")\n",
    "    recall = calculate_entity_recall(row['stage1_summary'], row['stage2_summary'])\n",
    "    entity_recalls.append(recall)\n",
    "    print(f\"âœ“ Recall: {recall:.2%}\")\n",
    "\n",
    "df['entity_recall'] = entity_recalls\n",
    "\n",
    "print(f\"\\nEntity Retention Statistics:\")\n",
    "print(f\"  Mean recall: {df['entity_recall'].mean():.2%}\")\n",
    "print(f\"  Median recall: {df['entity_recall'].median():.2%}\")\n",
    "print(f\"  Min recall: {df['entity_recall'].min():.2%}\")\n",
    "print(f\"  Max recall: {df['entity_recall'].max():.2%}\")\n",
    "print(f\"  Std dev: {df['entity_recall'].std():.2%}\")\n",
    "\n",
    "print(f\"\\nâœ“ Entity recall calculated\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRIC 2: ENTITY RETENTION (NER)\n",
      "================================================================================\n",
      "\n",
      "Loading SciSpacy model: en_core_sci_sm\n",
      "âœ“ NER model loaded\n",
      "\n",
      "Calculating entity recall for all samples...\n",
      "\n",
      "[1/10] Extracting entities... âœ“ Recall: 12.12%\n",
      "[2/10] Extracting entities... âœ“ Recall: 7.69%\n",
      "[3/10] Extracting entities... âœ“ Recall: 6.25%\n",
      "[4/10] Extracting entities... âœ“ Recall: 0.00%\n",
      "[5/10] Extracting entities... âœ“ Recall: 5.56%\n",
      "[6/10] Extracting entities... âœ“ Recall: 3.33%\n",
      "[7/10] Extracting entities... âœ“ Recall: 0.00%\n",
      "[8/10] Extracting entities... âœ“ Recall: 0.00%\n",
      "[9/10] Extracting entities... âœ“ Recall: 0.00%\n",
      "[10/10] Extracting entities... âœ“ Recall: 6.82%\n",
      "\n",
      "Entity Retention Statistics:\n",
      "  Mean recall: 4.18%\n",
      "  Median recall: 4.44%\n",
      "  Min recall: 0.00%\n",
      "  Max recall: 12.12%\n",
      "  Std dev: 4.21%\n",
      "\n",
      "âœ“ Entity recall calculated\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 10: EVALUATION - Metric 3: Clinical BERTScore\n",
    "\n",
    "**Method**: Measure semantic similarity between original clinical notes and final compressed summaries using Bio_ClinicalBERT."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:24:58.223838Z",
     "start_time": "2025-12-04T11:24:55.437971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 3: CLINICAL BERTSCORE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize Clinical BERTScore\n",
    "print(f\"\\nInitializing BERTScorer with {CLINICAL_BERT}\")\n",
    "print(\"This will download the model if not cached...\\n\")\n",
    "\n",
    "bert_scorer = BERTScorer(\n",
    "    model_type=CLINICAL_BERT,\n",
    "    num_layers=9,\n",
    "    rescale_with_baseline=False,\n",
    "    lang=\"en\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ BERTScorer initialized\")\n",
    "\n",
    "# Get the tokenizer from the scorer to do proper truncation\n",
    "bert_tokenizer = bert_scorer._tokenizer\n",
    "\n",
    "\n",
    "def truncate_with_bert_tokenizer(text: str, tokenizer, max_length: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Properly truncate text using BERT's tokenizer to ensure it fits within token limit.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to truncate\n",
    "        tokenizer: BERT tokenizer\n",
    "        max_length: Maximum number of tokens (BERT supports 512, we use 500 for safety)\n",
    "\n",
    "    Returns:\n",
    "        Truncated text that will tokenize to <= max_length tokens\n",
    "    \"\"\"\n",
    "    # Tokenize and truncate\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    # Decode back to text\n",
    "    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return truncated_text\n",
    "\n",
    "\n",
    "# Truncate texts to fit BERT's 512 token limit\n",
    "print(\"\\nPreparing texts (truncating long sequences for BERT)...\")\n",
    "print(\"  Using BERT tokenizer for accurate truncation...\")\n",
    "\n",
    "# Prepare data for batch scoring\n",
    "source_texts = df['target'].tolist()\n",
    "compressed_summaries = df['stage2_summary'].tolist()\n",
    "\n",
    "truncated_predictions = []\n",
    "truncated_references = []\n",
    "\n",
    "for pred, ref in zip(compressed_summaries, source_texts):\n",
    "    truncated_predictions.append(truncate_with_bert_tokenizer(pred, bert_tokenizer))\n",
    "    truncated_references.append(truncate_with_bert_tokenizer(ref, bert_tokenizer))\n",
    "\n",
    "# Check truncation statistics\n",
    "orig_pred_lens = [len(bert_tokenizer.encode(p)) for p in compressed_summaries]\n",
    "trunc_pred_lens = [len(bert_tokenizer.encode(p)) for p in truncated_predictions]\n",
    "num_truncated = sum(1 for o, t in zip(orig_pred_lens, trunc_pred_lens) if o != t)\n",
    "\n",
    "print(f\"  {num_truncated}/{len(compressed_summaries)} predictions were truncated\")\n",
    "print(f\"  Average prediction tokens: {np.mean(trunc_pred_lens):.0f}\")\n",
    "print(f\"  Max prediction tokens: {max(trunc_pred_lens)}\")\n",
    "\n",
    "print(\"\\nCalculating BERTScores (this may take a few minutes)...\\n\")\n",
    "\n",
    "# Calculate BERTScore\n",
    "# We compare compressed summaries (candidates) against original notes (references)\n",
    "P, R, F1 = bert_scorer.score(\n",
    "    cands=truncated_predictions,\n",
    "    refs=truncated_references\n",
    ")\n",
    "\n",
    "# Convert to numpy\n",
    "df['bertscore_precision'] = P.cpu().numpy()\n",
    "df['bertscore_recall'] = R.cpu().numpy()\n",
    "df['bertscore_f1'] = F1.cpu().numpy()\n",
    "\n",
    "print(f\"\\nClinical BERTScore Statistics:\")\n",
    "print(f\"\\nPrecision:\")\n",
    "print(f\"  Mean: {df['bertscore_precision'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_precision'].median():.4f}\")\n",
    "print(f\"\\nRecall:\")\n",
    "print(f\"  Mean: {df['bertscore_recall'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_recall'].median():.4f}\")\n",
    "print(f\"\\nF1:\")\n",
    "print(f\"  Mean: {df['bertscore_f1'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_f1'].median():.4f}\")\n",
    "\n",
    "print(f\"\\nâœ“ Clinical BERTScore calculated\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRIC 3: CLINICAL BERTSCORE\n",
      "================================================================================\n",
      "\n",
      "Initializing BERTScorer with emilyalsentzer/Bio_ClinicalBERT\n",
      "This will download the model if not cached...\n",
      "\n",
      "âœ“ BERTScorer initialized\n",
      "\n",
      "Preparing texts (truncating long sequences for BERT)...\n",
      "  Using BERT tokenizer for accurate truncation...\n",
      "  0/10 predictions were truncated\n",
      "  Average prediction tokens: 182\n",
      "  Max prediction tokens: 222\n",
      "\n",
      "Calculating BERTScores (this may take a few minutes)...\n",
      "\n",
      "\n",
      "Clinical BERTScore Statistics:\n",
      "\n",
      "Precision:\n",
      "  Mean: 0.5747\n",
      "  Median: 0.5623\n",
      "\n",
      "Recall:\n",
      "  Mean: 0.5708\n",
      "  Median: 0.5709\n",
      "\n",
      "F1:\n",
      "  Mean: 0.5725\n",
      "  Median: 0.5721\n",
      "\n",
      "âœ“ Clinical BERTScore calculated\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 11: Results Summary and Export"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:25:24.666704Z",
     "start_time": "2025-12-04T11:25:24.659025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select columns for final output\n",
    "results_df = df[[\n",
    "    'input',\n",
    "    'final_input',\n",
    "    'stage1_summary',\n",
    "    'stage2_summary',\n",
    "    'target',\n",
    "    'compression_ratio',\n",
    "    'entity_recall',\n",
    "    'bertscore_precision',\n",
    "    'bertscore_recall',\n",
    "    'bertscore_f1'\n",
    "]].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "results_df.columns = [\n",
    "    'Source_Text',\n",
    "    'Source_Text_Processed',\n",
    "    'Stage1_Summary_Verbose',\n",
    "    'Stage2_Summary_Compressed',\n",
    "    'Target_Summary',\n",
    "    'Compression_Ratio',\n",
    "    'Entity_Recall',\n",
    "    'BERTScore_Precision',\n",
    "    'BERTScore_Recall',\n",
    "    'BERTScore_F1'\n",
    "]\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nProcessed {len(results_df)} samples\\n\")\n",
    "print(\"Aggregate Metrics:\")\n",
    "print(\n",
    "    f\"  Compression Ratio: {results_df['Compression_Ratio'].mean():.2%} Â± {results_df['Compression_Ratio'].std():.2%}\")\n",
    "print(f\"  Entity Recall: {results_df['Entity_Recall'].mean():.2%} Â± {results_df['Entity_Recall'].std():.2%}\")\n",
    "print(f\"  BERTScore F1: {results_df['BERTScore_F1'].mean():.4f} Â± {results_df['BERTScore_F1'].std():.4f}\")\n",
    "\n",
    "# Display sample comparison\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"SAMPLE COMPARISON (First Example)\")\n",
    "print(f\"{'=' * 80}\")\n",
    "sample = results_df.iloc[0]\n",
    "print(f\"\\nSource Text Original (truncated): {sample['Source_Text'][:200]}...\")\n",
    "print(f\"\\nSource Text Processed (truncated): {sample['Source_Text_Processed'][:200]}...\")\n",
    "print(f\"\\nStage 1 (Verbose): {sample['Stage1_Summary_Verbose'][:300]}...\")\n",
    "print(f\"\\nStage 2 (Compressed): {sample['Stage2_Summary_Compressed']}\")\n",
    "print(f\"\\nTarget Summary: {sample['Target_Summary']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Compression: {sample['Compression_Ratio']:.2%}\")\n",
    "print(f\"  Entity Recall: {sample['Entity_Recall']:.2%}\")\n",
    "print(f\"  BERTScore F1: {sample['BERTScore_F1']:.4f}\")\n",
    "print(f\"{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Processed 10 samples\n",
      "\n",
      "Aggregate Metrics:\n",
      "  Compression Ratio: 318.16% Â± 400.38%\n",
      "  Entity Recall: 4.18% Â± 4.21%\n",
      "  BERTScore F1: 0.5725 Â± 0.0196\n",
      "\n",
      "================================================================================\n",
      "SAMPLE COMPARISON (First Example)\n",
      "================================================================================\n",
      "\n",
      "Source Text Original (truncated): summarize chief complaint worsening abd distension and pain history of present illness hcv cirrhosis cb ascites hiv on art ho ivdu copd bioplar ptsd presented from osh ed with worsening abd distension...\n",
      "\n",
      "Source Text Processed (truncated): summarize chief complaint worsening abd distension pain history present illness hcv cirrhosis cb ascites hiv art ho ivdu copd bioplar ptsd presented osh ed worsening abd distension past week pt report...\n",
      "\n",
      "Stage 1 (Verbose): ascites hiv art ho ivdu copd bioplar ptsd present worsening abd distension ascites portal hypertension ascites portal htn ascites portal hypertension patient presented worsening abdominal distension ascites setting spironolactone furosemide held one week prior presentation paracentesis ed removed l ...\n",
      "\n",
      "Stage 2 (Compressed): A 65-year-old male patient was admitted to the hospital on 2022-02-15 with a 2-day history of worsening chest pain and shortness of breath. He was treated with aspirin 325mg and metoprolol 25mg daily. On admission, his vital signs were: blood pressure 160/90mmHg, heart rate 110bpm, respiratory rate 20/min, and oxygen saturation 88% on room air. Laboratory results showed: troponin 0.5ng/mL, creatine kinase 150IU/L, and white blood cell count 12,000cells/Î¼L. The patient underwent an echocardiogram, which revealed a 30% ejection fraction. He was diagnosed with non-ST elevation myocardial infarction and underwent an urgent coronary angioplasty with stenting of the left anterior descending artery. The patient was discharged on 2022-02-22 with a plan for continued metoprolol therapy and scheduled follow-up appointments.\n",
      "\n",
      "Target Summary: hcv cirrhosis cb ascites hiv on art ho ivdu copd bioplar ptsd presented from osh ed with worsening abd distension over past week and confusion ascites pw worsening abd distension and discomfort for last week likely portal htn given underlying liver disease though no ascitic fluid available on night of admission no signs of heart failure noted on exam this was to med noncompliance and lack of diet restriction sbp negative diuretics furosemide mg po daily spironolactone mg po daily chosen over the usual mg dose dt k of cxr was wnl ua negative urine culture blood culture negative pt was losing excess fluid appropriately with stable lytes on the above regimen pt was scheduled with current pcp for check upon discharge pt was scheduled for new pcp with dr at and follow up in liver clinic to schedule outpatient screening egd and\n",
      "\n",
      "Metrics:\n",
      "  Compression: 92.08%\n",
      "  Entity Recall: 12.12%\n",
      "  BERTScore F1: 0.5629\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:27:29.260537Z",
     "start_time": "2025-12-04T11:27:29.251698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save results\n",
    "output_csv_path = os.path.join(OUTPUT_DIR, \"compression_pipeline_results.csv\")\n",
    "output_json_path = os.path.join(OUTPUT_DIR, \"compression_pipeline_results.json\")\n",
    "\n",
    "# Save as CSV\n",
    "results_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nâœ“ Results saved to CSV: {output_csv_path}\")\n",
    "\n",
    "# Save as JSON (for easier inspection)\n",
    "results_df.to_json(output_json_path, orient='records', indent=2)\n",
    "print(f\"âœ“ Results saved to JSON: {output_json_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"num_samples\": len(results_df),\n",
    "    \"compression_ratio\": {\n",
    "        \"mean\": float(results_df['Compression_Ratio'].mean()),\n",
    "        \"std\": float(results_df['Compression_Ratio'].std()),\n",
    "        \"min\": float(results_df['Compression_Ratio'].min()),\n",
    "        \"max\": float(results_df['Compression_Ratio'].max())\n",
    "    },\n",
    "    \"entity_recall\": {\n",
    "        \"mean\": float(results_df['Entity_Recall'].mean()),\n",
    "        \"std\": float(results_df['Entity_Recall'].std()),\n",
    "        \"min\": float(results_df['Entity_Recall'].min()),\n",
    "        \"max\": float(results_df['Entity_Recall'].max())\n",
    "    },\n",
    "    \"bertscore_f1\": {\n",
    "        \"mean\": float(results_df['BERTScore_F1'].mean()),\n",
    "        \"std\": float(results_df['BERTScore_F1'].std()),\n",
    "        \"min\": float(results_df['BERTScore_F1'].min()),\n",
    "        \"max\": float(results_df['BERTScore_F1'].max())\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"stage1_model\": MEDGEMMA_BASE_MODEL,\n",
    "        \"stage2_model\": LLAMA_MODEL,\n",
    "        \"num_samples_processed\": NUM_SAMPLES if NUM_SAMPLES > 0 else \"all\"\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"summary_statistics.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Summary statistics saved: {summary_path}\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(f\"\\nðŸ“ All outputs saved to Google Drive: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“ All outputs saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Results saved to CSV: ./compression_results\\compression_pipeline_results.csv\n",
      "âœ“ Results saved to JSON: ./compression_results\\compression_pipeline_results.json\n",
      "âœ“ Summary statistics saved: ./compression_results\\summary_statistics.json\n",
      "\n",
      "ðŸ“ All outputs saved to: ./compression_results\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 12: Analysis and Insights\n",
    "\n",
    "Key questions to investigate:\n",
    "1. **Compression vs Quality Trade-off**: Does higher compression correlate with lower entity recall?\n",
    "2. **BERTScore Reliability**: Does Clinical BERTScore align with entity retention?\n",
    "3. **Failure Modes**: Which samples fail to compress or lose critical entities?"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:27:39.024996Z",
     "start_time": "2025-12-04T11:27:39.017404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analysis 1: Correlation between compression and entity recall\n",
    "\n",
    "if len(results_df) > 1:\n",
    "    correlation = results_df['Compression_Ratio'].corr(results_df['Entity_Recall'])\n",
    "    print(f\"Correlation (Compression Ratio vs Entity Recall): {correlation:.3f}\")\n",
    "\n",
    "    if correlation < -0.3:\n",
    "        print(\"  â†’ Strong negative correlation: Higher compression â†’ Lower entity retention\")\n",
    "    elif correlation > 0.3:\n",
    "        print(\"  â†’ Strong positive correlation: Counterintuitive result, investigate further\")\n",
    "    else:\n",
    "        print(\"  â†’ Weak correlation: Compression and entity retention are relatively independent\")\n",
    "\n",
    "# Analysis 2: Identify best and worst performing samples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Best Performing Sample (Highest Entity Recall):\")\n",
    "print(\"=\" * 80)\n",
    "best_idx = results_df['Entity_Recall'].idxmax()\n",
    "best = results_df.iloc[best_idx]\n",
    "print(f\"Entity Recall: {best['Entity_Recall']:.2%}\")\n",
    "print(f\"Compression: {best['Compression_Ratio']:.2%}\")\n",
    "print(f\"BERTScore F1: {best['BERTScore_F1']:.4f}\")\n",
    "print(f\"\\nStage 2 Summary: {best['Stage2_Summary_Compressed'][:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Worst Performing Sample (Lowest Entity Recall):\")\n",
    "print(\"=\" * 80)\n",
    "worst_idx = results_df['Entity_Recall'].idxmin()\n",
    "worst = results_df.iloc[worst_idx]\n",
    "print(f\"Entity Recall: {worst['Entity_Recall']:.2%}\")\n",
    "print(f\"Compression: {worst['Compression_Ratio']:.2%}\")\n",
    "print(f\"BERTScore F1: {worst['BERTScore_F1']:.4f}\")\n",
    "print(f\"\\nStage 2 Summary: {worst['Stage2_Summary_Compressed'][:300]}...\")\n",
    "\n",
    "print(\"\\nâœ“ Analysis complete\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation (Compression Ratio vs Entity Recall): -0.664\n",
      "  â†’ Strong negative correlation: Higher compression â†’ Lower entity retention\n",
      "\n",
      "================================================================================\n",
      "Best Performing Sample (Highest Entity Recall):\n",
      "================================================================================\n",
      "Entity Recall: 12.12%\n",
      "Compression: 92.08%\n",
      "BERTScore F1: 0.5629\n",
      "\n",
      "Stage 2 Summary: A 65-year-old male patient was admitted to the hospital on 2022-02-15 with a 2-day history of worsening chest pain and shortness of breath. He was treated with aspirin 325mg and metoprolol 25mg daily. On admission, his vital signs were: blood pressure 160/90mmHg, heart rate 110bpm, respiratory rate ...\n",
      "\n",
      "================================================================================\n",
      "Worst Performing Sample (Lowest Entity Recall):\n",
      "================================================================================\n",
      "Entity Recall: 0.00%\n",
      "Compression: 70.16%\n",
      "BERTScore F1: 0.5638\n",
      "\n",
      "Stage 2 Summary: A 65-year-old male presented to the emergency department on 2022-02-15 with a 2-day history of worsening chest pain. He reported a 2-day history of fever, cough, and shortness of breath. His medications included aspirin 81mg daily and metoprolol 25mg twice daily. His vital signs were: blood pressure...\n",
      "\n",
      "âœ“ Analysis complete\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Final Notes and Next Steps\n",
    "\n",
    "### Memory Management Recap\n",
    "This notebook demonstrated:\n",
    "- âœ… Loading 4B and 8B models sequentially on a single GPU\n",
    "- âœ… Complete model unloading with `flush_memory()`\n",
    "- âœ… Successful execution without OOM errors\n",
    "\n",
    "### Results Interpretation\n",
    "- **Compression Ratio**: Target was ~50%, actual performance depends on prompt adherence\n",
    "- **Entity Recall**: Should be >85% for production use\n",
    "- **BERTScore**: Higher F1 indicates better semantic preservation\n",
    "\n",
    "### Potential Improvements\n",
    "1. **Prompt Engineering**: Refine compression prompt for better entity retention\n",
    "2. **Multi-stage Compression**: Iterative compression with entity verification\n",
    "3. **Entity-Aware Loss**: Fine-tune Llama on compression with entity-weighted loss\n",
    "4. **Hybrid Approach**: Extractive + abstractive compression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
