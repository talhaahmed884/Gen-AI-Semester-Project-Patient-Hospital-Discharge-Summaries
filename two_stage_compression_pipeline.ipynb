{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Two-Stage Clinical Summarization Pipeline: MedGemma â†’ Llama Compression\n",
    "\n",
    "**Course Project - Stage 2: Summary Compression with Entity Retention**\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "```\n",
    "Clinical Notes â†’ [Stage 1: MedGemma-4B] â†’ Verbose Summary â†’ [Stage 2: Llama-3-8B] â†’ Compressed Summary\n",
    "```\n",
    "\n",
    "**Critical Design Decisions**:\n",
    "- **Memory Safety**: Each model is fully unloaded before loading the next (single GPU constraint)\n",
    "- **4-bit Quantization**: Both models use bitsandbytes NF4 for VRAM efficiency\n",
    "- **Entity Preservation**: Chain-of-Density inspired compression with medical entity retention\n",
    "- **Clinical Evaluation**: NER-based entity recall + Clinical BERTScore\n",
    "\n",
    "**Execution Environment**: \n",
    "- Local: Runs with local file paths\n",
    "- Google Colab: Integrates with Google Drive for data I/O"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 0: Environment Detection and Setup\n",
    "\n",
    "Automatically detect execution environment (Colab vs Local) and configure accordingly."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:37:01.549936Z",
     "start_time": "2025-12-04T10:37:01.542553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# Detect execution environment\n",
    "import sys\n",
    "\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Execution Environment: {'Google Colab' if IS_COLAB else 'Local'}\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"\\nâš™ï¸  Colab-specific setup will be activated\")\n",
    "    print(\"   - Google Drive mounting\")\n",
    "    print(\"   - Drive-based data I/O\")\n",
    "    print(\"   - GPU verification\")\n",
    "else:\n",
    "    print(\"\\nâš™ï¸  Local execution mode\")\n",
    "    print(\"   - Using local file paths\")\n",
    "    print(\"   - Saving outputs to project directory\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Environment: Local\n",
      "\n",
      "âš™ï¸  Local execution mode\n",
      "   - Using local file paths\n",
      "   - Saving outputs to project directory\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 1: Installation and Imports\n",
    "\n",
    "**Memory-Critical Libraries**:\n",
    "- `bitsandbytes`: 4-bit quantization\n",
    "- `accelerate`: Device mapping and offloading\n",
    "- `scispacy`: Medical NER for entity extraction"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:37:07.856055Z",
     "start_time": "2025-12-04T10:37:03.641498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install required packages (Colab only - local assumes dependencies are installed)\n",
    "if IS_COLAB:\n",
    "    print(\"Installing dependencies for Colab...\\n\")\n",
    "    !pip install -q transformers accelerate bitsandbytes torch\n",
    "    !pip install -q scispacy\n",
    "    !pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
    "    !pip install -q bert-score\n",
    "    !pip install -q pandas numpy\n",
    "    print(\"âœ“ All dependencies installed\")\n",
    "else:\n",
    "    !pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
    "    print(\"Local mode: Assuming all dependencies are already installed via uv/pip\")\n",
    "    print(\"Required: transformers, accelerate, bitsandbytes, torch, scispacy, bert-score\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local mode: Assuming all dependencies are already installed via uv/pip\n",
      "Required: transformers, accelerate, bitsandbytes, torch, scispacy, bert-score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:37:24.391041Z",
     "start_time": "2025-12-04T10:37:18.429541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Core imports\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from bert_score import BERTScorer\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  WARNING: No GPU detected. This pipeline requires CUDA.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "VRAM: 8.55 GB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 2: Google Drive Setup (Colab Only)\n",
    "\n",
    "Mount Google Drive and configure I/O paths for Colab execution."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:40:15.670740Z",
     "start_time": "2025-12-04T10:40:15.666217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Configure paths - UPDATE THESE TO MATCH YOUR DRIVE STRUCTURE\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/Clinical_Summarization_Project\"\n",
    "\n",
    "    # Input: Path to your MIMIC cleaned dataset\n",
    "    INPUT_DATA_PATH = f\"{DRIVE_BASE}/mimic_cleaned_text_only.csv\"\n",
    "\n",
    "    # Input: Path to your fine-tuned MedGemma adapters (if using PEFT)\n",
    "    MEDGEMMA_ADAPTER_PATH = f\"{DRIVE_BASE}/medgemma-discharge-summarization/final\"\n",
    "\n",
    "    # Output: Where to save results\n",
    "    OUTPUT_DIR = f\"{DRIVE_BASE}/compression_results\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"âœ“ Google Drive mounted\")\n",
    "    print(f\"\\nConfigured paths:\")\n",
    "    print(f\"  Input data: {INPUT_DATA_PATH}\")\n",
    "    print(f\"  MedGemma adapters: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    # Verify input file exists\n",
    "    if os.path.exists(INPUT_DATA_PATH):\n",
    "        print(f\"\\nâœ“ Input data file found\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  WARNING: Input file not found at {INPUT_DATA_PATH}\")\n",
    "        print(f\"   Please update INPUT_DATA_PATH variable above\")\n",
    "\n",
    "else:\n",
    "    # Local paths\n",
    "    INPUT_DATA_PATH = \"mimic_cleaned_text_only.csv\"\n",
    "    MEDGEMMA_ADAPTER_PATH = \"./medgemma-discharge-summarization/final\"\n",
    "    OUTPUT_DIR = \"./compression_results\"\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"Local paths configured:\")\n",
    "    print(f\"  Input data: {INPUT_DATA_PATH}\")\n",
    "    print(f\"  MedGemma adapters: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local paths configured:\n",
      "  Input data: mimic_cleaned_full.csv\n",
      "  MedGemma adapters: ./medgemma-discharge-summarization/final\n",
      "  Output directory: ./compression_results\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 3: Configuration and Hyperparameters\n",
    "\n",
    "**Critical Memory Settings**:\n",
    "- Both models use NF4 4-bit quantization\n",
    "- `device_map=\"auto\"` for optimal GPU distribution\n",
    "- Explicit memory flushing between stages"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:40:18.705838Z",
     "start_time": "2025-12-04T10:40:18.698962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# MODEL IDENTIFIERS\n",
    "# ============================================================================\n",
    "\n",
    "# Stage 1: High-recall summary generation\n",
    "MEDGEMMA_BASE_MODEL = \"google/medgemma-4b-it\"\n",
    "USE_MEDGEMMA_ADAPTER = True  # Set to False if using base model without LoRA\n",
    "\n",
    "# Stage 2: Compression model\n",
    "LLAMA_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# ============================================================================\n",
    "# QUANTIZATION CONFIG (Shared across both models)\n",
    "# ============================================================================\n",
    "\n",
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Stage 1: Verbose summary generation\n",
    "STAGE1_MAX_TOKENS = 512\n",
    "STAGE1_TEMPERATURE = 0.7\n",
    "STAGE1_TOP_P = 0.9\n",
    "\n",
    "# Stage 2: Compression generation\n",
    "STAGE2_MAX_TOKENS = 256  # Deliberately shorter for compression\n",
    "STAGE2_TEMPERATURE = 0.3  # Lower temperature for more deterministic compression\n",
    "STAGE2_TOP_P = 0.9\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "# Number of samples to process (set lower for quick testing)\n",
    "NUM_SAMPLES = 10  # Change to -1 to process entire dataset\n",
    "\n",
    "# NER model for entity extraction\n",
    "SPACY_MODEL = \"en_core_sci_sm\"  # Medical NER model\n",
    "\n",
    "# Clinical BERTScore model\n",
    "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"\\nStage 1 Model: {MEDGEMMA_BASE_MODEL}\")\n",
    "print(f\"  - Using LoRA adapters: {USE_MEDGEMMA_ADAPTER}\")\n",
    "print(f\"  - Max tokens: {STAGE1_MAX_TOKENS}\")\n",
    "print(f\"\\nStage 2 Model: {LLAMA_MODEL}\")\n",
    "print(f\"  - Max tokens: {STAGE2_MAX_TOKENS}\")\n",
    "print(f\"  - Temperature: {STAGE2_TEMPERATURE} (deterministic compression)\")\n",
    "print(f\"\\nProcessing {NUM_SAMPLES if NUM_SAMPLES > 0 else 'ALL'} samples\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "\n",
      "Stage 1 Model: google/medgemma-4b-it\n",
      "  - Using LoRA adapters: True\n",
      "  - Max tokens: 512\n",
      "\n",
      "Stage 2 Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  - Max tokens: 256\n",
      "  - Temperature: 0.3 (deterministic compression)\n",
      "\n",
      "Processing 10 samples\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 4: Memory Management Utilities\n",
    "\n",
    "**Critical for Single-GPU Execution**: These functions ensure complete model unloading between stages."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:40:25.203298Z",
     "start_time": "2025-12-04T10:40:25.108250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flush_memory():\n",
    "    \"\"\"\n",
    "    Aggressively flush GPU and CPU memory.\n",
    "    \n",
    "    This is CRITICAL when switching between models on a single GPU.\n",
    "    Without this, you will encounter OOM errors.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "    # Report memory status\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        print(f\"GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "\n",
    "\n",
    "def unload_model(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Completely unload a model and tokenizer from memory.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to unload (or None)\n",
    "        tokenizer: The tokenizer to unload (or None)\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ§¹ Unloading model from memory...\")\n",
    "\n",
    "    if model is not None:\n",
    "        # Move model to CPU first (if on GPU)\n",
    "        if hasattr(model, 'cpu'):\n",
    "            model.cpu()\n",
    "        del model\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "\n",
    "    flush_memory()\n",
    "    print(\"âœ“ Model unloaded\\n\")\n",
    "\n",
    "\n",
    "# Test memory utilities\n",
    "print(\"Memory management utilities loaded\\n\")\n",
    "flush_memory()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory management utilities loaded\n",
      "\n",
      "GPU Memory: 0.00 GB allocated, 0.00 GB reserved\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 5: Load Input Data\n",
    "\n",
    "Load clinical notes dataset (preprocessed from Stage 0)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:41:02.691935Z",
     "start_time": "2025-12-04T10:40:28.071313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Loading data from: {INPUT_DATA_PATH}\\n\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(INPUT_DATA_PATH)\n",
    "\n",
    "print(f\"âœ“ Dataset loaded: {len(df)} total samples\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Select subset if configured\n",
    "if NUM_SAMPLES > 0 and NUM_SAMPLES < len(df):\n",
    "    df = df.head(NUM_SAMPLES)\n",
    "    print(f\"\\nðŸ“Š Processing {len(df)} samples (subset for testing)\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Processing all {len(df)} samples\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"SAMPLE INPUT (First 300 chars):\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(df.iloc[0]['final_input'][:300] + \"...\")\n",
    "print(f\"\\n{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: mimic_cleaned_full.csv\n",
      "\n",
      "âœ“ Dataset loaded: 269516 total samples\n",
      "  Columns: ['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq', 'charttime', 'storetime', 'input', 'target', 'input_length', 'target_length', 'tokenized_input', 'tokenized_target', 'lemmatized_input', 'lemmatized_target', 'filtered_input', 'final_input', 'final_target']\n",
      "\n",
      "ðŸ“Š Processing 10 samples (subset for testing)\n",
      "\n",
      "================================================================================\n",
      "SAMPLE INPUT (First 300 chars):\n",
      "================================================================================\n",
      "summarize chief complaint worsening abd distension pain history present illness hcv cirrhosis cb ascites hiv art ho ivdu copd bioplar ptsd presented osh ed worsening abd distension past week pt report selfdiscontinuing lasix spirnolactone week ago feel like dont anything doesnt want put chemical fol...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 6: STAGE 1 - MedGemma Verbose Summary Generation\n",
    "\n",
    "**Objective**: Generate high-recall, detailed clinical summaries.\n",
    "\n",
    "**Memory Strategy**: \n",
    "1. Load MedGemma-4B in 4-bit\n",
    "2. Generate summaries for all samples\n",
    "3. **CRITICAL**: Completely unload before Stage 2"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:41:32.026699Z",
     "start_time": "2025-12-04T10:41:32.022950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Enable synchronous CUDA for better error messages\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"âœ“ CUDA synchronous mode enabled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CUDA synchronous mode enabled\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:41:50.018513Z",
     "start_time": "2025-12-04T10:41:33.756641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1: LOADING MEDGEMMA-4B FOR VERBOSE SUMMARY GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Load base model FIRST (before tokenizer)\n",
    "print(f\"\\nLoading base model: {MEDGEMMA_BASE_MODEL}\")\n",
    "print(\"  Quantization: 4-bit NF4\")\n",
    "print(\"  This may take 2-3 minutes...\")\n",
    "\n",
    "medgemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MEDGEMMA_BASE_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "print(\"âœ“ Base model loaded\")\n",
    "\n",
    "# Step 2: Load LoRA adapters NEXT (before tokenizer)\n",
    "if USE_MEDGEMMA_ADAPTER and os.path.exists(MEDGEMMA_ADAPTER_PATH):\n",
    "    print(f\"\\nLoading LoRA adapters from: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    from peft import PeftModel\n",
    "\n",
    "    medgemma_model = PeftModel.from_pretrained(medgemma_model, MEDGEMMA_ADAPTER_PATH)\n",
    "    print(\"âœ“ LoRA adapters loaded\")\n",
    "\n",
    "    # CRITICAL: After loading LoRA, the model config may have updated vocab size\n",
    "    # We need to use THIS vocab size for the tokenizer\n",
    "\n",
    "elif USE_MEDGEMMA_ADAPTER:\n",
    "    print(f\"\\nâš ï¸  WARNING: Adapter path not found: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(\"   Continuing with base model only\")\n",
    "\n",
    "# Step 3: Get the ACTUAL vocab size from the loaded model\n",
    "embedding_layer = medgemma_model.get_input_embeddings()\n",
    "actual_vocab_size = embedding_layer.weight.shape[0]\n",
    "print(f\"\\n  Model embedding vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Step 4: Load tokenizer AFTER model is fully loaded\n",
    "# Try adapter path first, then base model\n",
    "if USE_MEDGEMMA_ADAPTER and os.path.exists(MEDGEMMA_ADAPTER_PATH):\n",
    "    print(f\"\\nAttempting to load tokenizer from adapter path: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    try:\n",
    "        medgemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MEDGEMMA_ADAPTER_PATH,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "        print(\"âœ“ Tokenizer loaded from adapter path\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Adapter tokenizer failed: {e}\")\n",
    "        print(\"   Loading base model tokenizer instead\")\n",
    "        medgemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MEDGEMMA_BASE_MODEL,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "else:\n",
    "    print(f\"\\nLoading tokenizer from base model: {MEDGEMMA_BASE_MODEL}\")\n",
    "    medgemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MEDGEMMA_BASE_MODEL,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\",\n",
    "        add_eos_token=True\n",
    "    )\n",
    "\n",
    "medgemma_tokenizer.pad_token = medgemma_tokenizer.eos_token\n",
    "\n",
    "print(f\"\\n  Tokenizer vocab size: {len(medgemma_tokenizer)}\")\n",
    "print(f\"  PAD token ID: {medgemma_tokenizer.pad_token_id}\")\n",
    "print(f\"  EOS token ID: {medgemma_tokenizer.eos_token_id}\")\n",
    "\n",
    "# Step 5: CRITICAL VALIDATION - Check if sizes match\n",
    "if len(medgemma_tokenizer) != actual_vocab_size:\n",
    "    print(f\"\\nâš ï¸  MISMATCH DETECTED!\")\n",
    "    print(f\"   Tokenizer vocab: {len(medgemma_tokenizer)}\")\n",
    "    print(f\"   Model vocab: {actual_vocab_size}\")\n",
    "\n",
    "    if len(medgemma_tokenizer) > actual_vocab_size:\n",
    "        print(f\"\\n   ERROR: Tokenizer is LARGER than model!\")\n",
    "        print(f\"   This WILL cause CUDA errors!\")\n",
    "        print(f\"\\n   SOLUTION: Resizing model embeddings to {len(medgemma_tokenizer)}...\")\n",
    "        medgemma_model.resize_token_embeddings(len(medgemma_tokenizer))\n",
    "        print(f\"   âœ“ Model embeddings resized\")\n",
    "\n",
    "        # Update actual vocab size\n",
    "        actual_vocab_size = medgemma_model.get_input_embeddings().weight.shape[0]\n",
    "        print(f\"   âœ“ New model vocab size: {actual_vocab_size}\")\n",
    "    else:\n",
    "        print(f\"\\n   WARNING: Model is larger than tokenizer\")\n",
    "        print(f\"   This is unusual but may work if all token IDs < {actual_vocab_size}\")\n",
    "\n",
    "# Step 6: VALIDATION TEST\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"CRITICAL VALIDATION TEST\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "test_text = \"Patient presented with chest pain.\"\n",
    "test_tokens = medgemma_tokenizer(test_text, return_tensors=\"pt\")\n",
    "max_id = test_tokens['input_ids'].max().item()\n",
    "min_id = test_tokens['input_ids'].min().item()\n",
    "\n",
    "print(f\"Test text: '{test_text}'\")\n",
    "print(f\"Token IDs: {test_tokens['input_ids'][0].tolist()}\")\n",
    "print(f\"Max token ID: {max_id}\")\n",
    "print(f\"Min token ID: {min_id}\")\n",
    "print(f\"Valid range: [0, {actual_vocab_size - 1}]\")\n",
    "\n",
    "if max_id >= actual_vocab_size:\n",
    "    print(f\"\\nâŒ CRITICAL ERROR: Token ID {max_id} >= vocab size {actual_vocab_size}\")\n",
    "    print(f\"   This WILL cause the CUDA error you're seeing!\")\n",
    "    print(f\"\\n   IMMEDIATE FIX REQUIRED:\")\n",
    "    print(f\"   medgemma_model.resize_token_embeddings({len(medgemma_tokenizer)})\")\n",
    "    raise ValueError(f\"Token ID out of range: {max_id} >= {actual_vocab_size}\")\n",
    "elif min_id < 0:\n",
    "    print(f\"\\nâŒ CRITICAL ERROR: Negative token ID {min_id}\")\n",
    "    raise ValueError(f\"Invalid token ID: {min_id}\")\n",
    "else:\n",
    "    print(f\"\\nâœ… VALIDATION PASSED!\")\n",
    "    print(f\"   All token IDs are within valid range\")\n",
    "    print(f\"   Safe to proceed with generation\")\n",
    "\n",
    "medgemma_model.eval()\n",
    "print(f\"\\nâœ“ MedGemma ready for inference\")\n",
    "flush_memory()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 1: LOADING MEDGEMMA-4B FOR VERBOSE SUMMARY GENERATION\n",
      "================================================================================\n",
      "\n",
      "Loading base model: google/medgemma-4b-it\n",
      "  Quantization: 4-bit NF4\n",
      "  This may take 2-3 minutes...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51d96e9da01a48ce89af688bc7706278"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Base model loaded\n",
      "\n",
      "Loading LoRA adapters from: ./medgemma-discharge-summarization/final\n",
      "âœ“ LoRA adapters loaded\n",
      "\n",
      "  Model embedding vocab size: 262208\n",
      "\n",
      "Attempting to load tokenizer from adapter path: ./medgemma-discharge-summarization/final\n",
      "âœ“ Tokenizer loaded from adapter path\n",
      "\n",
      "  Tokenizer vocab size: 262145\n",
      "  PAD token ID: 1\n",
      "  EOS token ID: 1\n",
      "\n",
      "âš ï¸  MISMATCH DETECTED!\n",
      "   Tokenizer vocab: 262145\n",
      "   Model vocab: 262208\n",
      "\n",
      "   WARNING: Model is larger than tokenizer\n",
      "   This is unusual but may work if all token IDs < 262208\n",
      "\n",
      "================================================================================\n",
      "CRITICAL VALIDATION TEST\n",
      "================================================================================\n",
      "Test text: 'Patient presented with chest pain.'\n",
      "Token IDs: [2, 52420, 6212, 607, 15350, 4331, 236761, 1]\n",
      "Max token ID: 236761\n",
      "Min token ID: 1\n",
      "Valid range: [0, 262207]\n",
      "\n",
      "âœ… VALIDATION PASSED!\n",
      "   All token IDs are within valid range\n",
      "   Safe to proceed with generation\n",
      "\n",
      "âœ“ MedGemma ready for inference\n",
      "GPU Memory: 3.50 GB allocated, 3.51 GB reserved\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:41:59.825487Z",
     "start_time": "2025-12-04T10:41:59.733635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# FINAL VALIDATION BEFORE GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL PRE-GENERATION CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get actual embedding size\n",
    "embedding_size = medgemma_model.get_input_embeddings().weight.shape[0]\n",
    "\n",
    "print(f\"\\nTokenizer vocab: {len(medgemma_tokenizer)}\")\n",
    "print(f\"Model embedding size: {embedding_size}\")\n",
    "print(f\"Match: {'âœ… YES' if len(medgemma_tokenizer) == embedding_size else 'âŒ NO'}\")\n",
    "\n",
    "# Test with actual prompt format from your generation function\n",
    "test_prompt = \"\"\"<start_of_turn>user\n",
    "Summarize the following clinical discharge notes.\n",
    "\n",
    "Clinical Notes:\n",
    "Patient with hypertension.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nTesting with actual prompt format...\")\n",
    "test_inputs = medgemma_tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "max_token = test_inputs['input_ids'].max().item()\n",
    "\n",
    "print(f\"  Prompt token count: {test_inputs['input_ids'].shape[1]}\")\n",
    "print(f\"  Max token ID in prompt: {max_token}\")\n",
    "print(f\"  Valid range: [0, {embedding_size - 1}]\")\n",
    "\n",
    "if max_token >= embedding_size:\n",
    "    print(f\"\\nâŒ STOP! Token ID {max_token} is out of range!\")\n",
    "    print(f\"   DO NOT PROCEED - will cause CUDA error\")\n",
    "    print(f\"\\n   Run this fix:\")\n",
    "    print(f\"   medgemma_model.resize_token_embeddings({len(medgemma_tokenizer)})\")\n",
    "else:\n",
    "    print(f\"\\nâœ… All checks passed - safe to generate summaries\")\n",
    "\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL PRE-GENERATION CHECK\n",
      "================================================================================\n",
      "\n",
      "Tokenizer vocab: 262145\n",
      "Model embedding size: 262208\n",
      "Match: âŒ NO\n",
      "\n",
      "Testing with actual prompt format...\n",
      "  Prompt token count: 27\n",
      "  Max token ID in prompt: 236787\n",
      "  Valid range: [0, 262207]\n",
      "\n",
      "âœ… All checks passed - safe to generate summaries\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:42:04.021115Z",
     "start_time": "2025-12-04T10:42:04.012743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_stage1_summary(clinical_note: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate verbose clinical summary using MedGemma.\n",
    "    \n",
    "    Args:\n",
    "        clinical_note: Input clinical text\n",
    "    \n",
    "    Returns:\n",
    "        Generated summary (high-recall, verbose)\n",
    "    \"\"\"\n",
    "    instruction = (\n",
    "        \"Summarize the following clinical discharge notes. \"\n",
    "        \"Include ALL diagnoses, medications, vitals, lab results, \"\n",
    "        \"procedures, and follow-up instructions. \"\n",
    "        \"Ensure complete coverage of all medical entities.\"\n",
    "    )\n",
    "\n",
    "    # Format using Gemma template\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{clinical_note}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    inputs = medgemma_tokenizer(prompt, return_tensors=\"pt\").to(medgemma_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = medgemma_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=STAGE1_MAX_TOKENS,\n",
    "            temperature=STAGE1_TEMPERATURE,\n",
    "            top_p=STAGE1_TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=medgemma_tokenizer.pad_token_id,\n",
    "            eos_token_id=medgemma_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = medgemma_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the model's response\n",
    "    marker = \"<start_of_turn>model\"\n",
    "    if marker in generated_text:\n",
    "        summary = generated_text.split(marker)[-1].strip()\n",
    "    else:\n",
    "        summary = generated_text[len(prompt):].strip()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "print(\"âœ“ Stage 1 generation function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Stage 1 generation function defined\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:46:26.563961Z",
     "start_time": "2025-12-04T10:42:11.599477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Stage 1 summaries for all samples\n",
    "print(f\"\\nGenerating Stage 1 summaries for {len(df)} samples...\\n\")\n",
    "\n",
    "stage1_summaries = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    clinical_note = row['final_input']\n",
    "\n",
    "    print(f\"[{idx + 1}/{len(df)}] Generating verbose summary...\", end=\" \")\n",
    "    summary = generate_stage1_summary(clinical_note)\n",
    "    stage1_summaries.append(summary)\n",
    "    print(f\"âœ“ ({len(summary)} chars)\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['stage1_summary'] = stage1_summaries\n",
    "\n",
    "print(f\"\\nâœ“ Stage 1 complete: {len(stage1_summaries)} summaries generated\")\n",
    "print(f\"  Average length: {np.mean([len(s) for s in stage1_summaries]):.0f} characters\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STAGE 1 OUTPUT SAMPLE:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(stage1_summaries[0][:400] + \"...\")\n",
    "print(f\"{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Stage 1 summaries for 10 samples...\n",
      "\n",
      "[1/10] Generating verbose summary... âœ“ (896 chars)\n",
      "[2/10] Generating verbose summary... âœ“ (2766 chars)\n",
      "[3/10] Generating verbose summary... âœ“ (383 chars)\n",
      "[4/10] Generating verbose summary... âœ“ (1163 chars)\n",
      "[5/10] Generating verbose summary... âœ“ (641 chars)\n",
      "[6/10] Generating verbose summary... âœ“ (788 chars)\n",
      "[7/10] Generating verbose summary... âœ“ (104 chars)\n",
      "[8/10] Generating verbose summary... âœ“ (67 chars)\n",
      "[9/10] Generating verbose summary... âœ“ (66 chars)\n",
      "[10/10] Generating verbose summary... âœ“ (1746 chars)\n",
      "\n",
      "âœ“ Stage 1 complete: 10 summaries generated\n",
      "  Average length: 862 characters\n",
      "\n",
      "================================================================================\n",
      "STAGE 1 OUTPUT SAMPLE:\n",
      "================================================================================\n",
      "ascites hiv art ho ivdu copd bioplar ptsd present worsening abd distension ascites portal hypertension ascites portal htn ascites portal hypertension patient presented worsening abdominal distension ascites setting spironolactone furosemide held one week prior presentation paracentesis ed removed l fluid spironolactone mg daily lasix mg daily home patient given mg iv lasix mg iv spironolactone ed ...\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CRITICAL: Unload MedGemma Before Stage 2\n",
    "\n",
    "**This step is NON-NEGOTIABLE for single-GPU execution.**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:46:28.063867Z",
     "start_time": "2025-12-04T10:46:26.820840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNLOADING MEDGEMMA-4B FROM MEMORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "unload_model(medgemma_model, medgemma_tokenizer)\n",
    "\n",
    "# Verify memory is cleared\n",
    "print(\"Memory status after unloading:\")\n",
    "flush_memory()\n",
    "\n",
    "print(\"\\nâœ“ Safe to proceed to Stage 2\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UNLOADING MEDGEMMA-4B FROM MEMORY\n",
      "================================================================================\n",
      "\n",
      "ðŸ§¹ Unloading model from memory...\n",
      "GPU Memory: 0.01 GB allocated, 0.02 GB reserved\n",
      "âœ“ Model unloaded\n",
      "\n",
      "Memory status after unloading:\n",
      "GPU Memory: 0.01 GB allocated, 0.02 GB reserved\n",
      "\n",
      "âœ“ Safe to proceed to Stage 2\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 7: STAGE 2 - Llama-3 Compression\n",
    "\n",
    "**Objective**: Compress verbose summaries while preserving medical entities.\n",
    "\n",
    "**Chain-of-Density Inspired Prompt**: Forces 50% compression with entity retention constraint."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:49:09.242593Z",
     "start_time": "2025-12-04T10:49:09.230611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5c8f6cf19de4b88b86515b540dc2db9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:50:19.137550Z",
     "start_time": "2025-12-04T10:50:02.258002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 2: LOADING LLAMA-3-8B FOR COMPRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if we need HuggingFace authentication for Llama\n",
    "if IS_COLAB:\n",
    "    print(\"\\nâš ï¸  Llama-3 requires HuggingFace authentication\")\n",
    "    print(\"   Running authentication flow...\\n\")\n",
    "    from huggingface_hub import notebook_login\n",
    "\n",
    "    notebook_login()\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {LLAMA_MODEL}\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LLAMA_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"  # Llama uses left padding\n",
    ")\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "print(\"âœ“ Tokenizer loaded\")\n",
    "\n",
    "# Load model with quantization\n",
    "print(f\"\\nLoading model: {LLAMA_MODEL}\")\n",
    "print(\"  Quantization: 4-bit NF4\")\n",
    "print(\"  This may take 2-3 minutes...\")\n",
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "llama_model.eval()\n",
    "print(f\"\\nâœ“ Llama-3 ready for compression\")\n",
    "flush_memory()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 2: LOADING LLAMA-3-8B FOR COMPRESSION\n",
      "================================================================================\n",
      "\n",
      "Loading tokenizer: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "âœ“ Tokenizer loaded\n",
      "\n",
      "Loading model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  Quantization: 4-bit NF4\n",
      "  This may take 2-3 minutes...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92210f65fbc94dda87994138b1285fae"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Llama-3 ready for compression\n",
      "GPU Memory: 5.71 GB allocated, 5.85 GB reserved\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:50:19.150256Z",
     "start_time": "2025-12-04T10:50:19.145820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_compressed_summary(verbose_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Compress summary using Llama-3 with entity retention constraint.\n",
    "    \n",
    "    Chain-of-Density inspired prompt: Forces 50% compression while\n",
    "    preserving all medical entities (medications, numbers, dates).\n",
    "    \n",
    "    Args:\n",
    "        verbose_summary: Stage 1 summary (high-recall)\n",
    "    \n",
    "    Returns:\n",
    "        Compressed summary\n",
    "    \"\"\"\n",
    "    # Chain-of-Density inspired compression prompt\n",
    "    system_prompt = (\n",
    "        \"You are a medical summarization expert. Your task is to compress \"\n",
    "        \"clinical summaries while preserving critical information.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Rewrite the following summary to be 50% shorter. \"\n",
    "        \"You MUST retain ALL entities: medications (with dosages), \"\n",
    "        \"vital signs (with numbers), lab results (with values), \"\n",
    "        \"diagnoses, procedures, and dates. \"\n",
    "        \"If you cannot shorten it without losing a critical fact, do not shorten it. \"\n",
    "        \"Remove only redundant phrasing and verbose descriptions.\\n\\n\"\n",
    "        f\"Summary to compress:\\n{verbose_summary}\"\n",
    "    )\n",
    "\n",
    "    # Format using Llama-3 chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    prompt = llama_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(llama_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=STAGE2_MAX_TOKENS,\n",
    "            temperature=STAGE2_TEMPERATURE,\n",
    "            top_p=STAGE2_TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=llama_tokenizer.pad_token_id,\n",
    "            eos_token_id=llama_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the assistant's response\n",
    "    # Llama-3 format: <|start_header_id|>assistant<|end_header_id|>\\n\\n{response}\n",
    "    if \"assistant\" in generated_text:\n",
    "        compressed = generated_text.split(\"assistant\")[-1].strip()\n",
    "        # Remove any remaining header artifacts\n",
    "        compressed = compressed.split(\"\\n\\n\", 1)[-1] if \"\\n\\n\" in compressed else compressed\n",
    "    else:\n",
    "        compressed = generated_text[len(prompt):].strip()\n",
    "\n",
    "    return compressed\n",
    "\n",
    "\n",
    "print(\"âœ“ Stage 2 compression function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Stage 2 compression function defined\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:53:47.700809Z",
     "start_time": "2025-12-04T10:50:52.374079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Stage 2 compressed summaries\n",
    "print(f\"\\nCompressing {len(df)} Stage 1 summaries...\\n\")\n",
    "\n",
    "stage2_summaries = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    verbose_summary = row['stage1_summary']\n",
    "\n",
    "    print(f\"[{idx + 1}/{len(df)}] Compressing summary...\", end=\" \")\n",
    "    compressed = generate_compressed_summary(verbose_summary)\n",
    "    stage2_summaries.append(compressed)\n",
    "\n",
    "    # Calculate compression ratio\n",
    "    ratio = len(compressed) / len(verbose_summary)\n",
    "    print(f\"âœ“ ({len(compressed)} chars, {ratio:.2%} of original)\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['stage2_summary'] = stage2_summaries\n",
    "\n",
    "print(f\"\\nâœ“ Stage 2 complete: {len(stage2_summaries)} summaries compressed\")\n",
    "print(f\"  Average length: {np.mean([len(s) for s in stage2_summaries]):.0f} characters\")\n",
    "print(\n",
    "    f\"  Overall compression: {np.mean([len(s2) / len(s1) for s1, s2 in zip(stage1_summaries, stage2_summaries)]):.2%}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STAGE 2 OUTPUT SAMPLE:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(stage2_summaries[0])\n",
    "print(f\"{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compressing 10 Stage 1 summaries...\n",
      "\n",
      "[1/10] Compressing summary... âœ“ (926 chars, 103.35% of original)\n",
      "[2/10] Compressing summary... âœ“ (984 chars, 35.57% of original)\n",
      "[3/10] Compressing summary... âœ“ (517 chars, 134.99% of original)\n",
      "[4/10] Compressing summary... âœ“ (862 chars, 74.12% of original)\n",
      "[5/10] Compressing summary... âœ“ (653 chars, 101.87% of original)\n",
      "[6/10] Compressing summary... âœ“ (943 chars, 119.67% of original)\n",
      "[7/10] Compressing summary... âœ“ (395 chars, 379.81% of original)\n",
      "[8/10] Compressing summary... âœ“ (600 chars, 895.52% of original)\n",
      "[9/10] Compressing summary... âœ“ (329 chars, 498.48% of original)\n",
      "[10/10] Compressing summary... âœ“ (1107 chars, 63.40% of original)\n",
      "\n",
      "âœ“ Stage 2 complete: 10 summaries compressed\n",
      "  Average length: 732 characters\n",
      "  Overall compression: 240.68%\n",
      "\n",
      "================================================================================\n",
      "STAGE 2 OUTPUT SAMPLE:\n",
      "================================================================================\n",
      "A 65-year-old patient with HIV, ART, COPD, bipolar disorder, and PTSD presented with worsening abdominal distension and ascites. The patient had been taking spironolactone 100mg daily and furosemide 40mg daily for one week prior to presentation. Paracentesis was performed, removing 1L of fluid. Lab results showed worsening renal function (baseline Cr: 1.5mg/dL). The patient received 100mg IV lasix and 100mg IV spironolactone. They were discharged home on lasix 40mg daily, spironolactone 100mg daily, and furosemide 40mg daily. A nutrition consult recommended a low-salt diet, and the patient was encouraged to continue this diet. Follow-up with the liver clinic was scheduled, and a colonoscopy/EGD was planned. The patient was advised to continue their HIV medications: raltegravir 400mg PO bid and emtricitabine/tenofovir 200mg PO daily. COPD treatment continued with albuterol neb and ipratropium neb as needed.\n",
      "\n",
      "Note:\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unload Llama-3 (Optional - only if running evaluation separately)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:54:39.039485Z",
     "start_time": "2025-12-04T10:54:37.263938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you need to free memory before evaluation, uncomment:\n",
    "unload_model(llama_model, llama_tokenizer)\n",
    "\n",
    "print(\"âœ“ Stage 2 generation complete. Proceeding to evaluation...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¹ Unloading model from memory...\n",
      "GPU Memory: 0.01 GB allocated, 0.02 GB reserved\n",
      "âœ“ Model unloaded\n",
      "\n",
      "âœ“ Stage 2 generation complete. Proceeding to evaluation...\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 8: EVALUATION - Metric 1: Compression Ratio"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:54:47.151184Z",
     "start_time": "2025-12-04T10:54:47.137770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 1: COMPRESSION RATIO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def calculate_compression_ratio(stage1: str, stage2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate compression ratio (Stage 2 length / Stage 1 length).\n",
    "    \n",
    "    Lower values indicate stronger compression.\n",
    "    \"\"\"\n",
    "    return len(stage2) / len(stage1)\n",
    "\n",
    "\n",
    "# Calculate for all samples\n",
    "df['compression_ratio'] = df.apply(\n",
    "    lambda row: calculate_compression_ratio(row['stage1_summary'], row['stage2_summary']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nCompression Statistics:\")\n",
    "print(f\"  Mean ratio: {df['compression_ratio'].mean():.2%}\")\n",
    "print(f\"  Median ratio: {df['compression_ratio'].median():.2%}\")\n",
    "print(f\"  Min ratio: {df['compression_ratio'].min():.2%}\")\n",
    "print(f\"  Max ratio: {df['compression_ratio'].max():.2%}\")\n",
    "print(f\"  Std dev: {df['compression_ratio'].std():.2%}\")\n",
    "\n",
    "print(f\"\\nâœ“ Compression ratios calculated\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRIC 1: COMPRESSION RATIO\n",
      "================================================================================\n",
      "\n",
      "Compression Statistics:\n",
      "  Mean ratio: 240.68%\n",
      "  Median ratio: 111.51%\n",
      "  Min ratio: 35.57%\n",
      "  Max ratio: 895.52%\n",
      "  Std dev: 274.84%\n",
      "\n",
      "âœ“ Compression ratios calculated\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 9: EVALUATION - Metric 2: Entity Retention (NER)\n",
    "\n",
    "**Method**: Extract medical entities from both Stage 1 and Stage 2 summaries using SciSpacy, then calculate recall."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:54:52.502269Z",
     "start_time": "2025-12-04T10:54:51.802697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 2: ENTITY RETENTION (NER)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load SciSpacy medical NER model\n",
    "print(f\"\\nLoading SciSpacy model: {SPACY_MODEL}\")\n",
    "nlp = spacy.load(SPACY_MODEL)\n",
    "print(\"âœ“ NER model loaded\")\n",
    "\n",
    "\n",
    "def extract_entities(text: str) -> set:\n",
    "    \"\"\"\n",
    "    Extract medical entities from text using SciSpacy.\n",
    "    \n",
    "    Returns set of entity texts (lowercased for comparison).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = {ent.text.lower() for ent in doc.ents}\n",
    "    return entities\n",
    "\n",
    "\n",
    "def calculate_entity_recall(stage1_text: str, stage2_text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate entity retention recall: \n",
    "    (Entities in Stage 2) / (Entities in Stage 1)\n",
    "    \n",
    "    Returns value between 0 and 1 (higher is better).\n",
    "    \"\"\"\n",
    "    entities_stage1 = extract_entities(stage1_text)\n",
    "    entities_stage2 = extract_entities(stage2_text)\n",
    "\n",
    "    if len(entities_stage1) == 0:\n",
    "        return 1.0  # No entities to preserve\n",
    "\n",
    "    # Calculate recall: how many Stage 1 entities appear in Stage 2?\n",
    "    retained = entities_stage1.intersection(entities_stage2)\n",
    "    recall = len(retained) / len(entities_stage1)\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "print(\"\\nCalculating entity recall for all samples...\\n\")\n",
    "\n",
    "entity_recalls = []\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"[{idx + 1}/{len(df)}] Extracting entities...\", end=\" \")\n",
    "    recall = calculate_entity_recall(row['stage1_summary'], row['stage2_summary'])\n",
    "    entity_recalls.append(recall)\n",
    "    print(f\"âœ“ Recall: {recall:.2%}\")\n",
    "\n",
    "df['entity_recall'] = entity_recalls\n",
    "\n",
    "print(f\"\\nEntity Retention Statistics:\")\n",
    "print(f\"  Mean recall: {df['entity_recall'].mean():.2%}\")\n",
    "print(f\"  Median recall: {df['entity_recall'].median():.2%}\")\n",
    "print(f\"  Min recall: {df['entity_recall'].min():.2%}\")\n",
    "print(f\"  Max recall: {df['entity_recall'].max():.2%}\")\n",
    "print(f\"  Std dev: {df['entity_recall'].std():.2%}\")\n",
    "\n",
    "print(f\"\\nâœ“ Entity recall calculated\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRIC 2: ENTITY RETENTION (NER)\n",
      "================================================================================\n",
      "\n",
      "Loading SciSpacy model: en_core_sci_sm\n",
      "âœ“ NER model loaded\n",
      "\n",
      "Calculating entity recall for all samples...\n",
      "\n",
      "[1/10] Extracting entities... âœ“ Recall: 48.48%\n",
      "[2/10] Extracting entities... âœ“ Recall: 38.46%\n",
      "[3/10] Extracting entities... âœ“ Recall: 25.00%\n",
      "[4/10] Extracting entities... âœ“ Recall: 23.53%\n",
      "[5/10] Extracting entities... âœ“ Recall: 61.11%\n",
      "[6/10] Extracting entities... âœ“ Recall: 46.67%\n",
      "[7/10] Extracting entities... âœ“ Recall: 33.33%\n",
      "[8/10] Extracting entities... âœ“ Recall: 66.67%\n",
      "[9/10] Extracting entities... âœ“ Recall: 60.00%\n",
      "[10/10] Extracting entities... âœ“ Recall: 38.64%\n",
      "\n",
      "Entity Retention Statistics:\n",
      "  Mean recall: 44.19%\n",
      "  Median recall: 42.65%\n",
      "  Min recall: 23.53%\n",
      "  Max recall: 66.67%\n",
      "  Std dev: 15.07%\n",
      "\n",
      "âœ“ Entity recall calculated\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 10: EVALUATION - Metric 3: Clinical BERTScore\n",
    "\n",
    "**Method**: Measure semantic similarity between original clinical notes and final compressed summaries using Bio_ClinicalBERT."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:55:11.155586Z",
     "start_time": "2025-12-04T10:55:07.638338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 3: CLINICAL BERTSCORE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize Clinical BERTScore\n",
    "print(f\"\\nInitializing BERTScorer with {CLINICAL_BERT}\")\n",
    "print(\"This will download the model if not cached...\\n\")\n",
    "\n",
    "bert_scorer = BERTScorer(\n",
    "    model_type=CLINICAL_BERT,\n",
    "    num_layers=9,\n",
    "    rescale_with_baseline=False,\n",
    "    lang=\"en\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ BERTScorer initialized\")\n",
    "\n",
    "# Get the tokenizer from the scorer to do proper truncation\n",
    "bert_tokenizer = bert_scorer._tokenizer\n",
    "\n",
    "\n",
    "def truncate_with_bert_tokenizer(text: str, tokenizer, max_length: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Properly truncate text using BERT's tokenizer to ensure it fits within token limit.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to truncate\n",
    "        tokenizer: BERT tokenizer\n",
    "        max_length: Maximum number of tokens (BERT supports 512, we use 500 for safety)\n",
    "\n",
    "    Returns:\n",
    "        Truncated text that will tokenize to <= max_length tokens\n",
    "    \"\"\"\n",
    "    # Tokenize and truncate\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    # Decode back to text\n",
    "    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return truncated_text\n",
    "\n",
    "\n",
    "# Truncate texts to fit BERT's 512 token limit\n",
    "print(\"\\nPreparing texts (truncating long sequences for BERT)...\")\n",
    "print(\"  Using BERT tokenizer for accurate truncation...\")\n",
    "\n",
    "# Prepare data for batch scoring\n",
    "source_texts = df['final_input'].tolist()\n",
    "compressed_summaries = df['stage2_summary'].tolist()\n",
    "\n",
    "truncated_predictions = []\n",
    "truncated_references = []\n",
    "\n",
    "for pred, ref in zip(compressed_summaries, source_texts):\n",
    "    truncated_predictions.append(truncate_with_bert_tokenizer(pred, bert_tokenizer))\n",
    "    truncated_references.append(truncate_with_bert_tokenizer(ref, bert_tokenizer))\n",
    "\n",
    "# Check truncation statistics\n",
    "orig_pred_lens = [len(bert_tokenizer.encode(p)) for p in compressed_summaries]\n",
    "trunc_pred_lens = [len(bert_tokenizer.encode(p)) for p in truncated_predictions]\n",
    "num_truncated = sum(1 for o, t in zip(orig_pred_lens, trunc_pred_lens) if o != t)\n",
    "\n",
    "print(f\"  {num_truncated}/{len(compressed_summaries)} predictions were truncated\")\n",
    "print(f\"  Average prediction tokens: {np.mean(trunc_pred_lens):.0f}\")\n",
    "print(f\"  Max prediction tokens: {max(trunc_pred_lens)}\")\n",
    "\n",
    "print(\"\\nCalculating BERTScores (this may take a few minutes)...\\n\")\n",
    "\n",
    "# Calculate BERTScore\n",
    "# We compare compressed summaries (candidates) against original notes (references)\n",
    "P, R, F1 = bert_scorer.score(\n",
    "    cands=truncated_predictions,\n",
    "    refs=truncated_references\n",
    ")\n",
    "\n",
    "# Convert to numpy\n",
    "df['bertscore_precision'] = P.cpu().numpy()\n",
    "df['bertscore_recall'] = R.cpu().numpy()\n",
    "df['bertscore_f1'] = F1.cpu().numpy()\n",
    "\n",
    "print(f\"\\nClinical BERTScore Statistics:\")\n",
    "print(f\"\\nPrecision:\")\n",
    "print(f\"  Mean: {df['bertscore_precision'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_precision'].median():.4f}\")\n",
    "print(f\"\\nRecall:\")\n",
    "print(f\"  Mean: {df['bertscore_recall'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_recall'].median():.4f}\")\n",
    "print(f\"\\nF1:\")\n",
    "print(f\"  Mean: {df['bertscore_f1'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_f1'].median():.4f}\")\n",
    "\n",
    "print(f\"\\nâœ“ Clinical BERTScore calculated\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRIC 3: CLINICAL BERTSCORE\n",
      "================================================================================\n",
      "\n",
      "Initializing BERTScorer with emilyalsentzer/Bio_ClinicalBERT\n",
      "This will download the model if not cached...\n",
      "\n",
      "âœ“ BERTScorer initialized\n",
      "\n",
      "Preparing texts (truncating long sequences for BERT)...\n",
      "  Using BERT tokenizer for accurate truncation...\n",
      "  0/10 predictions were truncated\n",
      "  Average prediction tokens: 182\n",
      "  Max prediction tokens: 269\n",
      "\n",
      "Calculating BERTScores (this may take a few minutes)...\n",
      "\n",
      "\n",
      "Clinical BERTScore Statistics:\n",
      "\n",
      "Precision:\n",
      "  Mean: 0.6552\n",
      "  Median: 0.6374\n",
      "\n",
      "Recall:\n",
      "  Mean: 0.6340\n",
      "  Median: 0.6414\n",
      "\n",
      "F1:\n",
      "  Mean: 0.6441\n",
      "  Median: 0.6475\n",
      "\n",
      "âœ“ Clinical BERTScore calculated\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 11: Results Summary and Export"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:57:31.817354Z",
     "start_time": "2025-12-04T10:57:31.808541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select columns for final output\n",
    "results_df = df[[\n",
    "    'final_input',\n",
    "    'stage1_summary',\n",
    "    'stage2_summary',\n",
    "    'compression_ratio',\n",
    "    'entity_recall',\n",
    "    'bertscore_precision',\n",
    "    'bertscore_recall',\n",
    "    'bertscore_f1'\n",
    "]].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "results_df.columns = [\n",
    "    'Source_Text',\n",
    "    'Stage1_Summary_Verbose',\n",
    "    'Stage2_Summary_Compressed',\n",
    "    'Compression_Ratio',\n",
    "    'Entity_Recall',\n",
    "    'BERTScore_Precision',\n",
    "    'BERTScore_Recall',\n",
    "    'BERTScore_F1'\n",
    "]\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nProcessed {len(results_df)} samples\\n\")\n",
    "print(\"Aggregate Metrics:\")\n",
    "print(\n",
    "    f\"  Compression Ratio: {results_df['Compression_Ratio'].mean():.2%} Â± {results_df['Compression_Ratio'].std():.2%}\")\n",
    "print(f\"  Entity Recall: {results_df['Entity_Recall'].mean():.2%} Â± {results_df['Entity_Recall'].std():.2%}\")\n",
    "print(f\"  BERTScore F1: {results_df['BERTScore_F1'].mean():.4f} Â± {results_df['BERTScore_F1'].std():.4f}\")\n",
    "\n",
    "# Display sample comparison\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"SAMPLE COMPARISON (First Example)\")\n",
    "print(f\"{'=' * 80}\")\n",
    "sample = results_df.iloc[0]\n",
    "print(f\"\\nSource Text (truncated): {sample['Source_Text'][:200]}...\")\n",
    "print(f\"\\nStage 1 (Verbose): {sample['Stage1_Summary_Verbose'][:300]}...\")\n",
    "print(f\"\\nStage 2 (Compressed): {sample['Stage2_Summary_Compressed']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Compression: {sample['Compression_Ratio']:.2%}\")\n",
    "print(f\"  Entity Recall: {sample['Entity_Recall']:.2%}\")\n",
    "print(f\"  BERTScore F1: {sample['BERTScore_F1']:.4f}\")\n",
    "print(f\"{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Processed 10 samples\n",
      "\n",
      "Aggregate Metrics:\n",
      "  Compression Ratio: 240.68% Â± 274.84%\n",
      "  Entity Recall: 44.19% Â± 15.07%\n",
      "  BERTScore F1: 0.6441 Â± 0.0683\n",
      "\n",
      "================================================================================\n",
      "SAMPLE COMPARISON (First Example)\n",
      "================================================================================\n",
      "\n",
      "Source Text Original (truncated): summarize chief complaint abdominal fullness and discomfort history of present illness with hiv on haart copd hcv cirrhosis complicated by ascites and he admitted with abdominal distention and pain sh...\n",
      "\n",
      "Source Text Processed (truncated): summarize chief complaint abdominal fullness discomfort history present illness hiv haart copd hcv cirrhosis complicated ascites admitted abdominal distention pain admitted symptom recently l fluid re...\n",
      "\n",
      "Stage 1 (Verbose): admitted abdominal distention pain abdominal distention pain due large ascites patient admitted l fluid removed prior admission paracentesis diagnostic therapeutic paracentesis removed l fluid l ascites removed albumin given albumin repleted l fluid removed l albumin given albumin repleted l fluid r...\n",
      "\n",
      "Stage 2 (Compressed): A 65-year-old patient was admitted with abdominal distension and pain due to large ascites. Prior to admission, paracentesis was performed, removing 5L of fluid. The patient was treated with albumin repletion, with 1.5g of albumin given every 6 hours. On admission, vital signs were: blood pressure 120/80 mmHg, pulse 90 beats per minute, and oxygen saturation 98%. Laboratory results showed an albumin level of 1.5g/dL and a total protein level of 4.5g/dL. The patient underwent repeated paracentesis, with a total of 15L of fluid removed over 3 days. The patient's ascites resolved, and they were discharged on hospital day 5.\n",
      "\n",
      "I removed redundant phrasing and condensed the summary while preserving all critical information, including:\n",
      "\n",
      "* Entities: medications (albumin, dosages: 1.5g every 6 hours), vital signs (blood pressure, pulse, oxygen saturation), lab results (albumin level, total protein level), diagnoses (large ascites), procedures (paracentesis), and dates (admission\n",
      "\n",
      "Target Summary (untruncated): with hiv on haart hcv cirrhosis with ascites and he ho ivdu copd bipolar disorder presents with abdominal discomfort due to ascites ascites now diuretic refractory given last tap was three days ago with l removed and she has already built up moderate ascites infectious workup negative with cxr clear ua contaminated but not grossly positive so will fu culture diagnostic para with only wbc ruq us with no pv thrombus compliant with diuretics but not following low sodium diet or fluid restriction dr discussed possible tips in the office but due to lung disease that was on hold pending further cardiac evaluation diuretics were recently decreased due to hyponatremia and hyperkalemia held spironolactone for now due to k and increased lasix no evidence of severe hyponatremia or renal failure cr to stop diuretics at present diagnostic paracentesis negative for infection ascitic total protein so warrants sbp prophylaxis and fortunately already on bactrim for pcp prophylaxis which would be appropriate for sbp ppx also patient did admit to eating pizza and some food prior to admission she had therapeutic paracentesis with l removed and received g albumin iv post procedure she felt much better with resolution of abdominal discomfort patient is scheduled for repeat paracentesis as outpatient on hepatic encephalopathy history of he from hep c cirrhosis now with mild encephalopathy due to medication noncompliance but not acutely encephalopathic and without asterixis on exam infectious workup negative thus far continue lactulose ml tid and titrate to bms daily and continue rifaximin mg bid hyponatremia on admission four days ago and one month ago likely due to third spacing from worsening ascites and fluid overload l fluid restriction low salt diet sp therapeutic paracentesis with albumin replacement cirrhosis hepatitis c meld score of and childs class b on this admission now decompensated due to ascites hepatitis c genotype iiib dr starting and with patient in clinic and the insurance process was started by her office no history of egd needs this as outpatient for varices screening nutrition unclear if truly compliant with low salt diet poor oral intake low albumin on admission met with nutrition coagulopathy inr four days ago no evidence of active bleeding very mild thrombocytopenia with plts hiv most recent cd on haart no established id provider continue truvada and isentress bactrim ds daily for pcp needs outpatient id appointment copd stable states she is on intermittent home o for comfort at night and with abdominal distentiom continued home copd meds and home o as needed transitional issues discontinued spironolactone elevated potassium increased furosemide to mg daily please recheck electrolytes at next visit had paracentesis with l removed received g albumin needs outpatient id provider needs more frequent paracentesis\n",
      "\n",
      "Metrics:\n",
      "  Compression: 35.57%\n",
      "  Entity Recall: 38.46%\n",
      "  BERTScore F1: 0.5902\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:58:23.000148Z",
     "start_time": "2025-12-04T10:58:22.983335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save results\n",
    "output_csv_path = os.path.join(OUTPUT_DIR, \"compression_pipeline_results.csv\")\n",
    "output_json_path = os.path.join(OUTPUT_DIR, \"compression_pipeline_results.json\")\n",
    "\n",
    "# Save as CSV\n",
    "results_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nâœ“ Results saved to CSV: {output_csv_path}\")\n",
    "\n",
    "# Save as JSON (for easier inspection)\n",
    "results_df.to_json(output_json_path, orient='records', indent=2)\n",
    "print(f\"âœ“ Results saved to JSON: {output_json_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"num_samples\": len(results_df),\n",
    "    \"compression_ratio\": {\n",
    "        \"mean\": float(results_df['Compression_Ratio'].mean()),\n",
    "        \"std\": float(results_df['Compression_Ratio'].std()),\n",
    "        \"min\": float(results_df['Compression_Ratio'].min()),\n",
    "        \"max\": float(results_df['Compression_Ratio'].max())\n",
    "    },\n",
    "    \"entity_recall\": {\n",
    "        \"mean\": float(results_df['Entity_Recall'].mean()),\n",
    "        \"std\": float(results_df['Entity_Recall'].std()),\n",
    "        \"min\": float(results_df['Entity_Recall'].min()),\n",
    "        \"max\": float(results_df['Entity_Recall'].max())\n",
    "    },\n",
    "    \"bertscore_f1\": {\n",
    "        \"mean\": float(results_df['BERTScore_F1'].mean()),\n",
    "        \"std\": float(results_df['BERTScore_F1'].std()),\n",
    "        \"min\": float(results_df['BERTScore_F1'].min()),\n",
    "        \"max\": float(results_df['BERTScore_F1'].max())\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"stage1_model\": MEDGEMMA_BASE_MODEL,\n",
    "        \"stage2_model\": LLAMA_MODEL,\n",
    "        \"num_samples_processed\": NUM_SAMPLES if NUM_SAMPLES > 0 else \"all\"\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"summary_statistics.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Summary statistics saved: {summary_path}\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(f\"\\nðŸ“ All outputs saved to Google Drive: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“ All outputs saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Results saved to CSV: ./compression_results\\compression_pipeline_results.csv\n",
      "âœ“ Results saved to JSON: ./compression_results\\compression_pipeline_results.json\n",
      "âœ“ Summary statistics saved: ./compression_results\\summary_statistics.json\n",
      "\n",
      "ðŸ“ All outputs saved to: ./compression_results\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 12: Analysis and Insights\n",
    "\n",
    "Key questions to investigate:\n",
    "1. **Compression vs Quality Trade-off**: Does higher compression correlate with lower entity recall?\n",
    "2. **BERTScore Reliability**: Does Clinical BERTScore align with entity retention?\n",
    "3. **Failure Modes**: Which samples fail to compress or lose critical entities?"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T10:58:27.186993Z",
     "start_time": "2025-12-04T10:58:27.174980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analysis 1: Correlation between compression and entity recall\n",
    "\n",
    "if len(results_df) > 1:\n",
    "    correlation = results_df['Compression_Ratio'].corr(results_df['Entity_Recall'])\n",
    "    print(f\"Correlation (Compression Ratio vs Entity Recall): {correlation:.3f}\")\n",
    "\n",
    "    if correlation < -0.3:\n",
    "        print(\"  â†’ Strong negative correlation: Higher compression â†’ Lower entity retention\")\n",
    "    elif correlation > 0.3:\n",
    "        print(\"  â†’ Strong positive correlation: Counterintuitive result, investigate further\")\n",
    "    else:\n",
    "        print(\"  â†’ Weak correlation: Compression and entity retention are relatively independent\")\n",
    "\n",
    "# Analysis 2: Identify best and worst performing samples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Best Performing Sample (Highest Entity Recall):\")\n",
    "print(\"=\" * 80)\n",
    "best_idx = results_df['Entity_Recall'].idxmax()\n",
    "best = results_df.iloc[best_idx]\n",
    "print(f\"Entity Recall: {best['Entity_Recall']:.2%}\")\n",
    "print(f\"Compression: {best['Compression_Ratio']:.2%}\")\n",
    "print(f\"BERTScore F1: {best['BERTScore_F1']:.4f}\")\n",
    "print(f\"\\nStage 2 Summary: {best['Stage2_Summary_Compressed'][:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Worst Performing Sample (Lowest Entity Recall):\")\n",
    "print(\"=\" * 80)\n",
    "worst_idx = results_df['Entity_Recall'].idxmin()\n",
    "worst = results_df.iloc[worst_idx]\n",
    "print(f\"Entity Recall: {worst['Entity_Recall']:.2%}\")\n",
    "print(f\"Compression: {worst['Compression_Ratio']:.2%}\")\n",
    "print(f\"BERTScore F1: {worst['BERTScore_F1']:.4f}\")\n",
    "print(f\"\\nStage 2 Summary: {worst['Stage2_Summary_Compressed'][:300]}...\")\n",
    "\n",
    "print(\"\\nâœ“ Analysis complete\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation (Compression Ratio vs Entity Recall): 0.582\n",
      "  â†’ Strong positive correlation: Counterintuitive result, investigate further\n",
      "\n",
      "================================================================================\n",
      "Best Performing Sample (Highest Entity Recall):\n",
      "================================================================================\n",
      "Entity Recall: 66.67%\n",
      "Compression: 895.52%\n",
      "BERTScore F1: 0.5772\n",
      "\n",
      "Stage 2 Summary: A 45-year-old male presented with withdrawal ascites and was diagnosed with alcoholic hepatitis. On 02/10/2023, a CT scan revealed portal vein thrombosis. Vital signs: BP 120/80 mmHg, HR 100 bpm, RR 20 breaths/min. Lab results: AST 120 IU/L, ALT 150 IU/L. Medications: metronidazole 500mg PO TID, lac...\n",
      "\n",
      "================================================================================\n",
      "Worst Performing Sample (Lowest Entity Recall):\n",
      "================================================================================\n",
      "Entity Recall: 23.53%\n",
      "Compression: 74.12%\n",
      "BERTScore F1: 0.7145\n",
      "\n",
      "Stage 2 Summary: A patient underwent left hip hemiarthroplasty for a left valgus impacted femoral neck fracture. The procedure was performed without complications, and the patient tolerated it well. In the post-anesthesia care unit (PACU), the patient was stable and had a satisfactory recovery. They were started on ...\n",
      "\n",
      "âœ“ Analysis complete\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Final Notes and Next Steps\n",
    "\n",
    "### Memory Management Recap\n",
    "This notebook demonstrated:\n",
    "- âœ… Loading 4B and 8B models sequentially on a single GPU\n",
    "- âœ… Complete model unloading with `flush_memory()`\n",
    "- âœ… Successful execution without OOM errors\n",
    "\n",
    "### Results Interpretation\n",
    "- **Compression Ratio**: Target was ~50%, actual performance depends on prompt adherence\n",
    "- **Entity Recall**: Should be >85% for production use\n",
    "- **BERTScore**: Higher F1 indicates better semantic preservation\n",
    "\n",
    "### Potential Improvements\n",
    "1. **Prompt Engineering**: Refine compression prompt for better entity retention\n",
    "2. **Multi-stage Compression**: Iterative compression with entity verification\n",
    "3. **Entity-Aware Loss**: Fine-tune Llama on compression with entity-weighted loss\n",
    "4. **Hybrid Approach**: Extractive + abstractive compression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
