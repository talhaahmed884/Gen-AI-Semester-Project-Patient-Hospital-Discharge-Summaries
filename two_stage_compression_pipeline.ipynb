{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Two-Stage Clinical Summarization Pipeline: MedGemma â†’ Llama Compression\n",
    "\n",
    "**Course Project - Stage 2: Summary Compression with Entity Retention**\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "```\n",
    "Clinical Notes â†’ [Stage 1: MedGemma-4B] â†’ Verbose Summary â†’ [Stage 2: Llama-3-8B] â†’ Compressed Summary\n",
    "```\n",
    "\n",
    "**Critical Design Decisions**:\n",
    "- **Memory Safety**: Each model is fully unloaded before loading the next (single GPU constraint)\n",
    "- **4-bit Quantization**: Both models use bitsandbytes NF4 for VRAM efficiency\n",
    "- **Entity Preservation**: Chain-of-Density inspired compression with medical entity retention\n",
    "- **Clinical Evaluation**: NER-based entity recall + Clinical BERTScore\n",
    "\n",
    "**Execution Environment**: \n",
    "- Local: Runs with local file paths\n",
    "- Google Colab: Integrates with Google Drive for data I/O"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 0: Environment Detection and Setup\n",
    "\n",
    "Automatically detect execution environment (Colab vs Local) and configure accordingly."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:08:36.340576Z",
     "start_time": "2025-12-04T07:08:36.333309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# Detect execution environment\n",
    "import sys\n",
    "\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Execution Environment: {'Google Colab' if IS_COLAB else 'Local'}\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"\\nâš™ï¸  Colab-specific setup will be activated\")\n",
    "    print(\"   - Google Drive mounting\")\n",
    "    print(\"   - Drive-based data I/O\")\n",
    "    print(\"   - GPU verification\")\n",
    "else:\n",
    "    print(\"\\nâš™ï¸  Local execution mode\")\n",
    "    print(\"   - Using local file paths\")\n",
    "    print(\"   - Saving outputs to project directory\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Environment: Local\n",
      "\n",
      "âš™ï¸  Local execution mode\n",
      "   - Using local file paths\n",
      "   - Saving outputs to project directory\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 1: Installation and Imports\n",
    "\n",
    "**Memory-Critical Libraries**:\n",
    "- `bitsandbytes`: 4-bit quantization\n",
    "- `accelerate`: Device mapping and offloading\n",
    "- `scispacy`: Medical NER for entity extraction"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T08:49:16.660722Z",
     "start_time": "2025-12-04T08:49:01.548463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install required packages (Colab only - local assumes dependencies are installed)\n",
    "if IS_COLAB:\n",
    "    print(\"Installing dependencies for Colab...\\n\")\n",
    "    !pip install -q transformers accelerate bitsandbytes torch\n",
    "    !pip install -q scispacy\n",
    "    !pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
    "    !pip install -q bert-score\n",
    "    !pip install -q pandas numpy\n",
    "    print(\"âœ“ All dependencies installed\")\n",
    "else:\n",
    "    !pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
    "    print(\"Local mode: Assuming all dependencies are already installed via uv/pip\")\n",
    "    print(\"Required: transformers, accelerate, bitsandbytes, torch, scispacy, bert-score\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local mode: Assuming all dependencies are already installed via uv/pip\n",
      "Required: transformers, accelerate, bitsandbytes, torch, scispacy, bert-score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\talha\\PycharmProjects\\Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries\\.venv\\Lib\\site-packages\\~pacy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:08:45.982487Z",
     "start_time": "2025-12-04T07:08:40.665765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Core imports\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from bert_score import BERTScorer\n",
    "import spacy\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  WARNING: No GPU detected. This pipeline requires CUDA.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "VRAM: 8.55 GB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 2: Google Drive Setup (Colab Only)\n",
    "\n",
    "Mount Google Drive and configure I/O paths for Colab execution."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:08:45.991052Z",
     "start_time": "2025-12-04T07:08:45.986282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Configure paths - UPDATE THESE TO MATCH YOUR DRIVE STRUCTURE\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive/Clinical_Summarization_Project\"\n",
    "\n",
    "    # Input: Path to your MIMIC cleaned dataset\n",
    "    INPUT_DATA_PATH = f\"{DRIVE_BASE}/mimic_cleaned_text_only.csv\"\n",
    "\n",
    "    # Input: Path to your fine-tuned MedGemma adapters (if using PEFT)\n",
    "    MEDGEMMA_ADAPTER_PATH = f\"{DRIVE_BASE}/medgemma-discharge-summarization/final\"\n",
    "\n",
    "    # Output: Where to save results\n",
    "    OUTPUT_DIR = f\"{DRIVE_BASE}/compression_results\"\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"âœ“ Google Drive mounted\")\n",
    "    print(f\"\\nConfigured paths:\")\n",
    "    print(f\"  Input data: {INPUT_DATA_PATH}\")\n",
    "    print(f\"  MedGemma adapters: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    # Verify input file exists\n",
    "    if os.path.exists(INPUT_DATA_PATH):\n",
    "        print(f\"\\nâœ“ Input data file found\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  WARNING: Input file not found at {INPUT_DATA_PATH}\")\n",
    "        print(f\"   Please update INPUT_DATA_PATH variable above\")\n",
    "\n",
    "else:\n",
    "    # Local paths\n",
    "    INPUT_DATA_PATH = \"mimic_cleaned_text_only.csv\"\n",
    "    MEDGEMMA_ADAPTER_PATH = \"./medgemma-discharge-summarization/final\"\n",
    "    OUTPUT_DIR = \"./compression_results\"\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"Local paths configured:\")\n",
    "    print(f\"  Input data: {INPUT_DATA_PATH}\")\n",
    "    print(f\"  MedGemma adapters: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(f\"  Output directory: {OUTPUT_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local paths configured:\n",
      "  Input data: mimic_cleaned_text_only.csv\n",
      "  MedGemma adapters: ./medgemma-discharge-summarization/final\n",
      "  Output directory: ./compression_results\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 3: Configuration and Hyperparameters\n",
    "\n",
    "**Critical Memory Settings**:\n",
    "- Both models use NF4 4-bit quantization\n",
    "- `device_map=\"auto\"` for optimal GPU distribution\n",
    "- Explicit memory flushing between stages"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:08:48.940768Z",
     "start_time": "2025-12-04T07:08:48.934680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# MODEL IDENTIFIERS\n",
    "# ============================================================================\n",
    "\n",
    "# Stage 1: High-recall summary generation\n",
    "MEDGEMMA_BASE_MODEL = \"google/medgemma-4b-it\"\n",
    "USE_MEDGEMMA_ADAPTER = True  # Set to False if using base model without LoRA\n",
    "\n",
    "# Stage 2: Compression model\n",
    "LLAMA_MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# ============================================================================\n",
    "# QUANTIZATION CONFIG (Shared across both models)\n",
    "# ============================================================================\n",
    "\n",
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Stage 1: Verbose summary generation\n",
    "STAGE1_MAX_TOKENS = 512\n",
    "STAGE1_TEMPERATURE = 0.7\n",
    "STAGE1_TOP_P = 0.9\n",
    "\n",
    "# Stage 2: Compression generation\n",
    "STAGE2_MAX_TOKENS = 256  # Deliberately shorter for compression\n",
    "STAGE2_TEMPERATURE = 0.3  # Lower temperature for more deterministic compression\n",
    "STAGE2_TOP_P = 0.9\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "# Number of samples to process (set lower for quick testing)\n",
    "NUM_SAMPLES = 10  # Change to -1 to process entire dataset\n",
    "\n",
    "# NER model for entity extraction\n",
    "SPACY_MODEL = \"en_core_sci_sm\"  # Medical NER model\n",
    "\n",
    "# Clinical BERTScore model\n",
    "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"\\nStage 1 Model: {MEDGEMMA_BASE_MODEL}\")\n",
    "print(f\"  - Using LoRA adapters: {USE_MEDGEMMA_ADAPTER}\")\n",
    "print(f\"  - Max tokens: {STAGE1_MAX_TOKENS}\")\n",
    "print(f\"\\nStage 2 Model: {LLAMA_MODEL}\")\n",
    "print(f\"  - Max tokens: {STAGE2_MAX_TOKENS}\")\n",
    "print(f\"  - Temperature: {STAGE2_TEMPERATURE} (deterministic compression)\")\n",
    "print(f\"\\nProcessing {NUM_SAMPLES if NUM_SAMPLES > 0 else 'ALL'} samples\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "\n",
      "Stage 1 Model: google/medgemma-4b-it\n",
      "  - Using LoRA adapters: True\n",
      "  - Max tokens: 512\n",
      "\n",
      "Stage 2 Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  - Max tokens: 256\n",
      "  - Temperature: 0.3 (deterministic compression)\n",
      "\n",
      "Processing 10 samples\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 4: Memory Management Utilities\n",
    "\n",
    "**Critical for Single-GPU Execution**: These functions ensure complete model unloading between stages."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:08:51.512237Z",
     "start_time": "2025-12-04T07:08:51.432921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flush_memory():\n",
    "    \"\"\"\n",
    "    Aggressively flush GPU and CPU memory.\n",
    "    \n",
    "    This is CRITICAL when switching between models on a single GPU.\n",
    "    Without this, you will encounter OOM errors.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "    # Report memory status\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        print(f\"GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "\n",
    "\n",
    "def unload_model(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Completely unload a model and tokenizer from memory.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to unload (or None)\n",
    "        tokenizer: The tokenizer to unload (or None)\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ§¹ Unloading model from memory...\")\n",
    "\n",
    "    if model is not None:\n",
    "        # Move model to CPU first (if on GPU)\n",
    "        if hasattr(model, 'cpu'):\n",
    "            model.cpu()\n",
    "        del model\n",
    "\n",
    "    if tokenizer is not None:\n",
    "        del tokenizer\n",
    "\n",
    "    flush_memory()\n",
    "    print(\"âœ“ Model unloaded\\n\")\n",
    "\n",
    "\n",
    "# Test memory utilities\n",
    "print(\"Memory management utilities loaded\\n\")\n",
    "flush_memory()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory management utilities loaded\n",
      "\n",
      "GPU Memory: 0.00 GB allocated, 0.00 GB reserved\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 5: Load Input Data\n",
    "\n",
    "Load clinical notes dataset (preprocessed from Stage 0)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:08:59.132158Z",
     "start_time": "2025-12-04T07:08:53.624256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Loading data from: {INPUT_DATA_PATH}\\n\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(INPUT_DATA_PATH)\n",
    "\n",
    "print(f\"âœ“ Dataset loaded: {len(df)} total samples\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Select subset if configured\n",
    "if NUM_SAMPLES > 0 and NUM_SAMPLES < len(df):\n",
    "    df = df.head(NUM_SAMPLES)\n",
    "    print(f\"\\nðŸ“Š Processing {len(df)} samples (subset for testing)\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Processing all {len(df)} samples\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"SAMPLE INPUT (First 300 chars):\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(df.iloc[0]['final_input'][:300] + \"...\")\n",
    "print(f\"\\n{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: mimic_cleaned_text_only.csv\n",
      "\n",
      "âœ“ Dataset loaded: 269516 total samples\n",
      "  Columns: ['final_input', 'final_target']\n",
      "\n",
      "ðŸ“Š Processing 10 samples (subset for testing)\n",
      "\n",
      "================================================================================\n",
      "SAMPLE INPUT (First 300 chars):\n",
      "================================================================================\n",
      "summarize chief complaint worsening abd distension pain history present illness hcv cirrhosis cb ascites hiv art ho ivdu copd bioplar ptsd presented osh ed worsening abd distension past week pt report selfdiscontinuing lasix spirnolactone week ago feel like dont anything doesnt want put chemical fol...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 6: STAGE 1 - MedGemma Verbose Summary Generation\n",
    "\n",
    "**Objective**: Generate high-recall, detailed clinical summaries.\n",
    "\n",
    "**Memory Strategy**: \n",
    "1. Load MedGemma-4B in 4-bit\n",
    "2. Generate summaries for all samples\n",
    "3. **CRITICAL**: Completely unload before Stage 2"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:08:59.139420Z",
     "start_time": "2025-12-04T07:08:59.134034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Enable synchronous CUDA for better error messages\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"âœ“ CUDA synchronous mode enabled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CUDA synchronous mode enabled\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:09:15.295178Z",
     "start_time": "2025-12-04T07:08:59.157738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1: LOADING MEDGEMMA-4B FOR VERBOSE SUMMARY GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Load base model FIRST (before tokenizer)\n",
    "print(f\"\\nLoading base model: {MEDGEMMA_BASE_MODEL}\")\n",
    "print(\"  Quantization: 4-bit NF4\")\n",
    "print(\"  This may take 2-3 minutes...\")\n",
    "\n",
    "medgemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MEDGEMMA_BASE_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "print(\"âœ“ Base model loaded\")\n",
    "\n",
    "# Step 2: Load LoRA adapters NEXT (before tokenizer)\n",
    "if USE_MEDGEMMA_ADAPTER and os.path.exists(MEDGEMMA_ADAPTER_PATH):\n",
    "    print(f\"\\nLoading LoRA adapters from: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    from peft import PeftModel\n",
    "\n",
    "    medgemma_model = PeftModel.from_pretrained(medgemma_model, MEDGEMMA_ADAPTER_PATH)\n",
    "    print(\"âœ“ LoRA adapters loaded\")\n",
    "\n",
    "    # CRITICAL: After loading LoRA, the model config may have updated vocab size\n",
    "    # We need to use THIS vocab size for the tokenizer\n",
    "\n",
    "elif USE_MEDGEMMA_ADAPTER:\n",
    "    print(f\"\\nâš ï¸  WARNING: Adapter path not found: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    print(\"   Continuing with base model only\")\n",
    "\n",
    "# Step 3: Get the ACTUAL vocab size from the loaded model\n",
    "embedding_layer = medgemma_model.get_input_embeddings()\n",
    "actual_vocab_size = embedding_layer.weight.shape[0]\n",
    "print(f\"\\n  Model embedding vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Step 4: Load tokenizer AFTER model is fully loaded\n",
    "# Try adapter path first, then base model\n",
    "if USE_MEDGEMMA_ADAPTER and os.path.exists(MEDGEMMA_ADAPTER_PATH):\n",
    "    print(f\"\\nAttempting to load tokenizer from adapter path: {MEDGEMMA_ADAPTER_PATH}\")\n",
    "    try:\n",
    "        medgemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MEDGEMMA_ADAPTER_PATH,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "        print(\"âœ“ Tokenizer loaded from adapter path\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Adapter tokenizer failed: {e}\")\n",
    "        print(\"   Loading base model tokenizer instead\")\n",
    "        medgemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MEDGEMMA_BASE_MODEL,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "else:\n",
    "    print(f\"\\nLoading tokenizer from base model: {MEDGEMMA_BASE_MODEL}\")\n",
    "    medgemma_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MEDGEMMA_BASE_MODEL,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\",\n",
    "        add_eos_token=True\n",
    "    )\n",
    "\n",
    "medgemma_tokenizer.pad_token = medgemma_tokenizer.eos_token\n",
    "\n",
    "print(f\"\\n  Tokenizer vocab size: {len(medgemma_tokenizer)}\")\n",
    "print(f\"  PAD token ID: {medgemma_tokenizer.pad_token_id}\")\n",
    "print(f\"  EOS token ID: {medgemma_tokenizer.eos_token_id}\")\n",
    "\n",
    "# Step 5: CRITICAL VALIDATION - Check if sizes match\n",
    "if len(medgemma_tokenizer) != actual_vocab_size:\n",
    "    print(f\"\\nâš ï¸  MISMATCH DETECTED!\")\n",
    "    print(f\"   Tokenizer vocab: {len(medgemma_tokenizer)}\")\n",
    "    print(f\"   Model vocab: {actual_vocab_size}\")\n",
    "\n",
    "    if len(medgemma_tokenizer) > actual_vocab_size:\n",
    "        print(f\"\\n   ERROR: Tokenizer is LARGER than model!\")\n",
    "        print(f\"   This WILL cause CUDA errors!\")\n",
    "        print(f\"\\n   SOLUTION: Resizing model embeddings to {len(medgemma_tokenizer)}...\")\n",
    "        medgemma_model.resize_token_embeddings(len(medgemma_tokenizer))\n",
    "        print(f\"   âœ“ Model embeddings resized\")\n",
    "\n",
    "        # Update actual vocab size\n",
    "        actual_vocab_size = medgemma_model.get_input_embeddings().weight.shape[0]\n",
    "        print(f\"   âœ“ New model vocab size: {actual_vocab_size}\")\n",
    "    else:\n",
    "        print(f\"\\n   WARNING: Model is larger than tokenizer\")\n",
    "        print(f\"   This is unusual but may work if all token IDs < {actual_vocab_size}\")\n",
    "\n",
    "# Step 6: VALIDATION TEST\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"CRITICAL VALIDATION TEST\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "test_text = \"Patient presented with chest pain.\"\n",
    "test_tokens = medgemma_tokenizer(test_text, return_tensors=\"pt\")\n",
    "max_id = test_tokens['input_ids'].max().item()\n",
    "min_id = test_tokens['input_ids'].min().item()\n",
    "\n",
    "print(f\"Test text: '{test_text}'\")\n",
    "print(f\"Token IDs: {test_tokens['input_ids'][0].tolist()}\")\n",
    "print(f\"Max token ID: {max_id}\")\n",
    "print(f\"Min token ID: {min_id}\")\n",
    "print(f\"Valid range: [0, {actual_vocab_size - 1}]\")\n",
    "\n",
    "if max_id >= actual_vocab_size:\n",
    "    print(f\"\\nâŒ CRITICAL ERROR: Token ID {max_id} >= vocab size {actual_vocab_size}\")\n",
    "    print(f\"   This WILL cause the CUDA error you're seeing!\")\n",
    "    print(f\"\\n   IMMEDIATE FIX REQUIRED:\")\n",
    "    print(f\"   medgemma_model.resize_token_embeddings({len(medgemma_tokenizer)})\")\n",
    "    raise ValueError(f\"Token ID out of range: {max_id} >= {actual_vocab_size}\")\n",
    "elif min_id < 0:\n",
    "    print(f\"\\nâŒ CRITICAL ERROR: Negative token ID {min_id}\")\n",
    "    raise ValueError(f\"Invalid token ID: {min_id}\")\n",
    "else:\n",
    "    print(f\"\\nâœ… VALIDATION PASSED!\")\n",
    "    print(f\"   All token IDs are within valid range\")\n",
    "    print(f\"   Safe to proceed with generation\")\n",
    "\n",
    "medgemma_model.eval()\n",
    "print(f\"\\nâœ“ MedGemma ready for inference\")\n",
    "flush_memory()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 1: LOADING MEDGEMMA-4B FOR VERBOSE SUMMARY GENERATION\n",
      "================================================================================\n",
      "\n",
      "Loading base model: google/medgemma-4b-it\n",
      "  Quantization: 4-bit NF4\n",
      "  This may take 2-3 minutes...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d249d35d951b4cf0b98067597ae48c04"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Base model loaded\n",
      "\n",
      "Loading LoRA adapters from: ./medgemma-discharge-summarization/final\n",
      "âœ“ LoRA adapters loaded\n",
      "\n",
      "  Model embedding vocab size: 262208\n",
      "\n",
      "Attempting to load tokenizer from adapter path: ./medgemma-discharge-summarization/final\n",
      "âœ“ Tokenizer loaded from adapter path\n",
      "\n",
      "  Tokenizer vocab size: 262145\n",
      "  PAD token ID: 1\n",
      "  EOS token ID: 1\n",
      "\n",
      "âš ï¸  MISMATCH DETECTED!\n",
      "   Tokenizer vocab: 262145\n",
      "   Model vocab: 262208\n",
      "\n",
      "   WARNING: Model is larger than tokenizer\n",
      "   This is unusual but may work if all token IDs < 262208\n",
      "\n",
      "================================================================================\n",
      "CRITICAL VALIDATION TEST\n",
      "================================================================================\n",
      "Test text: 'Patient presented with chest pain.'\n",
      "Token IDs: [2, 52420, 6212, 607, 15350, 4331, 236761, 1]\n",
      "Max token ID: 236761\n",
      "Min token ID: 1\n",
      "Valid range: [0, 262207]\n",
      "\n",
      "âœ… VALIDATION PASSED!\n",
      "   All token IDs are within valid range\n",
      "   Safe to proceed with generation\n",
      "\n",
      "âœ“ MedGemma ready for inference\n",
      "GPU Memory: 3.50 GB allocated, 3.51 GB reserved\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:09:15.418069Z",
     "start_time": "2025-12-04T07:09:15.325613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# FINAL VALIDATION BEFORE GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL PRE-GENERATION CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get actual embedding size\n",
    "embedding_size = medgemma_model.get_input_embeddings().weight.shape[0]\n",
    "\n",
    "print(f\"\\nTokenizer vocab: {len(medgemma_tokenizer)}\")\n",
    "print(f\"Model embedding size: {embedding_size}\")\n",
    "print(f\"Match: {'âœ… YES' if len(medgemma_tokenizer) == embedding_size else 'âŒ NO'}\")\n",
    "\n",
    "# Test with actual prompt format from your generation function\n",
    "test_prompt = \"\"\"<start_of_turn>user\n",
    "Summarize the following clinical discharge notes.\n",
    "\n",
    "Clinical Notes:\n",
    "Patient with hypertension.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nTesting with actual prompt format...\")\n",
    "test_inputs = medgemma_tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "max_token = test_inputs['input_ids'].max().item()\n",
    "\n",
    "print(f\"  Prompt token count: {test_inputs['input_ids'].shape[1]}\")\n",
    "print(f\"  Max token ID in prompt: {max_token}\")\n",
    "print(f\"  Valid range: [0, {embedding_size - 1}]\")\n",
    "\n",
    "if max_token >= embedding_size:\n",
    "    print(f\"\\nâŒ STOP! Token ID {max_token} is out of range!\")\n",
    "    print(f\"   DO NOT PROCEED - will cause CUDA error\")\n",
    "    print(f\"\\n   Run this fix:\")\n",
    "    print(f\"   medgemma_model.resize_token_embeddings({len(medgemma_tokenizer)})\")\n",
    "else:\n",
    "    print(f\"\\nâœ… All checks passed - safe to generate summaries\")\n",
    "\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL PRE-GENERATION CHECK\n",
      "================================================================================\n",
      "\n",
      "Tokenizer vocab: 262145\n",
      "Model embedding size: 262208\n",
      "Match: âŒ NO\n",
      "\n",
      "Testing with actual prompt format...\n",
      "  Prompt token count: 27\n",
      "  Max token ID in prompt: 236787\n",
      "  Valid range: [0, 262207]\n",
      "\n",
      "âœ… All checks passed - safe to generate summaries\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:09:15.435131Z",
     "start_time": "2025-12-04T07:09:15.429971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_stage1_summary(clinical_note: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate verbose clinical summary using MedGemma.\n",
    "    \n",
    "    Args:\n",
    "        clinical_note: Input clinical text\n",
    "    \n",
    "    Returns:\n",
    "        Generated summary (high-recall, verbose)\n",
    "    \"\"\"\n",
    "    instruction = (\n",
    "        \"Summarize the following clinical discharge notes. \"\n",
    "        \"Include ALL diagnoses, medications, vitals, lab results, \"\n",
    "        \"procedures, and follow-up instructions. \"\n",
    "        \"Ensure complete coverage of all medical entities.\"\n",
    "    )\n",
    "\n",
    "    # Format using Gemma template\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{clinical_note}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    inputs = medgemma_tokenizer(prompt, return_tensors=\"pt\").to(medgemma_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = medgemma_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=STAGE1_MAX_TOKENS,\n",
    "            temperature=STAGE1_TEMPERATURE,\n",
    "            top_p=STAGE1_TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=medgemma_tokenizer.pad_token_id,\n",
    "            eos_token_id=medgemma_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = medgemma_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the model's response\n",
    "    marker = \"<start_of_turn>model\"\n",
    "    if marker in generated_text:\n",
    "        summary = generated_text.split(marker)[-1].strip()\n",
    "    else:\n",
    "        summary = generated_text[len(prompt):].strip()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "print(\"âœ“ Stage 1 generation function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Stage 1 generation function defined\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:12:50.728016Z",
     "start_time": "2025-12-04T07:09:17.410577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Stage 1 summaries for all samples\n",
    "print(f\"\\nGenerating Stage 1 summaries for {len(df)} samples...\\n\")\n",
    "\n",
    "stage1_summaries = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    clinical_note = row['final_input']\n",
    "\n",
    "    print(f\"[{idx + 1}/{len(df)}] Generating verbose summary...\", end=\" \")\n",
    "    summary = generate_stage1_summary(clinical_note)\n",
    "    stage1_summaries.append(summary)\n",
    "    print(f\"âœ“ ({len(summary)} chars)\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['stage1_summary'] = stage1_summaries\n",
    "\n",
    "print(f\"\\nâœ“ Stage 1 complete: {len(stage1_summaries)} summaries generated\")\n",
    "print(f\"  Average length: {np.mean([len(s) for s in stage1_summaries]):.0f} characters\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STAGE 1 OUTPUT SAMPLE:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(stage1_summaries[0][:400] + \"...\")\n",
    "print(f\"{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Stage 1 summaries for 10 samples...\n",
      "\n",
      "[1/10] Generating verbose summary... âœ“ (100 chars)\n",
      "[2/10] Generating verbose summary... âœ“ (1403 chars)\n",
      "[3/10] Generating verbose summary... âœ“ (546 chars)\n",
      "[4/10] Generating verbose summary... âœ“ (1182 chars)\n",
      "[5/10] Generating verbose summary... âœ“ (33 chars)\n",
      "[6/10] Generating verbose summary... âœ“ (888 chars)\n",
      "[7/10] Generating verbose summary... âœ“ (104 chars)\n",
      "[8/10] Generating verbose summary... âœ“ (368 chars)\n",
      "[9/10] Generating verbose summary... âœ“ (46 chars)\n",
      "[10/10] Generating verbose summary... âœ“ (2113 chars)\n",
      "\n",
      "âœ“ Stage 1 complete: 10 summaries generated\n",
      "  Average length: 678 characters\n",
      "\n",
      "================================================================================\n",
      "STAGE 1 OUTPUT SAMPLE:\n",
      "================================================================================\n",
      "copd bioplar ptsd presented osh ed worsening abd distension found sbp paracentesis removal l ascites...\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CRITICAL: Unload MedGemma Before Stage 2\n",
    "\n",
    "**This step is NON-NEGOTIABLE for single-GPU execution.**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:13:38.693856Z",
     "start_time": "2025-12-04T07:13:37.017437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNLOADING MEDGEMMA-4B FROM MEMORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "unload_model(medgemma_model, medgemma_tokenizer)\n",
    "\n",
    "# Verify memory is cleared\n",
    "print(\"Memory status after unloading:\")\n",
    "flush_memory()\n",
    "\n",
    "print(\"\\nâœ“ Safe to proceed to Stage 2\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UNLOADING MEDGEMMA-4B FROM MEMORY\n",
      "================================================================================\n",
      "\n",
      "ðŸ§¹ Unloading model from memory...\n",
      "GPU Memory: 0.01 GB allocated, 0.02 GB reserved\n",
      "âœ“ Model unloaded\n",
      "\n",
      "Memory status after unloading:\n",
      "GPU Memory: 0.01 GB allocated, 0.02 GB reserved\n",
      "\n",
      "âœ“ Safe to proceed to Stage 2\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 7: STAGE 2 - Llama-3 Compression\n",
    "\n",
    "**Objective**: Compress verbose summaries while preserving medical entities.\n",
    "\n",
    "**Chain-of-Density Inspired Prompt**: Forces 50% compression with entity retention constraint."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:13:58.022331Z",
     "start_time": "2025-12-04T07:13:58.009981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c967a1e6028d41488dbd8913ef92bc49"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:14:36.048789Z",
     "start_time": "2025-12-04T07:14:16.428866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 2: LOADING LLAMA-3-8B FOR COMPRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if we need HuggingFace authentication for Llama\n",
    "if IS_COLAB:\n",
    "    print(\"\\nâš ï¸  Llama-3 requires HuggingFace authentication\")\n",
    "    print(\"   Running authentication flow...\\n\")\n",
    "    from huggingface_hub import notebook_login\n",
    "\n",
    "    notebook_login()\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {LLAMA_MODEL}\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LLAMA_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"  # Llama uses left padding\n",
    ")\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "print(\"âœ“ Tokenizer loaded\")\n",
    "\n",
    "# Load model with quantization\n",
    "print(f\"\\nLoading model: {LLAMA_MODEL}\")\n",
    "print(\"  Quantization: 4-bit NF4\")\n",
    "print(\"  This may take 2-3 minutes...\")\n",
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL,\n",
    "    quantization_config=BNB_CONFIG,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "llama_model.eval()\n",
    "print(f\"\\nâœ“ Llama-3 ready for compression\")\n",
    "flush_memory()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 2: LOADING LLAMA-3-8B FOR COMPRESSION\n",
      "================================================================================\n",
      "\n",
      "Loading tokenizer: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "âœ“ Tokenizer loaded\n",
      "\n",
      "Loading model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  Quantization: 4-bit NF4\n",
      "  This may take 2-3 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4a0da2d2a884865b4fb1297f4465fee"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3201f289f7945b9b629535715bffc0c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Llama-3 ready for compression\n",
      "GPU Memory: 5.71 GB allocated, 5.85 GB reserved\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:14:43.819486Z",
     "start_time": "2025-12-04T07:14:43.813390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_compressed_summary(verbose_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Compress summary using Llama-3 with entity retention constraint.\n",
    "    \n",
    "    Chain-of-Density inspired prompt: Forces 50% compression while\n",
    "    preserving all medical entities (medications, numbers, dates).\n",
    "    \n",
    "    Args:\n",
    "        verbose_summary: Stage 1 summary (high-recall)\n",
    "    \n",
    "    Returns:\n",
    "        Compressed summary\n",
    "    \"\"\"\n",
    "    # Chain-of-Density inspired compression prompt\n",
    "    system_prompt = (\n",
    "        \"You are a medical summarization expert. Your task is to compress \"\n",
    "        \"clinical summaries while preserving critical information.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Rewrite the following summary to be 50% shorter. \"\n",
    "        \"You MUST retain ALL entities: medications (with dosages), \"\n",
    "        \"vital signs (with numbers), lab results (with values), \"\n",
    "        \"diagnoses, procedures, and dates. \"\n",
    "        \"If you cannot shorten it without losing a critical fact, do not shorten it. \"\n",
    "        \"Remove only redundant phrasing and verbose descriptions.\\n\\n\"\n",
    "        f\"Summary to compress:\\n{verbose_summary}\"\n",
    "    )\n",
    "\n",
    "    # Format using Llama-3 chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    prompt = llama_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\").to(llama_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=STAGE2_MAX_TOKENS,\n",
    "            temperature=STAGE2_TEMPERATURE,\n",
    "            top_p=STAGE2_TOP_P,\n",
    "            do_sample=True,\n",
    "            pad_token_id=llama_tokenizer.pad_token_id,\n",
    "            eos_token_id=llama_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the assistant's response\n",
    "    # Llama-3 format: <|start_header_id|>assistant<|end_header_id|>\\n\\n{response}\n",
    "    if \"assistant\" in generated_text:\n",
    "        compressed = generated_text.split(\"assistant\")[-1].strip()\n",
    "        # Remove any remaining header artifacts\n",
    "        compressed = compressed.split(\"\\n\\n\", 1)[-1] if \"\\n\\n\" in compressed else compressed\n",
    "    else:\n",
    "        compressed = generated_text[len(prompt):].strip()\n",
    "\n",
    "    return compressed\n",
    "\n",
    "\n",
    "print(\"âœ“ Stage 2 compression function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Stage 2 compression function defined\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:54:32.541452Z",
     "start_time": "2025-12-04T07:14:49.676131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Stage 2 compressed summaries\n",
    "print(f\"\\nCompressing {len(df)} Stage 1 summaries...\\n\")\n",
    "\n",
    "stage2_summaries = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    verbose_summary = row['stage1_summary']\n",
    "\n",
    "    print(f\"[{idx + 1}/{len(df)}] Compressing summary...\", end=\" \")\n",
    "    compressed = generate_compressed_summary(verbose_summary)\n",
    "    stage2_summaries.append(compressed)\n",
    "\n",
    "    # Calculate compression ratio\n",
    "    ratio = len(compressed) / len(verbose_summary)\n",
    "    print(f\"âœ“ ({len(compressed)} chars, {ratio:.2%} of original)\")\n",
    "\n",
    "# Add to dataframe\n",
    "df['stage2_summary'] = stage2_summaries\n",
    "\n",
    "print(f\"\\nâœ“ Stage 2 complete: {len(stage2_summaries)} summaries compressed\")\n",
    "print(f\"  Average length: {np.mean([len(s) for s in stage2_summaries]):.0f} characters\")\n",
    "print(\n",
    "    f\"  Overall compression: {np.mean([len(s2) / len(s1) for s1, s2 in zip(stage1_summaries, stage2_summaries)]):.2%}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STAGE 2 OUTPUT SAMPLE:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(stage2_summaries[0])\n",
    "print(f\"{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compressing 10 Stage 1 summaries...\n",
      "\n",
      "[1/10] Compressing summary... âœ“ (383 chars, 383.00% of original)\n",
      "[2/10] Compressing summary... âœ“ (926 chars, 66.00% of original)\n",
      "[3/10] Compressing summary... âœ“ (906 chars, 165.93% of original)\n",
      "[4/10] Compressing summary... âœ“ (858 chars, 72.59% of original)\n",
      "[5/10] Compressing summary... âœ“ (615 chars, 1863.64% of original)\n",
      "[6/10] Compressing summary... âœ“ (735 chars, 82.77% of original)\n",
      "[7/10] Compressing summary... âœ“ (352 chars, 338.46% of original)\n",
      "[8/10] Compressing summary... âœ“ (742 chars, 201.63% of original)\n",
      "[9/10] Compressing summary... âœ“ (400 chars, 869.57% of original)\n",
      "[10/10] Compressing summary... âœ“ (1143 chars, 54.09% of original)\n",
      "\n",
      "âœ“ Stage 2 complete: 10 summaries compressed\n",
      "  Average length: 706 characters\n",
      "  Overall compression: 409.77%\n",
      "\n",
      "================================================================================\n",
      "STAGE 2 OUTPUT SAMPLE:\n",
      "================================================================================\n",
      "A 65-year-old patient with COPD and bipolar disorder, also experiencing PTSD, presented to the emergency department with worsening abdominal distension. SBP was 140/90 mmHg. Paracentesis was performed, and 2 liters of ascitic fluid were removed. The patient was diagnosed with chronic obstructive pulmonary disease (COPD), bipolar disorder, and post-traumatic stress disorder (PTSD).\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unload Llama-3 (Optional - only if running evaluation separately)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T08:47:39.541893Z",
     "start_time": "2025-12-04T08:47:39.536809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you need to free memory before evaluation, uncomment:\n",
    "# unload_model(llama_model, llama_tokenizer)\n",
    "\n",
    "print(\"âœ“ Stage 2 generation complete. Proceeding to evaluation...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Stage 2 generation complete. Proceeding to evaluation...\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 8: EVALUATION - Metric 1: Compression Ratio"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T08:47:46.752095Z",
     "start_time": "2025-12-04T08:47:46.740983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 1: COMPRESSION RATIO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def calculate_compression_ratio(stage1: str, stage2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate compression ratio (Stage 2 length / Stage 1 length).\n",
    "    \n",
    "    Lower values indicate stronger compression.\n",
    "    \"\"\"\n",
    "    return len(stage2) / len(stage1)\n",
    "\n",
    "\n",
    "# Calculate for all samples\n",
    "df['compression_ratio'] = df.apply(\n",
    "    lambda row: calculate_compression_ratio(row['stage1_summary'], row['stage2_summary']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nCompression Statistics:\")\n",
    "print(f\"  Mean ratio: {df['compression_ratio'].mean():.2%}\")\n",
    "print(f\"  Median ratio: {df['compression_ratio'].median():.2%}\")\n",
    "print(f\"  Min ratio: {df['compression_ratio'].min():.2%}\")\n",
    "print(f\"  Max ratio: {df['compression_ratio'].max():.2%}\")\n",
    "print(f\"  Std dev: {df['compression_ratio'].std():.2%}\")\n",
    "\n",
    "print(f\"\\nâœ“ Compression ratios calculated\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRIC 1: COMPRESSION RATIO\n",
      "================================================================================\n",
      "\n",
      "Compression Statistics:\n",
      "  Mean ratio: 409.77%\n",
      "  Median ratio: 183.78%\n",
      "  Min ratio: 54.09%\n",
      "  Max ratio: 1863.64%\n",
      "  Std dev: 567.55%\n",
      "\n",
      "âœ“ Compression ratios calculated\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 9: EVALUATION - Metric 2: Entity Retention (NER)\n",
    "\n",
    "**Method**: Extract medical entities from both Stage 1 and Stage 2 summaries using SciSpacy, then calculate recall."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T08:50:59.272535Z",
     "start_time": "2025-12-04T08:50:58.664276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 2: ENTITY RETENTION (NER)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load SciSpacy medical NER model\n",
    "print(f\"\\nLoading SciSpacy model: {SPACY_MODEL}\")\n",
    "nlp = spacy.load(SPACY_MODEL)\n",
    "print(\"âœ“ NER model loaded\")\n",
    "\n",
    "\n",
    "def extract_entities(text: str) -> set:\n",
    "    \"\"\"\n",
    "    Extract medical entities from text using SciSpacy.\n",
    "    \n",
    "    Returns set of entity texts (lowercased for comparison).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = {ent.text.lower() for ent in doc.ents}\n",
    "    return entities\n",
    "\n",
    "\n",
    "def calculate_entity_recall(stage1_text: str, stage2_text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate entity retention recall: \n",
    "    (Entities in Stage 2) / (Entities in Stage 1)\n",
    "    \n",
    "    Returns value between 0 and 1 (higher is better).\n",
    "    \"\"\"\n",
    "    entities_stage1 = extract_entities(stage1_text)\n",
    "    entities_stage2 = extract_entities(stage2_text)\n",
    "\n",
    "    if len(entities_stage1) == 0:\n",
    "        return 1.0  # No entities to preserve\n",
    "\n",
    "    # Calculate recall: how many Stage 1 entities appear in Stage 2?\n",
    "    retained = entities_stage1.intersection(entities_stage2)\n",
    "    recall = len(retained) / len(entities_stage1)\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "print(\"\\nCalculating entity recall for all samples...\\n\")\n",
    "\n",
    "entity_recalls = []\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"[{idx + 1}/{len(df)}] Extracting entities...\", end=\" \")\n",
    "    recall = calculate_entity_recall(row['stage1_summary'], row['stage2_summary'])\n",
    "    entity_recalls.append(recall)\n",
    "    print(f\"âœ“ Recall: {recall:.2%}\")\n",
    "\n",
    "df['entity_recall'] = entity_recalls\n",
    "\n",
    "print(f\"\\nEntity Retention Statistics:\")\n",
    "print(f\"  Mean recall: {df['entity_recall'].mean():.2%}\")\n",
    "print(f\"  Median recall: {df['entity_recall'].median():.2%}\")\n",
    "print(f\"  Min recall: {df['entity_recall'].min():.2%}\")\n",
    "print(f\"  Max recall: {df['entity_recall'].max():.2%}\")\n",
    "print(f\"  Std dev: {df['entity_recall'].std():.2%}\")\n",
    "\n",
    "print(f\"\\nâœ“ Entity recall calculated\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRIC 2: ENTITY RETENTION (NER)\n",
      "================================================================================\n",
      "\n",
      "Loading SciSpacy model: en_core_sci_sm\n",
      "âœ“ NER model loaded\n",
      "\n",
      "Calculating entity recall for all samples...\n",
      "\n",
      "[1/10] Extracting entities... âœ“ Recall: 33.33%\n",
      "[2/10] Extracting entities... âœ“ Recall: 55.56%\n",
      "[3/10] Extracting entities... âœ“ Recall: 60.00%\n",
      "[4/10] Extracting entities... âœ“ Recall: 22.22%\n",
      "[5/10] Extracting entities... âœ“ Recall: 100.00%\n",
      "[6/10] Extracting entities... âœ“ Recall: 35.29%\n",
      "[7/10] Extracting entities... âœ“ Recall: 33.33%\n",
      "[8/10] Extracting entities... âœ“ Recall: 40.00%\n",
      "[9/10] Extracting entities... âœ“ Recall: 100.00%\n",
      "[10/10] Extracting entities... âœ“ Recall: 39.58%\n",
      "\n",
      "Entity Retention Statistics:\n",
      "  Mean recall: 51.93%\n",
      "  Median recall: 39.79%\n",
      "  Min recall: 22.22%\n",
      "  Max recall: 100.00%\n",
      "  Std dev: 27.58%\n",
      "\n",
      "âœ“ Entity recall calculated\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 10: EVALUATION - Metric 3: Clinical BERTScore\n",
    "\n",
    "**Method**: Measure semantic similarity between original clinical notes and final compressed summaries using Bio_ClinicalBERT."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T08:51:46.478828Z",
     "start_time": "2025-12-04T08:51:25.067906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRIC 3: CLINICAL BERTSCORE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize Clinical BERTScore\n",
    "print(f\"\\nInitializing BERTScorer with {CLINICAL_BERT}\")\n",
    "print(\"This will download the model if not cached...\\n\")\n",
    "\n",
    "bert_scorer = BERTScorer(\n",
    "    model_type=CLINICAL_BERT,\n",
    "    num_layers=9,\n",
    "    rescale_with_baseline=True,\n",
    "    lang=\"en\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ BERTScorer initialized\")\n",
    "\n",
    "print(\"\\nCalculating BERTScores (this may take a few minutes)...\\n\")\n",
    "\n",
    "# Prepare data for batch scoring\n",
    "source_texts = df['final_input'].tolist()\n",
    "compressed_summaries = df['stage2_summary'].tolist()\n",
    "\n",
    "# Calculate BERTScore\n",
    "# We compare compressed summaries (candidates) against original notes (references)\n",
    "P, R, F1 = bert_scorer.score(\n",
    "    cands=compressed_summaries,\n",
    "    refs=source_texts\n",
    ")\n",
    "\n",
    "# Convert to numpy\n",
    "df['bertscore_precision'] = P.cpu().numpy()\n",
    "df['bertscore_recall'] = R.cpu().numpy()\n",
    "df['bertscore_f1'] = F1.cpu().numpy()\n",
    "\n",
    "print(f\"\\nClinical BERTScore Statistics:\")\n",
    "print(f\"\\nPrecision:\")\n",
    "print(f\"  Mean: {df['bertscore_precision'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_precision'].median():.4f}\")\n",
    "print(f\"\\nRecall:\")\n",
    "print(f\"  Mean: {df['bertscore_recall'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_recall'].median():.4f}\")\n",
    "print(f\"\\nF1:\")\n",
    "print(f\"  Mean: {df['bertscore_f1'].mean():.4f}\")\n",
    "print(f\"  Median: {df['bertscore_f1'].median():.4f}\")\n",
    "\n",
    "print(f\"\\nâœ“ Clinical BERTScore calculated\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION METRIC 3: CLINICAL BERTSCORE\n",
      "================================================================================\n",
      "\n",
      "Initializing BERTScorer with emilyalsentzer/Bio_ClinicalBERT\n",
      "This will download the model if not cached...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "223bcafc673a40a69f72127712e56cd4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43d818461a704b4aa8f6e0bb8607e804"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1a82a80dfb74995b806dd3e4bb4208f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02ea0a93e2924b5aa64552f3446b4e71"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ BERTScorer initialized\n",
      "\n",
      "Calculating BERTScores (this may take a few minutes)...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (832) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [20, 832].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     23\u001B[39m compressed_summaries = df[\u001B[33m'\u001B[39m\u001B[33mstage2_summary\u001B[39m\u001B[33m'\u001B[39m].tolist()\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# Calculate BERTScore\u001B[39;00m\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# We compare compressed summaries (candidates) against original notes (references)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m P, R, F1 = \u001B[43mbert_scorer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcands\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompressed_summaries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource_texts\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# Convert to numpy\u001B[39;00m\n\u001B[32m     33\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mbertscore_precision\u001B[39m\u001B[33m'\u001B[39m] = P.cpu().numpy()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries\\.venv\\Lib\\site-packages\\bert_score\\scorer.py:220\u001B[39m, in \u001B[36mBERTScorer.score\u001B[39m\u001B[34m(self, cands, refs, verbose, batch_size, return_hash)\u001B[39m\n\u001B[32m    217\u001B[39m     idf_dict[\u001B[38;5;28mself\u001B[39m._tokenizer.sep_token_id] = \u001B[32m0\u001B[39m\n\u001B[32m    218\u001B[39m     idf_dict[\u001B[38;5;28mself\u001B[39m._tokenizer.cls_token_id] = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m220\u001B[39m all_preds = \u001B[43mbert_cos_score_idf\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    222\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrefs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    223\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcands\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    224\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_tokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    225\u001B[39m \u001B[43m    \u001B[49m\u001B[43midf_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    227\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    228\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    229\u001B[39m \u001B[43m    \u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    230\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m.cpu()\n\u001B[32m    232\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ref_group_boundaries \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    233\u001B[39m     max_preds = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries\\.venv\\Lib\\site-packages\\bert_score\\utils.py:616\u001B[39m, in \u001B[36mbert_cos_score_idf\u001B[39m\u001B[34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001B[39m\n\u001B[32m    614\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch_start \u001B[38;5;129;01min\u001B[39;00m iter_range:\n\u001B[32m    615\u001B[39m     sen_batch = sentences[batch_start : batch_start + batch_size]\n\u001B[32m--> \u001B[39m\u001B[32m616\u001B[39m     embs, masks, padded_idf = \u001B[43mget_bert_embedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    617\u001B[39m \u001B[43m        \u001B[49m\u001B[43msen_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midf_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mall_layers\u001B[49m\n\u001B[32m    618\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    619\u001B[39m     embs = embs.cpu()\n\u001B[32m    620\u001B[39m     masks = masks.cpu()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries\\.venv\\Lib\\site-packages\\bert_score\\utils.py:455\u001B[39m, in \u001B[36mget_bert_embedding\u001B[39m\u001B[34m(all_sens, model, tokenizer, idf_dict, batch_size, device, all_layers)\u001B[39m\n\u001B[32m    453\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m    454\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(all_sens), batch_size):\n\u001B[32m--> \u001B[39m\u001B[32m455\u001B[39m         batch_embedding = \u001B[43mbert_encode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    456\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    457\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpadded_sens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    458\u001B[39m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    459\u001B[39m \u001B[43m            \u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    460\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    461\u001B[39m         embeddings.append(batch_embedding)\n\u001B[32m    462\u001B[39m         \u001B[38;5;28;01mdel\u001B[39;00m batch_embedding\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries\\.venv\\Lib\\site-packages\\bert_score\\utils.py:351\u001B[39m, in \u001B[36mbert_encode\u001B[39m\u001B[34m(model, x, attention_mask, all_layers)\u001B[39m\n\u001B[32m    349\u001B[39m model.eval()\n\u001B[32m    350\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m351\u001B[39m     out = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mall_layers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m all_layers:\n\u001B[32m    353\u001B[39m     emb = torch.stack(out[-\u001B[32m1\u001B[39m], dim=\u001B[32m2\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Gen-AI-Semester-Project-Patient-Hospital-Discharge-Summaries\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:931\u001B[39m, in \u001B[36mBertModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m    929\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.embeddings, \u001B[33m\"\u001B[39m\u001B[33mtoken_type_ids\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    930\u001B[39m     buffered_token_type_ids = \u001B[38;5;28mself\u001B[39m.embeddings.token_type_ids[:, :seq_length]\n\u001B[32m--> \u001B[39m\u001B[32m931\u001B[39m     buffered_token_type_ids_expanded = \u001B[43mbuffered_token_type_ids\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexpand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    932\u001B[39m     token_type_ids = buffered_token_type_ids_expanded\n\u001B[32m    933\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mRuntimeError\u001B[39m: The expanded size of the tensor (832) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [20, 832].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section 11: Results Summary and Export"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select columns for final output\n",
    "results_df = df[[\n",
    "    'final_input',\n",
    "    'stage1_summary',\n",
    "    'stage2_summary',\n",
    "    'compression_ratio',\n",
    "    'entity_recall',\n",
    "    'bertscore_precision',\n",
    "    'bertscore_recall',\n",
    "    'bertscore_f1'\n",
    "]].copy()\n",
    "\n",
    "# Rename for clarity\n",
    "results_df.columns = [\n",
    "    'Source_Text',\n",
    "    'Stage1_Summary_Verbose',\n",
    "    'Stage2_Summary_Compressed',\n",
    "    'Compression_Ratio',\n",
    "    'Entity_Recall',\n",
    "    'BERTScore_Precision',\n",
    "    'BERTScore_Recall',\n",
    "    'BERTScore_F1'\n",
    "]\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\nProcessed {len(results_df)} samples\\n\")\n",
    "print(\"Aggregate Metrics:\")\n",
    "print(\n",
    "    f\"  Compression Ratio: {results_df['Compression_Ratio'].mean():.2%} Â± {results_df['Compression_Ratio'].std():.2%}\")\n",
    "print(f\"  Entity Recall: {results_df['Entity_Recall'].mean():.2%} Â± {results_df['Entity_Recall'].std():.2%}\")\n",
    "print(f\"  BERTScore F1: {results_df['BERTScore_F1'].mean():.4f} Â± {results_df['BERTScore_F1'].std():.4f}\")\n",
    "\n",
    "# Display sample comparison\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"SAMPLE COMPARISON (First Example)\")\n",
    "print(f\"{'=' * 80}\")\n",
    "sample = results_df.iloc[0]\n",
    "print(f\"\\nSource Text (truncated): {sample['Source_Text'][:200]}...\")\n",
    "print(f\"\\nStage 1 (Verbose): {sample['Stage1_Summary_Verbose'][:300]}...\")\n",
    "print(f\"\\nStage 2 (Compressed): {sample['Stage2_Summary_Compressed']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Compression: {sample['Compression_Ratio']:.2%}\")\n",
    "print(f\"  Entity Recall: {sample['Entity_Recall']:.2%}\")\n",
    "print(f\"  BERTScore F1: {sample['BERTScore_F1']:.4f}\")\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save results\n",
    "output_csv_path = os.path.join(OUTPUT_DIR, \"compression_pipeline_results.csv\")\n",
    "output_json_path = os.path.join(OUTPUT_DIR, \"compression_pipeline_results.json\")\n",
    "\n",
    "# Save as CSV\n",
    "results_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nâœ“ Results saved to CSV: {output_csv_path}\")\n",
    "\n",
    "# Save as JSON (for easier inspection)\n",
    "results_df.to_json(output_json_path, orient='records', indent=2)\n",
    "print(f\"âœ“ Results saved to JSON: {output_json_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"num_samples\": len(results_df),\n",
    "    \"compression_ratio\": {\n",
    "        \"mean\": float(results_df['Compression_Ratio'].mean()),\n",
    "        \"std\": float(results_df['Compression_Ratio'].std()),\n",
    "        \"min\": float(results_df['Compression_Ratio'].min()),\n",
    "        \"max\": float(results_df['Compression_Ratio'].max())\n",
    "    },\n",
    "    \"entity_recall\": {\n",
    "        \"mean\": float(results_df['Entity_Recall'].mean()),\n",
    "        \"std\": float(results_df['Entity_Recall'].std()),\n",
    "        \"min\": float(results_df['Entity_Recall'].min()),\n",
    "        \"max\": float(results_df['Entity_Recall'].max())\n",
    "    },\n",
    "    \"bertscore_f1\": {\n",
    "        \"mean\": float(results_df['BERTScore_F1'].mean()),\n",
    "        \"std\": float(results_df['BERTScore_F1'].std()),\n",
    "        \"min\": float(results_df['BERTScore_F1'].min()),\n",
    "        \"max\": float(results_df['BERTScore_F1'].max())\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"stage1_model\": MEDGEMMA_BASE_MODEL,\n",
    "        \"stage2_model\": LLAMA_MODEL,\n",
    "        \"num_samples_processed\": NUM_SAMPLES if NUM_SAMPLES > 0 else \"all\"\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"summary_statistics.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Summary statistics saved: {summary_path}\")\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(f\"\\nðŸ“ All outputs saved to Google Drive: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“ All outputs saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 12: Analysis and Insights\n",
    "\n",
    "Key questions to investigate:\n",
    "1. **Compression vs Quality Trade-off**: Does higher compression correlate with lower entity recall?\n",
    "2. **BERTScore Reliability**: Does Clinical BERTScore align with entity retention?\n",
    "3. **Failure Modes**: Which samples fail to compress or lose critical entities?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Analysis 1: Correlation between compression and entity recall\n",
    "\n",
    "if len(results_df) > 1:\n",
    "    correlation = results_df['Compression_Ratio'].corr(results_df['Entity_Recall'])\n",
    "    print(f\"Correlation (Compression Ratio vs Entity Recall): {correlation:.3f}\")\n",
    "\n",
    "    if correlation < -0.3:\n",
    "        print(\"  â†’ Strong negative correlation: Higher compression â†’ Lower entity retention\")\n",
    "    elif correlation > 0.3:\n",
    "        print(\"  â†’ Strong positive correlation: Counterintuitive result, investigate further\")\n",
    "    else:\n",
    "        print(\"  â†’ Weak correlation: Compression and entity retention are relatively independent\")\n",
    "\n",
    "# Analysis 2: Identify best and worst performing samples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Best Performing Sample (Highest Entity Recall):\")\n",
    "print(\"=\" * 80)\n",
    "best_idx = results_df['Entity_Recall'].idxmax()\n",
    "best = results_df.iloc[best_idx]\n",
    "print(f\"Entity Recall: {best['Entity_Recall']:.2%}\")\n",
    "print(f\"Compression: {best['Compression_Ratio']:.2%}\")\n",
    "print(f\"BERTScore F1: {best['BERTScore_F1']:.4f}\")\n",
    "print(f\"\\nStage 2 Summary: {best['Stage2_Summary_Compressed'][:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Worst Performing Sample (Lowest Entity Recall):\")\n",
    "print(\"=\" * 80)\n",
    "worst_idx = results_df['Entity_Recall'].idxmin()\n",
    "worst = results_df.iloc[worst_idx]\n",
    "print(f\"Entity Recall: {worst['Entity_Recall']:.2%}\")\n",
    "print(f\"Compression: {worst['Compression_Ratio']:.2%}\")\n",
    "print(f\"BERTScore F1: {worst['BERTScore_F1']:.4f}\")\n",
    "print(f\"\\nStage 2 Summary: {worst['Stage2_Summary_Compressed'][:300]}...\")\n",
    "\n",
    "print(\"\\nâœ“ Analysis complete\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Final Notes and Next Steps\n",
    "\n",
    "### Memory Management Recap\n",
    "This notebook demonstrated:\n",
    "- âœ… Loading 4B and 8B models sequentially on a single GPU\n",
    "- âœ… Complete model unloading with `flush_memory()`\n",
    "- âœ… Successful execution without OOM errors\n",
    "\n",
    "### Results Interpretation\n",
    "- **Compression Ratio**: Target was ~50%, actual performance depends on prompt adherence\n",
    "- **Entity Recall**: Should be >85% for production use\n",
    "- **BERTScore**: Higher F1 indicates better semantic preservation\n",
    "\n",
    "### Potential Improvements\n",
    "1. **Prompt Engineering**: Refine compression prompt for better entity retention\n",
    "2. **Multi-stage Compression**: Iterative compression with entity verification\n",
    "3. **Entity-Aware Loss**: Fine-tune Llama on compression with entity-weighted loss\n",
    "4. **Hybrid Approach**: Extractive + abstractive compression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
