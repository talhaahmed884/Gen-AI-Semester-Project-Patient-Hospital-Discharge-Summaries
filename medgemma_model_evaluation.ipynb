{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MedGemma Fine-Tuned Model Evaluation\n",
    "\n",
    "**Purpose**: Evaluate the fine-tuned MedGemma-4B model using Clinical BERTScore and qualitative analysis.\n",
    "\n",
    "**Model**: Fine-tuned MedGemma-4B with LoRA adapters for clinical discharge summarization.\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Clinical BERTScore (Precision, Recall, F1)\n",
    "- Qualitative comparison of generated vs reference summaries"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Installation and Imports\n",
    "\n",
    "Install required libraries for model loading and evaluation."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T09:21:37.682946Z",
     "start_time": "2025-12-04T09:21:36.080746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Uncomment for Google Colab\n",
    "!pip install -q -U transformers peft bitsandbytes accelerate bert_score scipy torch\n",
    "\n",
    "print(\"✓ Installation complete (or skipped for local environment)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Installation complete (or skipped for local environment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T09:58:32.026966Z",
     "start_time": "2025-12-04T09:58:27.205092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert_score import BERTScorer\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "GPU Memory: 8.55 GB\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set paths and model parameters."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T09:58:34.345636Z",
     "start_time": "2025-12-04T09:58:34.340951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Base model\n",
    "MODEL_NAME = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Path to fine-tuned LoRA adapters\n",
    "ADAPTER_PATH = \"./medgemma-discharge-summarization/final\"\n",
    "\n",
    "# Dataset path\n",
    "MIMIC_CSV_PATH = \"mimic_cleaned_text_only.csv\"\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "# Number of test samples to evaluate (set to -1 for all)\n",
    "NUM_TEST_SAMPLES = 100\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "REPETITION_PENALTY = 1.1\n",
    "\n",
    "# ============================================================================\n",
    "# BERTSCORE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Base model: {MODEL_NAME}\")\n",
    "print(f\"  Adapter path: {ADAPTER_PATH}\")\n",
    "print(f\"  Dataset: {MIMIC_CSV_PATH}\")\n",
    "print(f\"  Test samples: {NUM_TEST_SAMPLES if NUM_TEST_SAMPLES > 0 else 'All'}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Base model: google/medgemma-4b-it\n",
      "  Adapter path: ./medgemma-discharge-summarization/final\n",
      "  Dataset: mimic_cleaned_text_only.csv\n",
      "  Test samples: 100\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Load Test Dataset\n",
    "\n",
    "Load and prepare the MIMIC dataset for evaluation."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T09:58:42.652181Z",
     "start_time": "2025-12-04T09:58:36.741167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "print(f\"Loading dataset from: {MIMIC_CSV_PATH}\\n\")\n",
    "\n",
    "if os.path.exists(MIMIC_CSV_PATH):\n",
    "    # Load the CSV file\n",
    "    mimic_df = pd.read_csv(MIMIC_CSV_PATH)\n",
    "\n",
    "    # Take subset for testing (first 10,000 samples)\n",
    "    mimic_df = mimic_df[:10_000]\n",
    "\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"  Total samples: {len(mimic_df)}\")\n",
    "    print(f\"  Columns: {list(mimic_df.columns)}\\n\")\n",
    "\n",
    "    # Add instruction column\n",
    "    instruction_text = \"Summarize the following clinical discharge notes. Include ALL diagnoses, medications, vitals, lab results, procedures, and follow-up instructions. Ensure complete coverage of all medical entities.\"\n",
    "    mimic_df['instruction'] = instruction_text\n",
    "\n",
    "    # Rename columns\n",
    "    mimic_df = mimic_df.rename(columns={\n",
    "        'final_input': 'input',\n",
    "        'final_target': 'output'\n",
    "    })\n",
    "\n",
    "    # Remove rows with missing data\n",
    "    mimic_df = mimic_df.dropna(subset=['input', 'output'])\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(mimic_df[['instruction', 'input', 'output']])\n",
    "\n",
    "    # Split into train and test sets (5% test)\n",
    "    dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "    test_dataset = dataset[\"test\"]\n",
    "\n",
    "    # Limit test samples if configured\n",
    "    if NUM_TEST_SAMPLES > 0 and NUM_TEST_SAMPLES < len(test_dataset):\n",
    "        test_dataset = test_dataset.select(range(NUM_TEST_SAMPLES))\n",
    "\n",
    "    print(f\"✓ Test dataset prepared!\")\n",
    "    print(f\"  Test samples: {len(test_dataset)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"⚠️  File not found: {MIMIC_CSV_PATH}\")\n",
    "    print(f\"Please ensure the dataset is in the project directory\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: mimic_cleaned_text_only.csv\n",
      "\n",
      "✓ Dataset loaded successfully!\n",
      "  Total samples: 10000\n",
      "  Columns: ['final_input', 'final_target']\n",
      "\n",
      "✓ Test dataset prepared!\n",
      "  Test samples: 100\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Load Fine-Tuned Model\n",
    "\n",
    "Load the base MedGemma model with 4-bit quantization, then load the fine-tuned LoRA adapters.\n",
    "\n",
    "**Memory Optimization**: Uses QLoRA (4-bit quantization) for efficient inference."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T09:58:49.317182Z",
     "start_time": "2025-12-04T09:58:49.314656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Enable synchronous CUDA for better error messages\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"✓ CUDA synchronous mode enabled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CUDA synchronous mode enabled\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T09:59:04.835169Z",
     "start_time": "2025-12-04T09:58:50.892639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOADING FINE-TUNED MEDGEMMA MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "compute_dtype = torch.bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "print(\"\\nStep 1: Loading base model...\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Quantization: 4-bit NF4\")\n",
    "print(f\"  This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "print(\"✓ Base model loaded\")\n",
    "\n",
    "# Load LoRA adapters\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    print(f\"\\nStep 2: Loading LoRA adapters...\")\n",
    "    print(f\"  Path: {ADAPTER_PATH}\")\n",
    "    model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "    print(\"✓ LoRA adapters loaded\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  WARNING: Adapter path not found: {ADAPTER_PATH}\")\n",
    "    print(\"   Using base model only (not fine-tuned)\")\n",
    "\n",
    "# Get actual vocab size from model\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "actual_vocab_size = embedding_layer.weight.shape[0]\n",
    "print(f\"\\n  Model embedding vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nStep 3: Loading tokenizer...\")\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            ADAPTER_PATH,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "        print(\"✓ Tokenizer loaded from adapter path\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Adapter tokenizer failed: {e}\")\n",
    "        print(\"   Loading base model tokenizer instead\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\",\n",
    "        add_eos_token=True\n",
    "    )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"\\n  Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"  EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Validation check\n",
    "if len(tokenizer) != actual_vocab_size:\n",
    "    print(f\"\\n⚠️  MISMATCH DETECTED!\")\n",
    "    print(f\"   Tokenizer vocab: {len(tokenizer)}\")\n",
    "    print(f\"   Model vocab: {actual_vocab_size}\")\n",
    "\n",
    "    if len(tokenizer) > actual_vocab_size:\n",
    "        print(f\"\\n   Resizing model embeddings to {len(tokenizer)}...\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        actual_vocab_size = model.get_input_embeddings().weight.shape[0]\n",
    "        print(f\"   ✓ New model vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Validation test\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"VALIDATION TEST\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "test_text = \"Patient presented with chest pain.\"\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "max_id = test_tokens['input_ids'].max().item()\n",
    "\n",
    "print(f\"Test text: '{test_text}'\")\n",
    "print(f\"Max token ID: {max_id}\")\n",
    "print(f\"Valid range: [0, {actual_vocab_size - 1}]\")\n",
    "\n",
    "if max_id >= actual_vocab_size:\n",
    "    print(f\"\\n❌ CRITICAL ERROR: Token ID out of range!\")\n",
    "    raise ValueError(f\"Token ID {max_id} >= vocab size {actual_vocab_size}\")\n",
    "else:\n",
    "    print(f\"\\n✅ VALIDATION PASSED!\")\n",
    "    print(f\"   All token IDs are within valid range\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"\\n✓ Model ready for evaluation\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING FINE-TUNED MEDGEMMA MODEL\n",
      "================================================================================\n",
      "\n",
      "Step 1: Loading base model...\n",
      "  Model: google/medgemma-4b-it\n",
      "  Quantization: 4-bit NF4\n",
      "  This may take 2-3 minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04d7b19761ff492abdbb1ddcf26368fe"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Base model loaded\n",
      "\n",
      "Step 2: Loading LoRA adapters...\n",
      "  Path: ./medgemma-discharge-summarization/final\n",
      "✓ LoRA adapters loaded\n",
      "\n",
      "  Model embedding vocab size: 262208\n",
      "\n",
      "Step 3: Loading tokenizer...\n",
      "✓ Tokenizer loaded from adapter path\n",
      "\n",
      "  Tokenizer vocab size: 262145\n",
      "  PAD token ID: 1\n",
      "  EOS token ID: 1\n",
      "\n",
      "⚠️  MISMATCH DETECTED!\n",
      "   Tokenizer vocab: 262145\n",
      "   Model vocab: 262208\n",
      "\n",
      "================================================================================\n",
      "VALIDATION TEST\n",
      "================================================================================\n",
      "Test text: 'Patient presented with chest pain.'\n",
      "Max token ID: 236761\n",
      "Valid range: [0, 262207]\n",
      "\n",
      "✅ VALIDATION PASSED!\n",
      "   All token IDs are within valid range\n",
      "\n",
      "✓ Model ready for evaluation\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Generate Predictions on Test Set\n",
    "\n",
    "Generate clinical summaries for all test samples."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Generating predictions on test set...\\n\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    print(f\"Generating summary {i + 1}/{len(test_dataset)}...\", end=\" \")\n",
    "\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample[\"input\"]\n",
    "    reference = sample[\"output\"]\n",
    "\n",
    "    # Format the prompt (without the model's response)\n",
    "    inference_prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the model's response\n",
    "    model_response_marker = \"<start_of_turn>model\"\n",
    "    if model_response_marker in generated_text:\n",
    "        generated_summary = generated_text.split(model_response_marker)[-1].strip()\n",
    "    else:\n",
    "        generated_summary = generated_text[len(inference_prompt):].strip()\n",
    "\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(reference)\n",
    "\n",
    "    print(f\"✓ ({len(generated_summary)} chars)\")\n",
    "\n",
    "print(f\"\\n✓ All predictions generated\")\n",
    "print(f\"  Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Compute Clinical BERTScore\n",
    "\n",
    "Evaluate semantic similarity using Bio_ClinicalBERT."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T09:31:50.868520Z",
     "start_time": "2025-12-04T09:29:47.804018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPUTING CLINICAL BERTSCORE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nInitializing BERTScorer with {CLINICAL_BERT}...\")\n",
    "clinical_scorer = BERTScorer(\n",
    "    model_type=CLINICAL_BERT,\n",
    "    num_layers=9,\n",
    "    rescale_with_baseline=False,\n",
    "    lang=\"en\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(\"✓ BERTScorer initialized\")\n",
    "\n",
    "# Get the tokenizer from the scorer to do proper truncation\n",
    "bert_tokenizer = clinical_scorer._tokenizer\n",
    "\n",
    "\n",
    "def truncate_with_bert_tokenizer(text: str, tokenizer, max_length: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Properly truncate text using BERT's tokenizer to ensure it fits within token limit.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to truncate\n",
    "        tokenizer: BERT tokenizer\n",
    "        max_length: Maximum number of tokens (BERT supports 512, we use 500 for safety)\n",
    "\n",
    "    Returns:\n",
    "        Truncated text that will tokenize to <= max_length tokens\n",
    "    \"\"\"\n",
    "    # Tokenize and truncate\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    # Decode back to text\n",
    "    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return truncated_text\n",
    "\n",
    "\n",
    "# Truncate texts to fit BERT's 512 token limit\n",
    "print(\"\\nPreparing texts (truncating long sequences for BERT)...\")\n",
    "print(\"  Using BERT tokenizer for accurate truncation...\")\n",
    "\n",
    "truncated_predictions = []\n",
    "truncated_references = []\n",
    "\n",
    "for pred, ref in zip(predictions, references):\n",
    "    truncated_predictions.append(truncate_with_bert_tokenizer(pred, bert_tokenizer))\n",
    "    truncated_references.append(truncate_with_bert_tokenizer(ref, bert_tokenizer))\n",
    "\n",
    "# Check truncation statistics\n",
    "orig_pred_lens = [len(bert_tokenizer.encode(p)) for p in predictions]\n",
    "trunc_pred_lens = [len(bert_tokenizer.encode(p)) for p in truncated_predictions]\n",
    "num_truncated = sum(1 for o, t in zip(orig_pred_lens, trunc_pred_lens) if o != t)\n",
    "\n",
    "print(f\"  {num_truncated}/{len(predictions)} predictions were truncated\")\n",
    "print(f\"  Average prediction tokens: {np.mean(trunc_pred_lens):.0f}\")\n",
    "print(f\"  Max prediction tokens: {max(trunc_pred_lens)}\")\n",
    "\n",
    "print(\"\\nCalculating BERTScores (this may take a few minutes)...\\n\")\n",
    "\n",
    "# Compute scores with truncated texts\n",
    "P, R, F1 = clinical_scorer.score(\n",
    "    cands=truncated_predictions,\n",
    "    refs=truncated_references,\n",
    ")\n",
    "\n",
    "# Convert to numpy\n",
    "precision_scores = P.cpu().numpy()\n",
    "recall_scores = R.cpu().numpy()\n",
    "f1_scores = F1.cpu().numpy()\n",
    "\n",
    "# Compute averages\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLINICAL BERTSCORE RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAverage Precision: {avg_precision:.4f}\")\n",
    "print(f\"  → How much of the generated summary is clinically relevant\")\n",
    "\n",
    "print(f\"\\nAverage Recall: {avg_recall:.4f}\")\n",
    "print(f\"  → How much of the reference summary is captured\")\n",
    "print(f\"  → PRIMARY METRIC FOR HIGH RECALL\")\n",
    "\n",
    "print(f\"\\nAverage F1: {avg_f1:.4f}\")\n",
    "print(f\"  → Harmonic mean of precision and recall\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"\\nNote: Texts were truncated to 500 tokens using BERT's tokenizer.\")\n",
    "print(f\"This ensures all texts fit within BERT's 512 token limit.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPUTING CLINICAL BERTSCORE\n",
      "================================================================================\n",
      "\n",
      "Initializing BERTScorer with emilyalsentzer/Bio_ClinicalBERT...\n",
      "✓ BERTScorer initialized\n",
      "\n",
      "Preparing texts (truncating long sequences for BERT)...\n",
      "  Using BERT tokenizer for accurate truncation...\n",
      "  5/100 predictions were truncated\n",
      "  Average prediction tokens: 190\n",
      "  Max prediction tokens: 500\n",
      "\n",
      "Calculating BERTScores (this may take a few minutes)...\n",
      "\n",
      "================================================================================\n",
      "CLINICAL BERTSCORE RESULTS\n",
      "================================================================================\n",
      "\n",
      "Average Precision: 0.6667\n",
      "  → How much of the generated summary is clinically relevant\n",
      "\n",
      "Average Recall: 0.6523\n",
      "  → How much of the reference summary is captured\n",
      "  → PRIMARY METRIC FOR HIGH RECALL\n",
      "\n",
      "Average F1: 0.6580\n",
      "  → Harmonic mean of precision and recall\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Note: Texts were truncated to 500 tokens using BERT's tokenizer.\n",
      "This ensures all texts fit within BERT's 512 token limit.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Detailed Per-Sample Analysis\n",
    "\n",
    "Display scores for each test sample."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T09:32:10.539880Z",
     "start_time": "2025-12-04T09:32:10.534904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Per-Sample BERTScore Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(min(10, len(predictions))):  # Show first 10 samples\n",
    "    print(f\"\\nSample {i + 1}:\")\n",
    "    print(f\"  Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"  F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n... (showing first 10 of {len(predictions)} samples)\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Sample BERTScore Results:\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Precision: 0.6414\n",
      "  Recall: 0.6541\n",
      "  F1: 0.6477\n",
      "\n",
      "Sample 2:\n",
      "  Precision: 0.6985\n",
      "  Recall: 0.7179\n",
      "  F1: 0.7081\n",
      "\n",
      "Sample 3:\n",
      "  Precision: 0.6831\n",
      "  Recall: 0.7153\n",
      "  F1: 0.6988\n",
      "\n",
      "Sample 4:\n",
      "  Precision: 0.6201\n",
      "  Recall: 0.7212\n",
      "  F1: 0.6668\n",
      "\n",
      "Sample 5:\n",
      "  Precision: 0.6377\n",
      "  Recall: 0.6203\n",
      "  F1: 0.6289\n",
      "\n",
      "Sample 6:\n",
      "  Precision: 0.5569\n",
      "  Recall: 0.6630\n",
      "  F1: 0.6053\n",
      "\n",
      "Sample 7:\n",
      "  Precision: 0.6875\n",
      "  Recall: 0.6641\n",
      "  F1: 0.6756\n",
      "\n",
      "Sample 8:\n",
      "  Precision: 0.6338\n",
      "  Recall: 0.5935\n",
      "  F1: 0.6130\n",
      "\n",
      "Sample 9:\n",
      "  Precision: 0.7892\n",
      "  Recall: 0.8666\n",
      "  F1: 0.8261\n",
      "\n",
      "Sample 10:\n",
      "  Precision: 0.6899\n",
      "  Recall: 0.6923\n",
      "  F1: 0.6911\n",
      "\n",
      "... (showing first 10 of 100 samples)\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Qualitative Analysis\n",
    "\n",
    "Compare generated summaries with reference summaries for qualitative assessment."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T09:32:15.897411Z",
     "start_time": "2025-12-04T09:32:15.891153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"QUALITATIVE ANALYSIS: Generated vs Reference Summaries\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show 3 examples\n",
    "num_examples = min(3, len(predictions))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"EXAMPLE {i + 1}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "    print(\"INPUT (Clinical Notes - first 400 chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(test_dataset[i][\"input\"][:400] + \"...\\n\")\n",
    "\n",
    "    print(\"REFERENCE SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(references[i])\n",
    "    print()\n",
    "\n",
    "    print(\"GENERATED SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(predictions[i])\n",
    "    print()\n",
    "\n",
    "    print(\"SCORES:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"END OF QUALITATIVE ANALYSIS\")\n",
    "print(f\"{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUALITATIVE ANALYSIS: Generated vs Reference Summaries\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 1\n",
      "================================================================================\n",
      "\n",
      "INPUT (Clinical Notes - first 400 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "write discharge summary chief complaint sob cough history present illness past medical history stage nsclc dxd sp cyberknife treatment followed rad onc asthmacopd pfts mod vent defect red diff capactiy fev fevfvc osteoporosis hip fracture anemiathrombocytopenia h pylorus sp tx etoh pancreatitis neuropathyosteoporosis prostate ca prostatectomy biochemical rec urrence time surgery node neg extracaps...\n",
      "\n",
      "REFERENCE SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "copd stage lung ca sp cyberknife week progressive productive cough shortness breath copd exacerbation superimposed pnabronchitis infection likely precipitant patient also continues smoke unclear aspergillosis involved unclear significance regard filled bulla clear post obstructive process treated levoflox x day prednisone taper starting mg daily taper day blood cx ngtd galactomanin normal b glucan pending discharge close follow pulmonologist advair neb continued case mgt helped obtain nebulizer sp cyberknife radiologic regression tumor cont pulm rad onc follow\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "status post chemoradiation cyberknifepresenting worsening dyspnea exertion productive cough x day admission patient noted desaturate ambulating room air initially concerning pneumonia however cxr ct chest showed evidence infection started levofloxacin azithromycin presumed copdasthma exacerbations patient also found acute chronic renal failure likely prerenal etiology due poor po intake diuresed improvement renal function upon arrival floor patient felt much better reported mild abdominal discomfort nausea vomiting diarrhea constipation patient continued improve discharged home stable condition plan return followup appointment dr week repeat imaging prior visit instructed call office schedule appointment well continue taper oral prednisone dose advised take tylenol ibuprofen needed abdominal pain\n",
      "\n",
      "SCORES:\n",
      "--------------------------------------------------------------------------------\n",
      "Precision: 0.6414\n",
      "Recall: 0.6541\n",
      "F1: 0.6477\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 2\n",
      "================================================================================\n",
      "\n",
      "INPUT (Clinical Notes - first 400 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "create summary based following information chief complaint thrombocytopenia history present illness yo man seronegative ra dm htn present home lab testing revealed thrombocytopenia note past month easy bruising bleeding minor cut took long time stop additionally dental work done visit bleeding bruise cheek dentist concerned spoke physician recommended stop ibuprofen dr fax lab rec cbc coags done d...\n",
      "\n",
      "REFERENCE SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "yo man ho seronegative arthritis diabetes melitus hypertension present acute severe isolated thrombocytopenia thrombocytopenia unclear etiology likely subacute chronic given history worse bleedingbruising month given recent medication drug induced thrombocytopenia plaquenil nsaid celebrex lisinopril possible idiopathic thrombocytopneic purpura also possible active bleeding throughout hospitalization noted despite low platelet count evaluated hematology started steroid decadorn mg iv day one followed mgkgday prednisone platelet count rose hospital day hospital day given determined safe discharge dose prednisone daily urgent hematology follow additional workup hepatitis serology sent showed past infection hepatitis b hepatitis c hiv negative antiplatelet antibody pending time discharge ultrasound spleen obtained evaluate splenomegally spleen normal given elevated blood pressure initially concern spontaneous bleeding particularly intracranially monitored neuro check every hour neurologic change require transfusion platelet follow dr hematology management thrombocytopenia hypertension admission blood pressure poorly controlled normal given low platelet count urgency reduce blood pressure goal treated iv labetolol metoprolol achieve rapid reduction started oral labetolol mg three time daily achieved good control blood pressure transitioned oral metoprolol mg twice daily ease home dosing lisinopril continued due possible antiplatelet effect diabetes melitus glycemic control markedly worse high dose steroid expected requiring large dos sliding scale insulin first hour despite continuing oral agent metformin mg twice daily glyburide mg morning given hospital day started unit lantus insulin daily continued require large dos sliding scale insulin increased unit lantus daily improved control taught check blood sugar administer insulin discharged prescription glucometer lancet test strip syrinx lantus insulin humalog insulin also scheduled follow continue management diabetes teaching instructed check blood sugar time daily bring information appointment rheumatoid arthritis ra medication stoped due thrombocytopenia stiffness house likely due steroid need follow dr management ra steroid tapered anemia noted admission microcytic anemia without evidence hemolysis review smear minor iron deficiency likely reflects underlying inflammaiton repletion recomended hematology vitamin b folate replete renal failure recent baseline admission creatinine elevated possibly reflection lisinopril use urine checked showed sign blood protein time discharge creatinine improved suggesting chronic component well recommended followed endocrinologist pcp likley related diabetes communication patient wife\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "office wks sob ecchymosis epistaxis w thrombocytopeniathrombocytopenia patient presented week prior admission new onset bruising epistaxispetechiae found low plts pt reported recent use nsaid recently stopped taking plaquinal however never stopped celecoxib rheumatologist called recommend holding celecoixb setting possible flare thrombocytenipotential adverse effect steroid given risk side effect likely etiology could drug reaction vs lupus anticoagulant v underlying myelodysplastic syndrome pt initially started ivig dose subsequently transitioned oral prednisone taper discharged mg prednisone daily scheduled followup pcp hematology outpatient management diabetes type ii pt uncontrolled dm last several year complication continued home regimen metformin glybidide well regular lantushumalog sliding scaleshould adjust based glucose control hypertension held home med aceinhibitor held setting acute renal failure continued metoplprolol mg tid gerd continued omeprazole mg qd code full contact son admission preadmission medication list accurate complete lisonopri mg po every day celebres mg oral bid metformin xr mg mouth twice day multivit tab po day hydrochloroguinine mg tab cap po twice per day hydromorphone mg ml injection every hour needed pain oxycodoneacetaminophen mgmg ml solution qhr prn pain miconazole nitrate powder apply rash foot time day aspirin mg chewable tab oral daily simvastatin mg capsule oral qpm cholecalciferol u oral every week pantoprazole dose po qh cyanocobalamin mcgml liquid injection xweek docusate sodium mgml syrup teaspoon mouth three time per week please take meal senna mg tabsust relfess mouth daily glucosamine sulfchondroitin nf mgmcg oral day glucosaminemchondroitinfood supplement discharge home service home health aide\n",
      "\n",
      "SCORES:\n",
      "--------------------------------------------------------------------------------\n",
      "Precision: 0.6985\n",
      "Recall: 0.7179\n",
      "F1: 0.7081\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 3\n",
      "================================================================================\n",
      "\n",
      "INPUT (Clinical Notes - first 400 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "write discharge summary chief complaint brbpr history present illness m female hx seizure disorder crohn disease followed dr cva residual arm weakness present evaluation lower gi bleeding pt state known crohn disease followed dr ongoing lower gi bleeding past day pt note cup red blood per rectum xday x day x past one day pt note associated lower abdominal cramping note nauseavomitingfeverschillsch...\n",
      "\n",
      "REFERENCE SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "year old female history crohn disease come endoscopy suite evaluation lower gi bleeding brbpr acute blood loss anemia chrons disease pt present evaluation brbpr day duration sigmoidoscopy outpatient evaluate source bleeding showed internal hemorrhoid diverticulosis bleeding throughout colon without source identified outpatient gi doctor referred patient inpatient monitoring work colonoscopy arrival patient hds hgb started ppi evaluated inpatient gi team patient reported reluctance undergoing colonoscopy requested instead follow gi physician gi consult team felt reasonable given stable blood count one additional episode blood mixed stool bright blood tp patient felt smaller quantity comparison episode home patient transitioned oral pantoprazole started iron discharge given iron deficiency blood work source bleeding remained unclear likely chrons flare v internal hemorrhoid instructed call outpatient gi physician set follow appointment within next day counseled red flag return precaution chronic medical problem seizure disorder ch keppra htn home verapamil held cf gib discharged instruction restart medication given stable hgb bps\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "including epilepsy gerd crohns disease presented brx recurrent issue acuteactive problem brr patient presenting br bx several day concerning possible crohdn flare however patient also noted hematocrit drop since last colonoscopy likely source bleed could related recent episode diarrhea however setting known diverticulum would certainly consistent diverticular bleeding well given fact patient taking aspirin though unclear whether patient actually take medication time also may contributing bleeding although less likely given lack melena h pylorus antibody ordered pending discharge discharged close followup gi outpatient repeat egdcolonoscopy anemia hg baseline likely chronic normocytic anemia fe study suggestive iron deficiency iron supplementation continued inpatient due risk upper gib given aspirin use addition patient recently received dose iv ferric gluconate transfusion unit prbcs admission required total unit ffp u plts unit cryo platelet transfused postoperatively hematology consulted recommended starting protonix patient plan transition back omeprazole discharge htn continued home regimen dm dietcontrolled house transitional issue consider starting iron replacement therapy given elevated ferritin low tibc consider referral hematologist discus option outpatient gi follow arranged new pcp appointment made gastroenterology evaluate crohmans disease potential need colonscopy scheduled\n",
      "\n",
      "SCORES:\n",
      "--------------------------------------------------------------------------------\n",
      "Precision: 0.6831\n",
      "Recall: 0.7153\n",
      "F1: 0.6988\n",
      "\n",
      "================================================================================\n",
      "END OF QUALITATIVE ANALYSIS\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Save Evaluation Results\n",
    "\n",
    "Save predictions and scores to files for further analysis."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T09:32:32.162083Z",
     "start_time": "2025-12-04T09:32:32.138913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Create results directory\n",
    "results_dir = \"./evaluation_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Prepare results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'input': [sample['input'] for sample in test_dataset],\n",
    "    'reference': references,\n",
    "    'prediction': predictions,\n",
    "    'bertscore_precision': precision_scores,\n",
    "    'bertscore_recall': recall_scores,\n",
    "    'bertscore_f1': f1_scores\n",
    "})\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = os.path.join(results_dir, \"evaluation_results.csv\")\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Results saved to CSV: {csv_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"adapter_path\": ADAPTER_PATH,\n",
    "    \"num_test_samples\": len(predictions),\n",
    "    \"bertscore\": {\n",
    "        \"precision\": {\n",
    "            \"mean\": float(avg_precision),\n",
    "            \"std\": float(np.std(precision_scores)),\n",
    "            \"min\": float(np.min(precision_scores)),\n",
    "            \"max\": float(np.max(precision_scores))\n",
    "        },\n",
    "        \"recall\": {\n",
    "            \"mean\": float(avg_recall),\n",
    "            \"std\": float(np.std(recall_scores)),\n",
    "            \"min\": float(np.min(recall_scores)),\n",
    "            \"max\": float(np.max(recall_scores))\n",
    "        },\n",
    "        \"f1\": {\n",
    "            \"mean\": float(avg_f1),\n",
    "            \"std\": float(np.std(f1_scores)),\n",
    "            \"min\": float(np.min(f1_scores)),\n",
    "            \"max\": float(np.max(f1_scores))\n",
    "        }\n",
    "    },\n",
    "    \"generation_config\": {\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"top_k\": TOP_K,\n",
    "        \"repetition_penalty\": REPETITION_PENALTY\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(results_dir, \"summary_statistics.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"✓ Summary statistics saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"\\nAll results saved to: {results_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to CSV: ./evaluation_results\\evaluation_results.csv\n",
      "✓ Summary statistics saved: ./evaluation_results\\summary_statistics.json\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "All results saved to: ./evaluation_results\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Evaluation Checklist\n",
    "\n",
    "Use this checklist to assess the quality of generated summaries:\n",
    "\n",
    "**High Recall Checklist**:\n",
    "- ☐ Are all diagnoses mentioned?\n",
    "- ☐ Are all medications listed with dosages?\n",
    "- ☐ Are vital signs included?\n",
    "- ☐ Are abnormal lab results reported?\n",
    "- ☐ Are procedures and treatments described?\n",
    "- ☐ Are follow-up instructions present?\n",
    "- ☐ Is the timeline/hospital course clear?\n",
    "\n",
    "**Quality Assessment**:\n",
    "- Target Recall: ≥0.90 for production use\n",
    "- Target F1: ≥0.85 for balanced performance\n",
    "- Check for hallucinations (invented facts not in source)\n",
    "- Verify medical terminology accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
