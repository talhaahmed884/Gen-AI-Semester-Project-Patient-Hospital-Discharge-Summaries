{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma Fine-Tuned Model Evaluation\n",
    "\n",
    "**Purpose**: Evaluate the fine-tuned MedGemma-4B model using Clinical BERTScore and qualitative analysis.\n",
    "\n",
    "**Model**: Fine-tuned MedGemma-4B with LoRA adapters for clinical discharge summarization.\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Clinical BERTScore (Precision, Recall, F1)\n",
    "- Qualitative comparison of generated vs reference summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Imports\n",
    "\n",
    "Install required libraries for model loading and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:10:32.320027Z",
     "start_time": "2025-12-04T07:10:32.315630Z"
    }
   },
   "source": [
    "# Uncomment for Google Colab\n",
    "# !pip install -q -U transformers peft bitsandbytes accelerate bert_score scipy torch\n",
    "\n",
    "print(\"✓ Installation complete (or skipped for local environment)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Installation complete (or skipped for local environment)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:10:36.771568Z",
     "start_time": "2025-12-04T07:10:34.187751Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert_score import BERTScorer\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "GPU Memory: 8.55 GB\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set paths and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:10:51.323302Z",
     "start_time": "2025-12-04T07:10:51.316298Z"
    }
   },
   "source": [
    "# ============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Base model\n",
    "MODEL_NAME = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Path to fine-tuned LoRA adapters\n",
    "ADAPTER_PATH = \"./medgemma-discharge-summarization/final\"\n",
    "\n",
    "# Dataset path\n",
    "MIMIC_CSV_PATH = \"mimic_cleaned_text_only.csv\"\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "# Number of test samples to evaluate (set to -1 for all)\n",
    "NUM_TEST_SAMPLES = 100\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "REPETITION_PENALTY = 1.1\n",
    "\n",
    "# ============================================================================\n",
    "# BERTSCORE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Base model: {MODEL_NAME}\")\n",
    "print(f\"  Adapter path: {ADAPTER_PATH}\")\n",
    "print(f\"  Dataset: {MIMIC_CSV_PATH}\")\n",
    "print(f\"  Test samples: {NUM_TEST_SAMPLES if NUM_TEST_SAMPLES > 0 else 'All'}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Base model: google/medgemma-4b-it\n",
      "  Adapter path: ./medgemma-discharge-summarization/final\n",
      "  Dataset: mimic_cleaned_text_only.csv\n",
      "  Test samples: 100\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Dataset\n",
    "\n",
    "Load and prepare the MIMIC dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:11:00.713605Z",
     "start_time": "2025-12-04T07:10:54.086344Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "print(f\"Loading dataset from: {MIMIC_CSV_PATH}\\n\")\n",
    "\n",
    "if os.path.exists(MIMIC_CSV_PATH):\n",
    "    # Load the CSV file\n",
    "    mimic_df = pd.read_csv(MIMIC_CSV_PATH)\n",
    "\n",
    "    # Take subset for testing (first 10,000 samples)\n",
    "    mimic_df = mimic_df[:10_000]\n",
    "\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"  Total samples: {len(mimic_df)}\")\n",
    "    print(f\"  Columns: {list(mimic_df.columns)}\\n\")\n",
    "\n",
    "    # Add instruction column\n",
    "    instruction_text = \"Summarize the following clinical discharge notes. Include ALL diagnoses, medications, vitals, lab results, procedures, and follow-up instructions. Ensure complete coverage of all medical entities.\"\n",
    "    mimic_df['instruction'] = instruction_text\n",
    "\n",
    "    # Rename columns\n",
    "    mimic_df = mimic_df.rename(columns={\n",
    "        'final_input': 'input',\n",
    "        'final_target': 'output'\n",
    "    })\n",
    "\n",
    "    # Remove rows with missing data\n",
    "    mimic_df = mimic_df.dropna(subset=['input', 'output'])\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(mimic_df[['instruction', 'input', 'output']])\n",
    "\n",
    "    # Split into train and test sets (5% test)\n",
    "    dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "    test_dataset = dataset[\"test\"]\n",
    "\n",
    "    # Limit test samples if configured\n",
    "    if NUM_TEST_SAMPLES > 0 and NUM_TEST_SAMPLES < len(test_dataset):\n",
    "        test_dataset = test_dataset.select(range(NUM_TEST_SAMPLES))\n",
    "\n",
    "    print(f\"✓ Test dataset prepared!\")\n",
    "    print(f\"  Test samples: {len(test_dataset)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"⚠️  File not found: {MIMIC_CSV_PATH}\")\n",
    "    print(f\"Please ensure the dataset is in the project directory\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: mimic_cleaned_text_only.csv\n",
      "\n",
      "✓ Dataset loaded successfully!\n",
      "  Total samples: 10000\n",
      "  Columns: ['final_input', 'final_target']\n",
      "\n",
      "✓ Test dataset prepared!\n",
      "  Test samples: 100\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Fine-Tuned Model\n",
    "\n",
    "Load the base MedGemma model with 4-bit quantization, then load the fine-tuned LoRA adapters.\n",
    "\n",
    "**Memory Optimization**: Uses QLoRA (4-bit quantization) for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:11:19.439312Z",
     "start_time": "2025-12-04T07:11:19.435150Z"
    }
   },
   "source": [
    "# Enable synchronous CUDA for better error messages\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"✓ CUDA synchronous mode enabled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CUDA synchronous mode enabled\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T07:11:56.980327Z",
     "start_time": "2025-12-04T07:11:46.612681Z"
    }
   },
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOADING FINE-TUNED MEDGEMMA MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "compute_dtype = torch.bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "print(\"\\nStep 1: Loading base model...\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Quantization: 4-bit NF4\")\n",
    "print(f\"  This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "print(\"✓ Base model loaded\")\n",
    "\n",
    "# Load LoRA adapters\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    print(f\"\\nStep 2: Loading LoRA adapters...\")\n",
    "    print(f\"  Path: {ADAPTER_PATH}\")\n",
    "    model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "    print(\"✓ LoRA adapters loaded\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  WARNING: Adapter path not found: {ADAPTER_PATH}\")\n",
    "    print(\"   Using base model only (not fine-tuned)\")\n",
    "\n",
    "# Get actual vocab size from model\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "actual_vocab_size = embedding_layer.weight.shape[0]\n",
    "print(f\"\\n  Model embedding vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nStep 3: Loading tokenizer...\")\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            ADAPTER_PATH,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "        print(\"✓ Tokenizer loaded from adapter path\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Adapter tokenizer failed: {e}\")\n",
    "        print(\"   Loading base model tokenizer instead\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\",\n",
    "        add_eos_token=True\n",
    "    )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"\\n  Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"  EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Validation check\n",
    "if len(tokenizer) != actual_vocab_size:\n",
    "    print(f\"\\n⚠️  MISMATCH DETECTED!\")\n",
    "    print(f\"   Tokenizer vocab: {len(tokenizer)}\")\n",
    "    print(f\"   Model vocab: {actual_vocab_size}\")\n",
    "\n",
    "    if len(tokenizer) > actual_vocab_size:\n",
    "        print(f\"\\n   Resizing model embeddings to {len(tokenizer)}...\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        actual_vocab_size = model.get_input_embeddings().weight.shape[0]\n",
    "        print(f\"   ✓ New model vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Validation test\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"VALIDATION TEST\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "test_text = \"Patient presented with chest pain.\"\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "max_id = test_tokens['input_ids'].max().item()\n",
    "\n",
    "print(f\"Test text: '{test_text}'\")\n",
    "print(f\"Max token ID: {max_id}\")\n",
    "print(f\"Valid range: [0, {actual_vocab_size - 1}]\")\n",
    "\n",
    "if max_id >= actual_vocab_size:\n",
    "    print(f\"\\n❌ CRITICAL ERROR: Token ID out of range!\")\n",
    "    raise ValueError(f\"Token ID {max_id} >= vocab size {actual_vocab_size}\")\n",
    "else:\n",
    "    print(f\"\\n✅ VALIDATION PASSED!\")\n",
    "    print(f\"   All token IDs are within valid range\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"\\n✓ Model ready for evaluation\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING FINE-TUNED MEDGEMMA MODEL\n",
      "================================================================================\n",
      "\n",
      "Step 1: Loading base model...\n",
      "  Model: google/medgemma-4b-it\n",
      "  Quantization: 4-bit NF4\n",
      "  This may take 2-3 minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6880c3346fc94771b110a0e65467a878"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Base model loaded\n",
      "\n",
      "Step 2: Loading LoRA adapters...\n",
      "  Path: ./medgemma-discharge-summarization/final\n",
      "✓ LoRA adapters loaded\n",
      "\n",
      "  Model embedding vocab size: 262208\n",
      "\n",
      "Step 3: Loading tokenizer...\n",
      "✓ Tokenizer loaded from adapter path\n",
      "\n",
      "  Tokenizer vocab size: 262145\n",
      "  PAD token ID: 1\n",
      "  EOS token ID: 1\n",
      "\n",
      "⚠️  MISMATCH DETECTED!\n",
      "   Tokenizer vocab: 262145\n",
      "   Model vocab: 262208\n",
      "\n",
      "================================================================================\n",
      "VALIDATION TEST\n",
      "================================================================================\n",
      "Test text: 'Patient presented with chest pain.'\n",
      "Max token ID: 236761\n",
      "Valid range: [0, 262207]\n",
      "\n",
      "✅ VALIDATION PASSED!\n",
      "   All token IDs are within valid range\n",
      "\n",
      "✓ Model ready for evaluation\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Predictions on Test Set\n",
    "\n",
    "Generate clinical summaries for all test samples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T08:36:49.714938Z",
     "start_time": "2025-12-04T07:13:30.357070Z"
    }
   },
   "source": [
    "print(\"Generating predictions on test set...\\n\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    print(f\"Generating summary {i + 1}/{len(test_dataset)}...\", end=\" \")\n",
    "\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample[\"input\"]\n",
    "    reference = sample[\"output\"]\n",
    "\n",
    "    # Format the prompt (without the model's response)\n",
    "    inference_prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the model's response\n",
    "    model_response_marker = \"<start_of_turn>model\"\n",
    "    if model_response_marker in generated_text:\n",
    "        generated_summary = generated_text.split(model_response_marker)[-1].strip()\n",
    "    else:\n",
    "        generated_summary = generated_text[len(inference_prompt):].strip()\n",
    "\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(reference)\n",
    "\n",
    "    print(f\"✓ ({len(generated_summary)} chars)\")\n",
    "\n",
    "print(f\"\\n✓ All predictions generated\")\n",
    "print(f\"  Total predictions: {len(predictions)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test set...\n",
      "\n",
      "Generating summary 1/100... ✓ (807 chars)\n",
      "Generating summary 2/100... ✓ (1775 chars)\n",
      "Generating summary 3/100... ✓ (1409 chars)\n",
      "Generating summary 4/100... ✓ (530 chars)\n",
      "Generating summary 5/100... ✓ (800 chars)\n",
      "Generating summary 6/100... ✓ (1470 chars)\n",
      "Generating summary 7/100... ✓ (1152 chars)\n",
      "Generating summary 8/100... ✓ (348 chars)\n",
      "Generating summary 9/100... ✓ (367 chars)\n",
      "Generating summary 10/100... ✓ (1734 chars)\n",
      "Generating summary 11/100... ✓ (1236 chars)\n",
      "Generating summary 12/100... ✓ (2064 chars)\n",
      "Generating summary 13/100... ✓ (1194 chars)\n",
      "Generating summary 14/100... ✓ (322 chars)\n",
      "Generating summary 15/100... ✓ (670 chars)\n",
      "Generating summary 16/100... ✓ (373 chars)\n",
      "Generating summary 17/100... ✓ (3233 chars)\n",
      "Generating summary 18/100... ✓ (1026 chars)\n",
      "Generating summary 19/100... ✓ (1470 chars)\n",
      "Generating summary 20/100... ✓ (205 chars)\n",
      "Generating summary 21/100... ✓ (58 chars)\n",
      "Generating summary 22/100... ✓ (249 chars)\n",
      "Generating summary 23/100... ✓ (500 chars)\n",
      "Generating summary 24/100... ✓ (1902 chars)\n",
      "Generating summary 25/100... ✓ (376 chars)\n",
      "Generating summary 26/100... ✓ (33 chars)\n",
      "Generating summary 27/100... ✓ (330 chars)\n",
      "Generating summary 28/100... ✓ (385 chars)\n",
      "Generating summary 29/100... ✓ (929 chars)\n",
      "Generating summary 30/100... ✓ (222 chars)\n",
      "Generating summary 31/100... ✓ (602 chars)\n",
      "Generating summary 32/100... ✓ (995 chars)\n",
      "Generating summary 33/100... ✓ (285 chars)\n",
      "Generating summary 34/100... ✓ (246 chars)\n",
      "Generating summary 35/100... ✓ (399 chars)\n",
      "Generating summary 36/100... ✓ (568 chars)\n",
      "Generating summary 37/100... ✓ (2256 chars)\n",
      "Generating summary 38/100... ✓ (249 chars)\n",
      "Generating summary 39/100... ✓ (821 chars)\n",
      "Generating summary 40/100... ✓ (1509 chars)\n",
      "Generating summary 41/100... ✓ (1560 chars)\n",
      "Generating summary 42/100... ✓ (460 chars)\n",
      "Generating summary 43/100... ✓ (184 chars)\n",
      "Generating summary 44/100... ✓ (490 chars)\n",
      "Generating summary 45/100... ✓ (526 chars)\n",
      "Generating summary 46/100... ✓ (276 chars)\n",
      "Generating summary 47/100... ✓ (442 chars)\n",
      "Generating summary 48/100... ✓ (1145 chars)\n",
      "Generating summary 49/100... ✓ (240 chars)\n",
      "Generating summary 50/100... ✓ (1946 chars)\n",
      "Generating summary 51/100... ✓ (87 chars)\n",
      "Generating summary 52/100... ✓ (474 chars)\n",
      "Generating summary 53/100... ✓ (1297 chars)\n",
      "Generating summary 54/100... ✓ (532 chars)\n",
      "Generating summary 55/100... ✓ (434 chars)\n",
      "Generating summary 56/100... ✓ (2102 chars)\n",
      "Generating summary 57/100... ✓ (2130 chars)\n",
      "Generating summary 58/100... ✓ (1003 chars)\n",
      "Generating summary 59/100... ✓ (952 chars)\n",
      "Generating summary 60/100... ✓ (3416 chars)\n",
      "Generating summary 61/100... ✓ (3482 chars)\n",
      "Generating summary 62/100... ✓ (223 chars)\n",
      "Generating summary 63/100... ✓ (566 chars)\n",
      "Generating summary 64/100... ✓ (779 chars)\n",
      "Generating summary 65/100... ✓ (2363 chars)\n",
      "Generating summary 66/100... ✓ (568 chars)\n",
      "Generating summary 67/100... ✓ (128 chars)\n",
      "Generating summary 68/100... ✓ (1136 chars)\n",
      "Generating summary 69/100... ✓ (1188 chars)\n",
      "Generating summary 70/100... ✓ (830 chars)\n",
      "Generating summary 71/100... ✓ (507 chars)\n",
      "Generating summary 72/100... ✓ (776 chars)\n",
      "Generating summary 73/100... ✓ (729 chars)\n",
      "Generating summary 74/100... ✓ (2352 chars)\n",
      "Generating summary 75/100... ✓ (576 chars)\n",
      "Generating summary 76/100... ✓ (502 chars)\n",
      "Generating summary 77/100... ✓ (1100 chars)\n",
      "Generating summary 78/100... ✓ (83 chars)\n",
      "Generating summary 79/100... ✓ (564 chars)\n",
      "Generating summary 80/100... ✓ (671 chars)\n",
      "Generating summary 81/100... ✓ (1127 chars)\n",
      "Generating summary 82/100... ✓ (1487 chars)\n",
      "Generating summary 83/100... ✓ (568 chars)\n",
      "Generating summary 84/100... ✓ (846 chars)\n",
      "Generating summary 85/100... ✓ (3073 chars)\n",
      "Generating summary 86/100... ✓ (1100 chars)\n",
      "Generating summary 87/100... ✓ (535 chars)\n",
      "Generating summary 88/100... ✓ (833 chars)\n",
      "Generating summary 89/100... ✓ (613 chars)\n",
      "Generating summary 90/100... ✓ (716 chars)\n",
      "Generating summary 91/100... ✓ (988 chars)\n",
      "Generating summary 92/100... ✓ (2068 chars)\n",
      "Generating summary 93/100... ✓ (1913 chars)\n",
      "Generating summary 94/100... ✓ (273 chars)\n",
      "Generating summary 95/100... ✓ (1011 chars)\n",
      "Generating summary 96/100... ✓ (794 chars)\n",
      "Generating summary 97/100... ✓ (614 chars)\n",
      "Generating summary 98/100... ✓ (378 chars)\n",
      "Generating summary 99/100... ✓ (111 chars)\n",
      "Generating summary 100/100... ✓ (1106 chars)\n",
      "\n",
      "✓ All predictions generated\n",
      "  Total predictions: 100\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Clinical BERTScore\n",
    "\n",
    "Evaluate semantic similarity using Bio_ClinicalBERT."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T08:52:01.079098Z",
     "start_time": "2025-12-04T08:51:57.730119Z"
    }
   },
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPUTING CLINICAL BERTSCORE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nInitializing BERTScorer with {CLINICAL_BERT}...\")\n",
    "clinical_scorer = BERTScorer(\n",
    "    model_type=CLINICAL_BERT,\n",
    "    num_layers=9,\n",
    "    rescale_with_baseline=True,\n",
    "    lang=\"en\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(\"✓ BERTScorer initialized\")\n",
    "\n",
    "# Get the tokenizer from the scorer to do proper truncation\n",
    "bert_tokenizer = clinical_scorer._tokenizer\n",
    "\n",
    "\n",
    "def truncate_with_bert_tokenizer(text: str, tokenizer, max_length: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Properly truncate text using BERT's tokenizer to ensure it fits within token limit.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to truncate\n",
    "        tokenizer: BERT tokenizer\n",
    "        max_length: Maximum number of tokens (BERT supports 512, we use 500 for safety)\n",
    "    \n",
    "    Returns:\n",
    "        Truncated text that will tokenize to <= max_length tokens\n",
    "    \"\"\"\n",
    "    # Tokenize and truncate\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    # Decode back to text\n",
    "    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return truncated_text\n",
    "\n",
    "\n",
    "# Truncate texts to fit BERT's 512 token limit\n",
    "print(\"\\nPreparing texts (truncating long sequences for BERT)...\")\n",
    "print(\"  Using BERT tokenizer for accurate truncation...\")\n",
    "\n",
    "truncated_predictions = []\n",
    "truncated_references = []\n",
    "\n",
    "for pred, ref in zip(predictions, references):\n",
    "    truncated_predictions.append(truncate_with_bert_tokenizer(pred, bert_tokenizer))\n",
    "    truncated_references.append(truncate_with_bert_tokenizer(ref, bert_tokenizer))\n",
    "\n",
    "# Check truncation statistics\n",
    "orig_pred_lens = [len(bert_tokenizer.encode(p)) for p in predictions]\n",
    "trunc_pred_lens = [len(bert_tokenizer.encode(p)) for p in truncated_predictions]\n",
    "num_truncated = sum(1 for o, t in zip(orig_pred_lens, trunc_pred_lens) if o != t)\n",
    "\n",
    "print(f\"  {num_truncated}/{len(predictions)} predictions were truncated\")\n",
    "print(f\"  Average prediction tokens: {np.mean(trunc_pred_lens):.0f}\")\n",
    "print(f\"  Max prediction tokens: {max(trunc_pred_lens)}\")\n",
    "\n",
    "print(\"\\nCalculating BERTScores (this may take a few minutes)...\\n\")\n",
    "\n",
    "# Compute scores with truncated texts\n",
    "P, R, F1 = clinical_scorer.score(\n",
    "    cands=truncated_predictions,\n",
    "    refs=truncated_references,\n",
    ")\n",
    "\n",
    "# Convert to numpy\n",
    "precision_scores = P.cpu().numpy()\n",
    "recall_scores = R.cpu().numpy()\n",
    "f1_scores = F1.cpu().numpy()\n",
    "\n",
    "# Compute averages\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLINICAL BERTSCORE RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAverage Precision: {avg_precision:.4f}\")\n",
    "print(f\"  → How much of the generated summary is clinically relevant\")\n",
    "\n",
    "print(f\"\\nAverage Recall: {avg_recall:.4f}\")\n",
    "print(f\"  → How much of the reference summary is captured\")\n",
    "print(f\"  → PRIMARY METRIC FOR HIGH RECALL\")\n",
    "\n",
    "print(f\"\\nAverage F1: {avg_f1:.4f}\")\n",
    "print(f\"  → Harmonic mean of precision and recall\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"\\nNote: Texts were truncated to 500 tokens using BERT's tokenizer.\")\n",
    "print(f\"This ensures all texts fit within BERT's 512 token limit.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Per-Sample Analysis\n",
    "\n",
    "Display scores for each test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per-Sample BERTScore Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(min(10, len(predictions))):  # Show first 10 samples\n",
    "    print(f\"\\nSample {i + 1}:\")\n",
    "    print(f\"  Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"  F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n... (showing first 10 of {len(predictions)} samples)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Qualitative Analysis\n",
    "\n",
    "Compare generated summaries with reference summaries for qualitative assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"QUALITATIVE ANALYSIS: Generated vs Reference Summaries\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show 3 examples\n",
    "num_examples = min(3, len(predictions))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"EXAMPLE {i + 1}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "    print(\"INPUT (Clinical Notes - first 400 chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(test_dataset[i][\"input\"][:400] + \"...\\n\")\n",
    "\n",
    "    print(\"REFERENCE SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(references[i])\n",
    "    print()\n",
    "\n",
    "    print(\"GENERATED SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(predictions[i])\n",
    "    print()\n",
    "\n",
    "    print(\"SCORES:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"END OF QUALITATIVE ANALYSIS\")\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Evaluation Results\n",
    "\n",
    "Save predictions and scores to files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create results directory\n",
    "results_dir = \"./evaluation_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Prepare results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'input': [sample['input'] for sample in test_dataset],\n",
    "    'reference': references,\n",
    "    'prediction': predictions,\n",
    "    'bertscore_precision': precision_scores,\n",
    "    'bertscore_recall': recall_scores,\n",
    "    'bertscore_f1': f1_scores\n",
    "})\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = os.path.join(results_dir, \"evaluation_results.csv\")\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Results saved to CSV: {csv_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"adapter_path\": ADAPTER_PATH,\n",
    "    \"num_test_samples\": len(predictions),\n",
    "    \"bertscore\": {\n",
    "        \"precision\": {\n",
    "            \"mean\": float(avg_precision),\n",
    "            \"std\": float(np.std(precision_scores)),\n",
    "            \"min\": float(np.min(precision_scores)),\n",
    "            \"max\": float(np.max(precision_scores))\n",
    "        },\n",
    "        \"recall\": {\n",
    "            \"mean\": float(avg_recall),\n",
    "            \"std\": float(np.std(recall_scores)),\n",
    "            \"min\": float(np.min(recall_scores)),\n",
    "            \"max\": float(np.max(recall_scores))\n",
    "        },\n",
    "        \"f1\": {\n",
    "            \"mean\": float(avg_f1),\n",
    "            \"std\": float(np.std(f1_scores)),\n",
    "            \"min\": float(np.min(f1_scores)),\n",
    "            \"max\": float(np.max(f1_scores))\n",
    "        }\n",
    "    },\n",
    "    \"generation_config\": {\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"top_k\": TOP_K,\n",
    "        \"repetition_penalty\": REPETITION_PENALTY\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(results_dir, \"summary_statistics.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"✓ Summary statistics saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"\\nAll results saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Checklist\n",
    "\n",
    "Use this checklist to assess the quality of generated summaries:\n",
    "\n",
    "**High Recall Checklist**:\n",
    "- ☐ Are all diagnoses mentioned?\n",
    "- ☐ Are all medications listed with dosages?\n",
    "- ☐ Are vital signs included?\n",
    "- ☐ Are abnormal lab results reported?\n",
    "- ☐ Are procedures and treatments described?\n",
    "- ☐ Are follow-up instructions present?\n",
    "- ☐ Is the timeline/hospital course clear?\n",
    "\n",
    "**Quality Assessment**:\n",
    "- Target Recall: ≥0.90 for production use\n",
    "- Target F1: ≥0.85 for balanced performance\n",
    "- Check for hallucinations (invented facts not in source)\n",
    "- Verify medical terminology accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
