{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma Fine-Tuned Model Evaluation\n",
    "\n",
    "**Purpose**: Evaluate the fine-tuned MedGemma-4B model using Clinical BERTScore and qualitative analysis.\n",
    "\n",
    "**Model**: Fine-tuned MedGemma-4B with LoRA adapters for clinical discharge summarization.\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Clinical BERTScore (Precision, Recall, F1)\n",
    "- Qualitative comparison of generated vs reference summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Imports\n",
    "\n",
    "Install required libraries for model loading and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for Google Colab\n",
    "# !pip install -q -U transformers peft bitsandbytes accelerate bert_score scipy torch\n",
    "\n",
    "print(\"✓ Installation complete (or skipped for local environment)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert_score import BERTScorer\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set paths and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Base model\n",
    "MODEL_NAME = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Path to fine-tuned LoRA adapters\n",
    "ADAPTER_PATH = \"./medgemma-discharge-summarization/final\"\n",
    "\n",
    "# Dataset path\n",
    "MIMIC_CSV_PATH = \"mimic_cleaned_text_only.csv\"\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "# Number of test samples to evaluate (set to -1 for all)\n",
    "NUM_TEST_SAMPLES = 50\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "REPETITION_PENALTY = 1.1\n",
    "\n",
    "# ============================================================================\n",
    "# BERTSCORE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Base model: {MODEL_NAME}\")\n",
    "print(f\"  Adapter path: {ADAPTER_PATH}\")\n",
    "print(f\"  Dataset: {MIMIC_CSV_PATH}\")\n",
    "print(f\"  Test samples: {NUM_TEST_SAMPLES if NUM_TEST_SAMPLES > 0 else 'All'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Dataset\n",
    "\n",
    "Load and prepare the MIMIC dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Loading dataset from: {MIMIC_CSV_PATH}\\n\")\n",
    "\n",
    "if os.path.exists(MIMIC_CSV_PATH):\n",
    "    # Load the CSV file\n",
    "    mimic_df = pd.read_csv(MIMIC_CSV_PATH)\n",
    "\n",
    "    # Take subset for testing (first 10,000 samples)\n",
    "    mimic_df = mimic_df[:10_000]\n",
    "\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"  Total samples: {len(mimic_df)}\")\n",
    "    print(f\"  Columns: {list(mimic_df.columns)}\\n\")\n",
    "\n",
    "    # Add instruction column\n",
    "    instruction_text = \"Summarize the following clinical discharge notes. Include ALL diagnoses, medications, vitals, lab results, procedures, and follow-up instructions. Ensure complete coverage of all medical entities.\"\n",
    "    mimic_df['instruction'] = instruction_text\n",
    "\n",
    "    # Rename columns\n",
    "    mimic_df = mimic_df.rename(columns={\n",
    "        'final_input': 'input',\n",
    "        'final_target': 'output'\n",
    "    })\n",
    "\n",
    "    # Remove rows with missing data\n",
    "    mimic_df = mimic_df.dropna(subset=['input', 'output'])\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(mimic_df[['instruction', 'input', 'output']])\n",
    "\n",
    "    # Split into train and test sets (5% test)\n",
    "    dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "    test_dataset = dataset[\"test\"]\n",
    "\n",
    "    # Limit test samples if configured\n",
    "    if NUM_TEST_SAMPLES > 0 and NUM_TEST_SAMPLES < len(test_dataset):\n",
    "        test_dataset = test_dataset.select(range(NUM_TEST_SAMPLES))\n",
    "\n",
    "    print(f\"✓ Test dataset prepared!\")\n",
    "    print(f\"  Test samples: {len(test_dataset)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"⚠️  File not found: {MIMIC_CSV_PATH}\")\n",
    "    print(f\"Please ensure the dataset is in the project directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Fine-Tuned Model\n",
    "\n",
    "Load the base MedGemma model with 4-bit quantization, then load the fine-tuned LoRA adapters.\n",
    "\n",
    "**Memory Optimization**: Uses QLoRA (4-bit quantization) for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable synchronous CUDA for better error messages\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"✓ CUDA synchronous mode enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOADING FINE-TUNED MEDGEMMA MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "compute_dtype = torch.float16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "print(\"\\nStep 1: Loading base model...\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Quantization: 4-bit NF4\")\n",
    "print(f\"  This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "print(\"✓ Base model loaded\")\n",
    "\n",
    "# Load LoRA adapters\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    print(f\"\\nStep 2: Loading LoRA adapters...\")\n",
    "    print(f\"  Path: {ADAPTER_PATH}\")\n",
    "    model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "    print(\"✓ LoRA adapters loaded\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  WARNING: Adapter path not found: {ADAPTER_PATH}\")\n",
    "    print(\"   Using base model only (not fine-tuned)\")\n",
    "\n",
    "# Get actual vocab size from model\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "actual_vocab_size = embedding_layer.weight.shape[0]\n",
    "print(f\"\\n  Model embedding vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nStep 3: Loading tokenizer...\")\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            ADAPTER_PATH,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "        print(\"✓ Tokenizer loaded from adapter path\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Adapter tokenizer failed: {e}\")\n",
    "        print(\"   Loading base model tokenizer instead\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\",\n",
    "        add_eos_token=True\n",
    "    )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"\\n  Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"  EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Validation check\n",
    "if len(tokenizer) != actual_vocab_size:\n",
    "    print(f\"\\n⚠️  MISMATCH DETECTED!\")\n",
    "    print(f\"   Tokenizer vocab: {len(tokenizer)}\")\n",
    "    print(f\"   Model vocab: {actual_vocab_size}\")\n",
    "\n",
    "    if len(tokenizer) > actual_vocab_size:\n",
    "        print(f\"\\n   Resizing model embeddings to {len(tokenizer)}...\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        actual_vocab_size = model.get_input_embeddings().weight.shape[0]\n",
    "        print(f\"   ✓ New model vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Validation test\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"VALIDATION TEST\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "test_text = \"Patient presented with chest pain.\"\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "max_id = test_tokens['input_ids'].max().item()\n",
    "\n",
    "print(f\"Test text: '{test_text}'\")\n",
    "print(f\"Max token ID: {max_id}\")\n",
    "print(f\"Valid range: [0, {actual_vocab_size - 1}]\")\n",
    "\n",
    "if max_id >= actual_vocab_size:\n",
    "    print(f\"\\n❌ CRITICAL ERROR: Token ID out of range!\")\n",
    "    raise ValueError(f\"Token ID {max_id} >= vocab size {actual_vocab_size}\")\n",
    "else:\n",
    "    print(f\"\\n✅ VALIDATION PASSED!\")\n",
    "    print(f\"   All token IDs are within valid range\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"\\n✓ Model ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Predictions on Test Set\n",
    "\n",
    "Generate clinical summaries for all test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating predictions on test set...\\n\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    print(f\"Generating summary {i + 1}/{len(test_dataset)}...\", end=\" \")\n",
    "\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample[\"input\"]\n",
    "    reference = sample[\"output\"]\n",
    "\n",
    "    # Format the prompt (without the model's response)\n",
    "    inference_prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the model's response\n",
    "    model_response_marker = \"<start_of_turn>model\"\n",
    "    if model_response_marker in generated_text:\n",
    "        generated_summary = generated_text.split(model_response_marker)[-1].strip()\n",
    "    else:\n",
    "        generated_summary = generated_text[len(inference_prompt):].strip()\n",
    "\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(reference)\n",
    "\n",
    "    print(f\"✓ ({len(generated_summary)} chars)\")\n",
    "\n",
    "print(f\"\\n✓ All predictions generated\")\n",
    "print(f\"  Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Clinical BERTScore\n",
    "\n",
    "Evaluate semantic similarity using Bio_ClinicalBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPUTING CLINICAL BERTSCORE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nInitializing BERTScorer with {CLINICAL_BERT}...\")\n",
    "clinical_scorer = BERTScorer(\n",
    "    model_type=CLINICAL_BERT,\n",
    "    num_layers=9,\n",
    "    rescale_with_baseline=True,\n",
    "    lang=\"en\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(\"✓ BERTScorer initialized\")\n",
    "\n",
    "print(\"\\nCalculating BERTScores...\\n\")\n",
    "\n",
    "# Compute scores\n",
    "P, R, F1 = clinical_scorer.score(\n",
    "    cands=predictions,\n",
    "    refs=references,\n",
    ")\n",
    "\n",
    "# Convert to numpy\n",
    "precision_scores = P.cpu().numpy()\n",
    "recall_scores = R.cpu().numpy()\n",
    "f1_scores = F1.cpu().numpy()\n",
    "\n",
    "# Compute averages\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLINICAL BERTSCORE RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAverage Precision: {avg_precision:.4f}\")\n",
    "print(f\"  → How much of the generated summary is clinically relevant\")\n",
    "\n",
    "print(f\"\\nAverage Recall: {avg_recall:.4f}\")\n",
    "print(f\"  → How much of the reference summary is captured\")\n",
    "print(f\"  → PRIMARY METRIC FOR HIGH RECALL\")\n",
    "\n",
    "print(f\"\\nAverage F1: {avg_f1:.4f}\")\n",
    "print(f\"  → Harmonic mean of precision and recall\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Per-Sample Analysis\n",
    "\n",
    "Display scores for each test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per-Sample BERTScore Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(min(10, len(predictions))):  # Show first 10 samples\n",
    "    print(f\"\\nSample {i + 1}:\")\n",
    "    print(f\"  Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"  F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n... (showing first 10 of {len(predictions)} samples)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Qualitative Analysis\n",
    "\n",
    "Compare generated summaries with reference summaries for qualitative assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"QUALITATIVE ANALYSIS: Generated vs Reference Summaries\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show 3 examples\n",
    "num_examples = min(3, len(predictions))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"EXAMPLE {i + 1}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "    print(\"INPUT (Clinical Notes - first 400 chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(test_dataset[i][\"input\"][:400] + \"...\\n\")\n",
    "\n",
    "    print(\"REFERENCE SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(references[i])\n",
    "    print()\n",
    "\n",
    "    print(\"GENERATED SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(predictions[i])\n",
    "    print()\n",
    "\n",
    "    print(\"SCORES:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"END OF QUALITATIVE ANALYSIS\")\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Evaluation Results\n",
    "\n",
    "Save predictions and scores to files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create results directory\n",
    "results_dir = \"./evaluation_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Prepare results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'input': [sample['input'] for sample in test_dataset],\n",
    "    'reference': references,\n",
    "    'prediction': predictions,\n",
    "    'bertscore_precision': precision_scores,\n",
    "    'bertscore_recall': recall_scores,\n",
    "    'bertscore_f1': f1_scores\n",
    "})\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = os.path.join(results_dir, \"evaluation_results.csv\")\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Results saved to CSV: {csv_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"adapter_path\": ADAPTER_PATH,\n",
    "    \"num_test_samples\": len(predictions),\n",
    "    \"bertscore\": {\n",
    "        \"precision\": {\n",
    "            \"mean\": float(avg_precision),\n",
    "            \"std\": float(np.std(precision_scores)),\n",
    "            \"min\": float(np.min(precision_scores)),\n",
    "            \"max\": float(np.max(precision_scores))\n",
    "        },\n",
    "        \"recall\": {\n",
    "            \"mean\": float(avg_recall),\n",
    "            \"std\": float(np.std(recall_scores)),\n",
    "            \"min\": float(np.min(recall_scores)),\n",
    "            \"max\": float(np.max(recall_scores))\n",
    "        },\n",
    "        \"f1\": {\n",
    "            \"mean\": float(avg_f1),\n",
    "            \"std\": float(np.std(f1_scores)),\n",
    "            \"min\": float(np.min(f1_scores)),\n",
    "            \"max\": float(np.max(f1_scores))\n",
    "        }\n",
    "    },\n",
    "    \"generation_config\": {\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"top_k\": TOP_K,\n",
    "        \"repetition_penalty\": REPETITION_PENALTY\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(results_dir, \"summary_statistics.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"✓ Summary statistics saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"\\nAll results saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Checklist\n",
    "\n",
    "Use this checklist to assess the quality of generated summaries:\n",
    "\n",
    "**High Recall Checklist**:\n",
    "- ☐ Are all diagnoses mentioned?\n",
    "- ☐ Are all medications listed with dosages?\n",
    "- ☐ Are vital signs included?\n",
    "- ☐ Are abnormal lab results reported?\n",
    "- ☐ Are procedures and treatments described?\n",
    "- ☐ Are follow-up instructions present?\n",
    "- ☐ Is the timeline/hospital course clear?\n",
    "\n",
    "**Quality Assessment**:\n",
    "- Target Recall: ≥0.90 for production use\n",
    "- Target F1: ≥0.85 for balanced performance\n",
    "- Check for hallucinations (invented facts not in source)\n",
    "- Verify medical terminology accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
