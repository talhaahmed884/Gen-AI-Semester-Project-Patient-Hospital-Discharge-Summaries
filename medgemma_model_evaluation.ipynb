{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MedGemma Fine-Tuned Model Evaluation\n",
    "\n",
    "**Purpose**: Evaluate the fine-tuned MedGemma-4B model using Clinical BERTScore and qualitative analysis.\n",
    "\n",
    "**Model**: Fine-tuned MedGemma-4B with LoRA adapters for clinical discharge summarization.\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Clinical BERTScore (Precision, Recall, F1)\n",
    "- Qualitative comparison of generated vs reference summaries"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Installation and Imports\n",
    "\n",
    "Install required libraries for model loading and evaluation."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T05:53:35.044273700Z",
     "start_time": "2025-12-11T05:53:33.511774100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Uncomment for Google Colab\n",
    "!pip install -q -U transformers peft bitsandbytes accelerate bert_score scipy torch\n",
    "\n",
    "print(\"✓ Installation complete (or skipped for local environment)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Installation complete (or skipped for local environment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T05:54:08.868367300Z",
     "start_time": "2025-12-11T05:53:35.048127700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert_score import BERTScorer\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu130\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "GPU Memory: 8.55 GB\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set paths and model parameters."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T05:54:08.958402200Z",
     "start_time": "2025-12-11T05:54:08.935426700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Base model\n",
    "MODEL_NAME = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Path to fine-tuned LoRA adapters\n",
    "ADAPTER_PATH = \"./medgemma-discharge-summarization/final\"\n",
    "\n",
    "# Dataset path\n",
    "MIMIC_CSV_PATH = \"mimic_cleaned_text_only.csv\"\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "# Number of test samples to evaluate (set to -1 for all)\n",
    "NUM_TEST_SAMPLES = 100\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "MAX_NEW_TOKENS = 512\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "REPETITION_PENALTY = 1.1\n",
    "\n",
    "# ============================================================================\n",
    "# BERTSCORE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CLINICAL_BERT = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Base model: {MODEL_NAME}\")\n",
    "print(f\"  Adapter path: {ADAPTER_PATH}\")\n",
    "print(f\"  Dataset: {MIMIC_CSV_PATH}\")\n",
    "print(f\"  Test samples: {NUM_TEST_SAMPLES if NUM_TEST_SAMPLES > 0 else 'All'}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Base model: google/medgemma-4b-it\n",
      "  Adapter path: ./medgemma-discharge-summarization/final\n",
      "  Dataset: mimic_cleaned_text_only.csv\n",
      "  Test samples: 100\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Load Test Dataset\n",
    "\n",
    "Load and prepare the MIMIC dataset for evaluation."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T05:54:15.123122200Z",
     "start_time": "2025-12-11T05:54:08.994657600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "print(f\"Loading dataset from: {MIMIC_CSV_PATH}\\n\")\n",
    "\n",
    "if os.path.exists(MIMIC_CSV_PATH):\n",
    "    # Load the CSV file\n",
    "    mimic_df = pd.read_csv(MIMIC_CSV_PATH)\n",
    "\n",
    "    # Take subset for testing (first 10,000 samples)\n",
    "    mimic_df = mimic_df[:10_000]\n",
    "\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"  Total samples: {len(mimic_df)}\")\n",
    "    print(f\"  Columns: {list(mimic_df.columns)}\\n\")\n",
    "\n",
    "    # Add instruction column\n",
    "    instruction_text = \"Summarize the following clinical discharge notes. Include ALL diagnoses, medications, vitals, lab results, procedures, and follow-up instructions. Ensure complete coverage of all medical entities.\"\n",
    "    mimic_df['instruction'] = instruction_text\n",
    "\n",
    "    # Rename columns\n",
    "    mimic_df = mimic_df.rename(columns={\n",
    "        'final_input': 'input',\n",
    "        'final_target': 'output'\n",
    "    })\n",
    "\n",
    "    # Remove rows with missing data\n",
    "    mimic_df = mimic_df.dropna(subset=['input', 'output'])\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(mimic_df[['instruction', 'input', 'output']])\n",
    "\n",
    "    # Split into train and test sets (5% test)\n",
    "    dataset = dataset.train_test_split(test_size=0.05, seed=42, shuffle=True)\n",
    "    test_dataset = dataset[\"test\"]\n",
    "\n",
    "    # Limit test samples if configured\n",
    "    if NUM_TEST_SAMPLES > 0 and NUM_TEST_SAMPLES < len(test_dataset):\n",
    "        test_dataset = test_dataset.select(range(NUM_TEST_SAMPLES))\n",
    "\n",
    "    print(f\"✓ Test dataset prepared!\")\n",
    "    print(f\"  Test samples: {len(test_dataset)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"⚠️  File not found: {MIMIC_CSV_PATH}\")\n",
    "    print(f\"Please ensure the dataset is in the project directory\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: mimic_cleaned_text_only.csv\n",
      "\n",
      "✓ Dataset loaded successfully!\n",
      "  Total samples: 10000\n",
      "  Columns: ['final_input', 'final_target']\n",
      "\n",
      "✓ Test dataset prepared!\n",
      "  Test samples: 100\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Load Fine-Tuned Model\n",
    "\n",
    "Load the base MedGemma model with 4-bit quantization, then load the fine-tuned LoRA adapters.\n",
    "\n",
    "**Memory Optimization**: Uses QLoRA (4-bit quantization) for efficient inference."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T05:54:15.184960800Z",
     "start_time": "2025-12-11T05:54:15.147071800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Enable synchronous CUDA for better error messages\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"✓ CUDA synchronous mode enabled\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CUDA synchronous mode enabled\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T05:54:37.458399300Z",
     "start_time": "2025-12-11T05:54:21.397423700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOADING FINE-TUNED MEDGEMMA MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "compute_dtype = torch.bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "print(\"\\nStep 1: Loading base model...\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Quantization: 4-bit NF4\")\n",
    "print(f\"  This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "print(\"✓ Base model loaded\")\n",
    "\n",
    "# Load LoRA adapters\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    print(f\"\\nStep 2: Loading LoRA adapters...\")\n",
    "    print(f\"  Path: {ADAPTER_PATH}\")\n",
    "    model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
    "    print(\"✓ LoRA adapters loaded\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  WARNING: Adapter path not found: {ADAPTER_PATH}\")\n",
    "    print(\"   Using base model only (not fine-tuned)\")\n",
    "\n",
    "# Get actual vocab size from model\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "actual_vocab_size = embedding_layer.weight.shape[0]\n",
    "print(f\"\\n  Model embedding vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nStep 3: Loading tokenizer...\")\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            ADAPTER_PATH,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "        print(\"✓ Tokenizer loaded from adapter path\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Adapter tokenizer failed: {e}\")\n",
    "        print(\"   Loading base model tokenizer instead\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\",\n",
    "            add_eos_token=True\n",
    "        )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"right\",\n",
    "        add_eos_token=True\n",
    "    )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"\\n  Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"  PAD token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"  EOS token ID: {tokenizer.eos_token_id}\")\n",
    "\n",
    "# Validation check\n",
    "if len(tokenizer) != actual_vocab_size:\n",
    "    print(f\"\\n⚠️  MISMATCH DETECTED!\")\n",
    "    print(f\"   Tokenizer vocab: {len(tokenizer)}\")\n",
    "    print(f\"   Model vocab: {actual_vocab_size}\")\n",
    "\n",
    "    if len(tokenizer) > actual_vocab_size:\n",
    "        print(f\"\\n   Resizing model embeddings to {len(tokenizer)}...\")\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        actual_vocab_size = model.get_input_embeddings().weight.shape[0]\n",
    "        print(f\"   ✓ New model vocab size: {actual_vocab_size}\")\n",
    "\n",
    "# Validation test\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"VALIDATION TEST\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "test_text = \"Patient presented with chest pain.\"\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "max_id = test_tokens['input_ids'].max().item()\n",
    "\n",
    "print(f\"Test text: '{test_text}'\")\n",
    "print(f\"Max token ID: {max_id}\")\n",
    "print(f\"Valid range: [0, {actual_vocab_size - 1}]\")\n",
    "\n",
    "if max_id >= actual_vocab_size:\n",
    "    print(f\"\\n❌ CRITICAL ERROR: Token ID out of range!\")\n",
    "    raise ValueError(f\"Token ID {max_id} >= vocab size {actual_vocab_size}\")\n",
    "else:\n",
    "    print(f\"\\n✅ VALIDATION PASSED!\")\n",
    "    print(f\"   All token IDs are within valid range\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"\\n✓ Model ready for evaluation\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING FINE-TUNED MEDGEMMA MODEL\n",
      "================================================================================\n",
      "\n",
      "Step 1: Loading base model...\n",
      "  Model: google/medgemma-4b-it\n",
      "  Quantization: 4-bit NF4\n",
      "  This may take 2-3 minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a28b187141ad4b69910ae6a2fb0d174b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Base model loaded\n",
      "\n",
      "Step 2: Loading LoRA adapters...\n",
      "  Path: ./medgemma-discharge-summarization/final\n",
      "✓ LoRA adapters loaded\n",
      "\n",
      "  Model embedding vocab size: 262208\n",
      "\n",
      "Step 3: Loading tokenizer...\n",
      "✓ Tokenizer loaded from adapter path\n",
      "\n",
      "  Tokenizer vocab size: 262145\n",
      "  PAD token ID: 1\n",
      "  EOS token ID: 1\n",
      "\n",
      "⚠️  MISMATCH DETECTED!\n",
      "   Tokenizer vocab: 262145\n",
      "   Model vocab: 262208\n",
      "\n",
      "================================================================================\n",
      "VALIDATION TEST\n",
      "================================================================================\n",
      "Test text: 'Patient presented with chest pain.'\n",
      "Max token ID: 236761\n",
      "Valid range: [0, 262207]\n",
      "\n",
      "✅ VALIDATION PASSED!\n",
      "   All token IDs are within valid range\n",
      "\n",
      "✓ Model ready for evaluation\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Generate Predictions on Test Set\n",
    "\n",
    "Generate clinical summaries for all test samples."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T06:53:53.449518400Z",
     "start_time": "2025-12-11T05:54:37.494692900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Generating predictions on test set...\\n\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    print(f\"Generating summary {i + 1}/{len(test_dataset)}...\", end=\" \")\n",
    "\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample[\"input\"]\n",
    "    reference = sample[\"output\"]\n",
    "\n",
    "    # Format the prompt (without the model's response)\n",
    "    inference_prompt = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "Clinical Notes:\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(inference_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the model's response\n",
    "    model_response_marker = \"<start_of_turn>model\"\n",
    "    if model_response_marker in generated_text:\n",
    "        generated_summary = generated_text.split(model_response_marker)[-1].strip()\n",
    "    else:\n",
    "        generated_summary = generated_text[len(inference_prompt):].strip()\n",
    "\n",
    "    predictions.append(generated_summary)\n",
    "    references.append(reference)\n",
    "\n",
    "    print(f\"✓ ({len(generated_summary)} chars)\")\n",
    "\n",
    "print(f\"\\n✓ All predictions generated\")\n",
    "print(f\"  Total predictions: {len(predictions)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test set...\n",
      "\n",
      "Generating summary 1/100... ✓ (1093 chars)\n",
      "Generating summary 2/100... ✓ (923 chars)\n",
      "Generating summary 3/100... ✓ (1740 chars)\n",
      "Generating summary 4/100... ✓ (237 chars)\n",
      "Generating summary 5/100... ✓ (310 chars)\n",
      "Generating summary 6/100... ✓ (1354 chars)\n",
      "Generating summary 7/100... ✓ (1419 chars)\n",
      "Generating summary 8/100... ✓ (915 chars)\n",
      "Generating summary 9/100... ✓ (512 chars)\n",
      "Generating summary 10/100... ✓ (2981 chars)\n",
      "Generating summary 11/100... ✓ (1386 chars)\n",
      "Generating summary 12/100... ✓ (1665 chars)\n",
      "Generating summary 13/100... ✓ (1926 chars)\n",
      "Generating summary 14/100... ✓ (431 chars)\n",
      "Generating summary 15/100... ✓ (422 chars)\n",
      "Generating summary 16/100... ✓ (1813 chars)\n",
      "Generating summary 17/100... ✓ (202 chars)\n",
      "Generating summary 18/100... ✓ (408 chars)\n",
      "Generating summary 19/100... ✓ (485 chars)\n",
      "Generating summary 20/100... ✓ (1023 chars)\n",
      "Generating summary 21/100... ✓ (1149 chars)\n",
      "Generating summary 22/100... ✓ (561 chars)\n",
      "Generating summary 23/100... ✓ (3451 chars)\n",
      "Generating summary 24/100... ✓ (807 chars)\n",
      "Generating summary 25/100... ✓ (596 chars)\n",
      "Generating summary 26/100... ✓ (494 chars)\n",
      "Generating summary 27/100... ✓ (619 chars)\n",
      "Generating summary 28/100... ✓ (3093 chars)\n",
      "Generating summary 29/100... ✓ (2081 chars)\n",
      "Generating summary 30/100... ✓ (692 chars)\n",
      "Generating summary 31/100... ✓ (728 chars)\n",
      "Generating summary 32/100... ✓ (281 chars)\n",
      "Generating summary 33/100... ✓ (799 chars)\n",
      "Generating summary 34/100... ✓ (593 chars)\n",
      "Generating summary 35/100... ✓ (230 chars)\n",
      "Generating summary 36/100... ✓ (1776 chars)\n",
      "Generating summary 37/100... ✓ (906 chars)\n",
      "Generating summary 38/100... ✓ (1401 chars)\n",
      "Generating summary 39/100... ✓ (957 chars)\n",
      "Generating summary 40/100... ✓ (1524 chars)\n",
      "Generating summary 41/100... ✓ (733 chars)\n",
      "Generating summary 42/100... ✓ (3161 chars)\n",
      "Generating summary 43/100... ✓ (536 chars)\n",
      "Generating summary 44/100... ✓ (733 chars)\n",
      "Generating summary 45/100... ✓ (2730 chars)\n",
      "Generating summary 46/100... ✓ (674 chars)\n",
      "Generating summary 47/100... ✓ (818 chars)\n",
      "Generating summary 48/100... ✓ (508 chars)\n",
      "Generating summary 49/100... ✓ (198 chars)\n",
      "Generating summary 50/100... ✓ (179 chars)\n",
      "Generating summary 51/100... ✓ (36 chars)\n",
      "Generating summary 52/100... ✓ (220 chars)\n",
      "Generating summary 53/100... ✓ (502 chars)\n",
      "Generating summary 54/100... ✓ (678 chars)\n",
      "Generating summary 55/100... ✓ (707 chars)\n",
      "Generating summary 56/100... ✓ (606 chars)\n",
      "Generating summary 57/100... ✓ (3471 chars)\n",
      "Generating summary 58/100... ✓ (2096 chars)\n",
      "Generating summary 59/100... ✓ (622 chars)\n",
      "Generating summary 60/100... ✓ (3360 chars)\n",
      "Generating summary 61/100... ✓ (2761 chars)\n",
      "Generating summary 62/100... ✓ (2937 chars)\n",
      "Generating summary 63/100... ✓ (529 chars)\n",
      "Generating summary 64/100... ✓ (145 chars)\n",
      "Generating summary 65/100... ✓ (1091 chars)\n",
      "Generating summary 66/100... ✓ (1596 chars)\n",
      "Generating summary 67/100... ✓ (664 chars)\n",
      "Generating summary 68/100... ✓ (522 chars)\n",
      "Generating summary 69/100... ✓ (1808 chars)\n",
      "Generating summary 70/100... ✓ (1902 chars)\n",
      "Generating summary 71/100... ✓ (2954 chars)\n",
      "Generating summary 72/100... ✓ (710 chars)\n",
      "Generating summary 73/100... ✓ (1342 chars)\n",
      "Generating summary 74/100... ✓ (1388 chars)\n",
      "Generating summary 75/100... ✓ (651 chars)\n",
      "Generating summary 76/100... ✓ (1100 chars)\n",
      "Generating summary 77/100... ✓ (1180 chars)\n",
      "Generating summary 78/100... ✓ (3018 chars)\n",
      "Generating summary 79/100... ✓ (212 chars)\n",
      "Generating summary 80/100... ✓ (895 chars)\n",
      "Generating summary 81/100... ✓ (523 chars)\n",
      "Generating summary 82/100... ✓ (1096 chars)\n",
      "Generating summary 83/100... ✓ (2597 chars)\n",
      "Generating summary 84/100... ✓ (1391 chars)\n",
      "Generating summary 85/100... ✓ (3616 chars)\n",
      "Generating summary 86/100... ✓ (164 chars)\n",
      "Generating summary 87/100... ✓ (130 chars)\n",
      "Generating summary 88/100... ✓ (2539 chars)\n",
      "Generating summary 89/100... ✓ (135 chars)\n",
      "Generating summary 90/100... ✓ (844 chars)\n",
      "Generating summary 91/100... ✓ (1522 chars)\n",
      "Generating summary 92/100... ✓ (2300 chars)\n",
      "Generating summary 93/100... ✓ (349 chars)\n",
      "Generating summary 94/100... ✓ (3327 chars)\n",
      "Generating summary 95/100... ✓ (1383 chars)\n",
      "Generating summary 96/100... ✓ (425 chars)\n",
      "Generating summary 97/100... ✓ (1409 chars)\n",
      "Generating summary 98/100... ✓ (879 chars)\n",
      "Generating summary 99/100... ✓ (108 chars)\n",
      "Generating summary 100/100... ✓ (3540 chars)\n",
      "\n",
      "✓ All predictions generated\n",
      "  Total predictions: 100\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Compute Clinical BERTScore\n",
    "\n",
    "Evaluate semantic similarity using Bio_ClinicalBERT."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T06:53:59.216400300Z",
     "start_time": "2025-12-11T06:53:53.537418200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPUTING CLINICAL BERTSCORE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nInitializing BERTScorer with {CLINICAL_BERT}...\")\n",
    "clinical_scorer = BERTScorer(\n",
    "    model_type=CLINICAL_BERT,\n",
    "    num_layers=9,\n",
    "    rescale_with_baseline=False,\n",
    "    lang=\"en\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "print(\"✓ BERTScorer initialized\")\n",
    "\n",
    "# Get the tokenizer from the scorer to do proper truncation\n",
    "bert_tokenizer = clinical_scorer._tokenizer\n",
    "\n",
    "\n",
    "def truncate_with_bert_tokenizer(text: str, tokenizer, max_length: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Properly truncate text using BERT's tokenizer to ensure it fits within token limit.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to truncate\n",
    "        tokenizer: BERT tokenizer\n",
    "        max_length: Maximum number of tokens (BERT supports 512, we use 500 for safety)\n",
    "\n",
    "    Returns:\n",
    "        Truncated text that will tokenize to <= max_length tokens\n",
    "    \"\"\"\n",
    "    # Tokenize and truncate\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    # Decode back to text\n",
    "    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return truncated_text\n",
    "\n",
    "\n",
    "# Truncate texts to fit BERT's 512 token limit\n",
    "print(\"\\nPreparing texts (truncating long sequences for BERT)...\")\n",
    "print(\"  Using BERT tokenizer for accurate truncation...\")\n",
    "\n",
    "truncated_predictions = []\n",
    "truncated_references = []\n",
    "\n",
    "for pred, ref in zip(predictions, references):\n",
    "    truncated_predictions.append(truncate_with_bert_tokenizer(pred, bert_tokenizer))\n",
    "    truncated_references.append(truncate_with_bert_tokenizer(ref, bert_tokenizer))\n",
    "\n",
    "# Check truncation statistics\n",
    "orig_pred_lens = [len(bert_tokenizer.encode(p)) for p in predictions]\n",
    "trunc_pred_lens = [len(bert_tokenizer.encode(p)) for p in truncated_predictions]\n",
    "num_truncated = sum(1 for o, t in zip(orig_pred_lens, trunc_pred_lens) if o != t)\n",
    "\n",
    "print(f\"  {num_truncated}/{len(predictions)} predictions were truncated\")\n",
    "print(f\"  Average prediction tokens: {np.mean(trunc_pred_lens):.0f}\")\n",
    "print(f\"  Max prediction tokens: {max(trunc_pred_lens)}\")\n",
    "\n",
    "print(\"\\nCalculating BERTScores (this may take a few minutes)...\\n\")\n",
    "\n",
    "# Compute scores with truncated texts\n",
    "P, R, F1 = clinical_scorer.score(\n",
    "    cands=truncated_predictions,\n",
    "    refs=truncated_references,\n",
    ")\n",
    "\n",
    "# Convert to numpy\n",
    "precision_scores = P.cpu().numpy()\n",
    "recall_scores = R.cpu().numpy()\n",
    "f1_scores = F1.cpu().numpy()\n",
    "\n",
    "# Compute averages\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLINICAL BERTSCORE RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAverage Precision: {avg_precision:.4f}\")\n",
    "print(f\"  → How much of the generated summary is clinically relevant\")\n",
    "\n",
    "print(f\"\\nAverage Recall: {avg_recall:.4f}\")\n",
    "print(f\"  → How much of the reference summary is captured\")\n",
    "print(f\"  → PRIMARY METRIC FOR HIGH RECALL\")\n",
    "\n",
    "print(f\"\\nAverage F1: {avg_f1:.4f}\")\n",
    "print(f\"  → Harmonic mean of precision and recall\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"\\nNote: Texts were truncated to 500 tokens using BERT's tokenizer.\")\n",
    "print(f\"This ensures all texts fit within BERT's 512 token limit.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPUTING CLINICAL BERTSCORE\n",
      "================================================================================\n",
      "\n",
      "Initializing BERTScorer with emilyalsentzer/Bio_ClinicalBERT...\n",
      "✓ BERTScorer initialized\n",
      "\n",
      "Preparing texts (truncating long sequences for BERT)...\n",
      "  Using BERT tokenizer for accurate truncation...\n",
      "  15/100 predictions were truncated\n",
      "  Average prediction tokens: 227\n",
      "  Max prediction tokens: 500\n",
      "\n",
      "Calculating BERTScores (this may take a few minutes)...\n",
      "\n",
      "================================================================================\n",
      "CLINICAL BERTSCORE RESULTS\n",
      "================================================================================\n",
      "\n",
      "Average Precision: 0.6684\n",
      "  → How much of the generated summary is clinically relevant\n",
      "\n",
      "Average Recall: 0.6677\n",
      "  → How much of the reference summary is captured\n",
      "  → PRIMARY METRIC FOR HIGH RECALL\n",
      "\n",
      "Average F1: 0.6670\n",
      "  → Harmonic mean of precision and recall\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Note: Texts were truncated to 500 tokens using BERT's tokenizer.\n",
      "This ensures all texts fit within BERT's 512 token limit.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Detailed Per-Sample Analysis\n",
    "\n",
    "Display scores for each test sample."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T06:53:59.324253300Z",
     "start_time": "2025-12-11T06:53:59.302060500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Per-Sample BERTScore Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(min(10, len(predictions))):  # Show first 10 samples\n",
    "    print(f\"\\nSample {i + 1}:\")\n",
    "    print(f\"  Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"  F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n... (showing first 10 of {len(predictions)} samples)\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Sample BERTScore Results:\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Precision: 0.6535\n",
      "  Recall: 0.6740\n",
      "  F1: 0.6636\n",
      "\n",
      "Sample 2:\n",
      "  Precision: 0.7098\n",
      "  Recall: 0.6814\n",
      "  F1: 0.6953\n",
      "\n",
      "Sample 3:\n",
      "  Precision: 0.6585\n",
      "  Recall: 0.7089\n",
      "  F1: 0.6827\n",
      "\n",
      "Sample 4:\n",
      "  Precision: 0.6227\n",
      "  Recall: 0.6166\n",
      "  F1: 0.6196\n",
      "\n",
      "Sample 5:\n",
      "  Precision: 0.6646\n",
      "  Recall: 0.6018\n",
      "  F1: 0.6316\n",
      "\n",
      "Sample 6:\n",
      "  Precision: 0.5618\n",
      "  Recall: 0.6760\n",
      "  F1: 0.6137\n",
      "\n",
      "Sample 7:\n",
      "  Precision: 0.6857\n",
      "  Recall: 0.6775\n",
      "  F1: 0.6816\n",
      "\n",
      "Sample 8:\n",
      "  Precision: 0.6708\n",
      "  Recall: 0.6562\n",
      "  F1: 0.6634\n",
      "\n",
      "Sample 9:\n",
      "  Precision: 0.7060\n",
      "  Recall: 0.8628\n",
      "  F1: 0.7765\n",
      "\n",
      "Sample 10:\n",
      "  Precision: 0.6902\n",
      "  Recall: 0.7007\n",
      "  F1: 0.6954\n",
      "\n",
      "... (showing first 10 of 100 samples)\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Qualitative Analysis\n",
    "\n",
    "Compare generated summaries with reference summaries for qualitative assessment."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T06:53:59.346782200Z",
     "start_time": "2025-12-11T06:53:59.326258300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"QUALITATIVE ANALYSIS: Generated vs Reference Summaries\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show 3 examples\n",
    "num_examples = min(3, len(predictions))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"EXAMPLE {i + 1}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "    print(\"INPUT (Clinical Notes - first 400 chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(test_dataset[i][\"input\"][:400] + \"...\\n\")\n",
    "\n",
    "    print(\"REFERENCE SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(references[i])\n",
    "    print()\n",
    "\n",
    "    print(\"GENERATED SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(predictions[i])\n",
    "    print()\n",
    "\n",
    "    print(\"SCORES:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Precision: {precision_scores[i]:.4f}\")\n",
    "    print(f\"Recall: {recall_scores[i]:.4f}\")\n",
    "    print(f\"F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"END OF QUALITATIVE ANALYSIS\")\n",
    "print(f\"{'=' * 80}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUALITATIVE ANALYSIS: Generated vs Reference Summaries\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 1\n",
      "================================================================================\n",
      "\n",
      "INPUT (Clinical Notes - first 400 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "write discharge summary chief complaint sob cough history present illness past medical history stage nsclc dxd sp cyberknife treatment followed rad onc asthmacopd pfts mod vent defect red diff capactiy fev fevfvc osteoporosis hip fracture anemiathrombocytopenia h pylorus sp tx etoh pancreatitis neuropathyosteoporosis prostate ca prostatectomy biochemical rec urrence time surgery node neg extracaps...\n",
      "\n",
      "REFERENCE SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "copd stage lung ca sp cyberknife week progressive productive cough shortness breath copd exacerbation superimposed pnabronchitis infection likely precipitant patient also continues smoke unclear aspergillosis involved unclear significance regard filled bulla clear post obstructive process treated levoflox x day prednisone taper starting mg daily taper day blood cx ngtd galactomanin normal b glucan pending discharge close follow pulmonologist advair neb continued case mgt helped obtain nebulizer sp cyberknife radiologic regression tumor cont pulm rad onc follow\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "ronic pancereatitis presented productive coughsob found leukocytosis cxr consistent copdasthma exacerbatiion setting nsccscc copdacopdfibrosis treated iv solumedrol day x neb prn continued home inhaler azithromycin levofloxacin also started mdi prednisone tapered discharged prednisone taper complete day course levonoxazithrolevo foley placed due urine retention likely secondary chronic nerve damage related spinal stenosis urinary tract infection uaucx negative uti patient denied dysuria hematuria however received ceftriaxone g iv ed transitioned ciprofloxacin bid house prior discharge able void without difficulty anemia patient baseline iron deficient anemia thought chronic disease evidence active bleeding transfused unit packed rbc hospital stay need colonoscopy outpatient rule malignancy lung cancer screening age high risk smoker htn benign normotensive throughout hospitalization hyperlipidemia ho pancreatitis unclear etiology stable admission transitional issue followup ct chest week evaluate resolution pneumonia consider repeat colonscopy screen gi malignancy consideration\n",
      "\n",
      "SCORES:\n",
      "--------------------------------------------------------------------------------\n",
      "Precision: 0.6535\n",
      "Recall: 0.6740\n",
      "F1: 0.6636\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 2\n",
      "================================================================================\n",
      "\n",
      "INPUT (Clinical Notes - first 400 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "create summary based following information chief complaint thrombocytopenia history present illness yo man seronegative ra dm htn present home lab testing revealed thrombocytopenia note past month easy bruising bleeding minor cut took long time stop additionally dental work done visit bleeding bruise cheek dentist concerned spoke physician recommended stop ibuprofen dr fax lab rec cbc coags done d...\n",
      "\n",
      "REFERENCE SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "yo man ho seronegative arthritis diabetes melitus hypertension present acute severe isolated thrombocytopenia thrombocytopenia unclear etiology likely subacute chronic given history worse bleedingbruising month given recent medication drug induced thrombocytopenia plaquenil nsaid celebrex lisinopril possible idiopathic thrombocytopneic purpura also possible active bleeding throughout hospitalization noted despite low platelet count evaluated hematology started steroid decadorn mg iv day one followed mgkgday prednisone platelet count rose hospital day hospital day given determined safe discharge dose prednisone daily urgent hematology follow additional workup hepatitis serology sent showed past infection hepatitis b hepatitis c hiv negative antiplatelet antibody pending time discharge ultrasound spleen obtained evaluate splenomegally spleen normal given elevated blood pressure initially concern spontaneous bleeding particularly intracranially monitored neuro check every hour neurologic change require transfusion platelet follow dr hematology management thrombocytopenia hypertension admission blood pressure poorly controlled normal given low platelet count urgency reduce blood pressure goal treated iv labetolol metoprolol achieve rapid reduction started oral labetolol mg three time daily achieved good control blood pressure transitioned oral metoprolol mg twice daily ease home dosing lisinopril continued due possible antiplatelet effect diabetes melitus glycemic control markedly worse high dose steroid expected requiring large dos sliding scale insulin first hour despite continuing oral agent metformin mg twice daily glyburide mg morning given hospital day started unit lantus insulin daily continued require large dos sliding scale insulin increased unit lantus daily improved control taught check blood sugar administer insulin discharged prescription glucometer lancet test strip syrinx lantus insulin humalog insulin also scheduled follow continue management diabetes teaching instructed check blood sugar time daily bring information appointment rheumatoid arthritis ra medication stoped due thrombocytopenia stiffness house likely due steroid need follow dr management ra steroid tapered anemia noted admission microcytic anemia without evidence hemolysis review smear minor iron deficiency likely reflects underlying inflammaiton repletion recomended hematology vitamin b folate replete renal failure recent baseline admission creatinine elevated possibly reflection lisinopril use urine checked showed sign blood protein time discharge creatinine improved suggesting chronic component well recommended followed endocrinologist pcp likley related diabetes communication patient wife\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "decrease plt baseline likely secondary viral illness received ivig x ed platelet increase appropriate response subsequently improved remained normal range discharged close monitoring hematology rheumatology outpatient continued plaquenil prednisone held celecoxib setting thrombocytopeniainflammation lfts wnl throughout hospitalization pt monitored closely platelet rebound discharged instruction monitor sign bleeding diabetes control difficult given diabetic diet restarted lantushumaqidacoralis insulin adjusted need close followup pcp regarding management blood glucose hypertension wellcontrolled continued home dose lisenoprilsingazepine metropolol hypothyroidism tsh mildly elevated however patient thyroid function test normal year last ultrasound showed diffuse nodular goiter consistent benign process patient followup primary care provider consideration additional testing evaluate underlying cause elevated tsh\n",
      "\n",
      "SCORES:\n",
      "--------------------------------------------------------------------------------\n",
      "Precision: 0.7098\n",
      "Recall: 0.6814\n",
      "F1: 0.6953\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 3\n",
      "================================================================================\n",
      "\n",
      "INPUT (Clinical Notes - first 400 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "write discharge summary chief complaint brbpr history present illness m female hx seizure disorder crohn disease followed dr cva residual arm weakness present evaluation lower gi bleeding pt state known crohn disease followed dr ongoing lower gi bleeding past day pt note cup red blood per rectum xday x day x past one day pt note associated lower abdominal cramping note nauseavomitingfeverschillsch...\n",
      "\n",
      "REFERENCE SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "year old female history crohn disease come endoscopy suite evaluation lower gi bleeding brbpr acute blood loss anemia chrons disease pt present evaluation brbpr day duration sigmoidoscopy outpatient evaluate source bleeding showed internal hemorrhoid diverticulosis bleeding throughout colon without source identified outpatient gi doctor referred patient inpatient monitoring work colonoscopy arrival patient hds hgb started ppi evaluated inpatient gi team patient reported reluctance undergoing colonoscopy requested instead follow gi physician gi consult team felt reasonable given stable blood count one additional episode blood mixed stool bright blood tp patient felt smaller quantity comparison episode home patient transitioned oral pantoprazole started iron discharge given iron deficiency blood work source bleeding remained unclear likely chrons flare v internal hemorrhoid instructed call outpatient gi physician set follow appointment within next day counseled red flag return precaution chronic medical problem seizure disorder ch keppra htn home verapamil held cf gib discharged instruction restart medication given stable hgb bps\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "--------------------------------------------------------------------------------\n",
      "ightred blood stool likely due chronic crohdn flare setting worsening diarrhea patient monitored closely throughout hospitalization received iv fluid electrolyte repletion needed hematocrit remained within normal limit hospital course also noted several episode loose bowel movement concerning croahndisease flare time stool study obtained including viral culture fecal antigen testing salmonella shigella campylobacter cryptosporidium cyclospora negative patient continued outpatient regimen mesalamine well prednisone dose increased mgmg mg tolerated change well discharge acute chronic anemia likely secondary slow gi bleed consistent patient known gastrointestinal bleed setting crohne disease baseline hb hgac low ferritin iron study consistent chronic iron deficiency anemia patient transfused unit prbc hospital stay hh stabilized hemoglobin level appropriate discharge tolerating regular diet discharged close followup gi physician gastroenterology inpatient patient underwent flexible sigmoioscopy colonoscopy revealed extensive colitis involving transverse descending sigmoid colon focal area erythema inflammation biopsied gi recommended continue mesalazine mg tid increase prednisone mg twice weekly total mg mg return gi followup appointment next week management crohna disease may benefit initiation humira therapy upon discharge hyponatremia hyervolemic hyaline cast urine suggestive prerenal etiology improved hydration hypertension maintained home antihypertensive regimen transitional issue please continue monitor hematocrt high dose prednisone taper consider starting antiinflammatory agent like humera discontinue meslamine need repeat lab check include chem cr ca tsh hbac pending discharge ensure resolution infection\n",
      "\n",
      "SCORES:\n",
      "--------------------------------------------------------------------------------\n",
      "Precision: 0.6585\n",
      "Recall: 0.7089\n",
      "F1: 0.6827\n",
      "\n",
      "================================================================================\n",
      "END OF QUALITATIVE ANALYSIS\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Save Evaluation Results\n",
    "\n",
    "Save predictions and scores to files for further analysis."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T06:53:59.378604300Z",
     "start_time": "2025-12-11T06:53:59.346782200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# Create results directory\n",
    "results_dir = \"./evaluation_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Prepare results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'input': [sample['input'] for sample in test_dataset],\n",
    "    'reference': references,\n",
    "    'prediction': predictions,\n",
    "    'bertscore_precision': precision_scores,\n",
    "    'bertscore_recall': recall_scores,\n",
    "    'bertscore_f1': f1_scores\n",
    "})\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = os.path.join(results_dir, \"evaluation_results.csv\")\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Results saved to CSV: {csv_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"adapter_path\": ADAPTER_PATH,\n",
    "    \"num_test_samples\": len(predictions),\n",
    "    \"bertscore\": {\n",
    "        \"precision\": {\n",
    "            \"mean\": float(avg_precision),\n",
    "            \"std\": float(np.std(precision_scores)),\n",
    "            \"min\": float(np.min(precision_scores)),\n",
    "            \"max\": float(np.max(precision_scores))\n",
    "        },\n",
    "        \"recall\": {\n",
    "            \"mean\": float(avg_recall),\n",
    "            \"std\": float(np.std(recall_scores)),\n",
    "            \"min\": float(np.min(recall_scores)),\n",
    "            \"max\": float(np.max(recall_scores))\n",
    "        },\n",
    "        \"f1\": {\n",
    "            \"mean\": float(avg_f1),\n",
    "            \"std\": float(np.std(f1_scores)),\n",
    "            \"min\": float(np.min(f1_scores)),\n",
    "            \"max\": float(np.max(f1_scores))\n",
    "        }\n",
    "    },\n",
    "    \"generation_config\": {\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"top_k\": TOP_K,\n",
    "        \"repetition_penalty\": REPETITION_PENALTY\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(results_dir, \"summary_statistics.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"✓ Summary statistics saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"\\nAll results saved to: {results_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Results saved to CSV: ./evaluation_results\\evaluation_results.csv\n",
      "✓ Summary statistics saved: ./evaluation_results\\summary_statistics.json\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "All results saved to: ./evaluation_results\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Evaluation Checklist\n",
    "\n",
    "Use this checklist to assess the quality of generated summaries:\n",
    "\n",
    "**High Recall Checklist**:\n",
    "- ☐ Are all diagnoses mentioned?\n",
    "- ☐ Are all medications listed with dosages?\n",
    "- ☐ Are vital signs included?\n",
    "- ☐ Are abnormal lab results reported?\n",
    "- ☐ Are procedures and treatments described?\n",
    "- ☐ Are follow-up instructions present?\n",
    "- ☐ Is the timeline/hospital course clear?\n",
    "\n",
    "**Quality Assessment**:\n",
    "- Target Recall: ≥0.90 for production use\n",
    "- Target F1: ≥0.85 for balanced performance\n",
    "- Check for hallucinations (invented facts not in source)\n",
    "- Verify medical terminology accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
